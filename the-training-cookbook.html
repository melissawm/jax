
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>The Training Cookbook &#8212; JAX  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/style.css?v=17fd2dad" />
    <link rel="stylesheet" href="_static/style.css" type="text/css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=30646c52"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs"></script>
    <script type="module" src="https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs"></script>
    <script type="module">import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";import elkLayouts from "https://cdn.jsdelivr.net/npm/@mermaid-js/layout-elk@0.1.4/dist/mermaid-layout-elk.esm.min.mjs";mermaid.registerLayoutLoaders(elkLayouts);mermaid.initialize({startOnLoad:false});</script>
    <script src="https://cdn.jsdelivr.net/npm/d3@7.9.0/dist/d3.min.js"></script>
    <script type="module">
import mermaid from "https://cdn.jsdelivr.net/npm/mermaid@11.2.0/dist/mermaid.esm.min.mjs";
window.addEventListener("load", () => mermaid.run());
</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'the-training-cookbook';</script>
    <link rel="icon" href="_static/favicon.png"/>
    <link rel="author" title="About these documents" href="about.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="The Autodiff Cookbook" href="notebooks/autodiff_cookbook.html" />
    <link rel="prev" title="Distributed data loading" href="distributed_data_loading.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/jax_logo_250px.png" class="logo__image only-light" alt="JAX  documentation - Home"/>
    <script>document.write(`<img src="_static/jax_logo_250px.png" class="logo__image only-dark" alt="JAX  documentation - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/thinking_in_jax.html">Quickstart: How to think in JAX</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="notebooks/Common_Gotchas_in_JAX.html">üî™ JAX - The Sharp Bits üî™</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="jax-101.html">JAX 101</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="jit-compilation.html">Just-in-time compilation</a></li>
<li class="toctree-l2"><a class="reference internal" href="automatic-vectorization.html">Automatic vectorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="automatic-differentiation.html">Automatic differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="working-with-pytrees.html">Working with pytrees</a></li>
<li class="toctree-l2"><a class="reference internal" href="random-numbers.html">Pseudorandom numbers</a></li>
<li class="toctree-l2"><a class="reference internal" href="control-flow.html">Control flow and logical operators with JIT</a></li>
<li class="toctree-l2"><a class="reference internal" href="stateful-computations.html">Stateful computations</a></li>
<li class="toctree-l2"><a class="reference internal" href="sharded-computation.html">Introduction to parallel programming</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Resources, guides, and references</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="key-concepts.html">Key concepts</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="advanced_guides.html">Resources and Advanced Guides</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="notebooks/Distributed_arrays_and_automatic_parallelization.html">Distributed arrays and automatic parallelization</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/explicit-sharding.html">Explicit sharding (a.k.a. ‚Äúsharding in types‚Äù)</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/shard_map.html">Manual parallelism with <code class="docutils literal notranslate"><span class="pre">shard_map</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/layout.html">Device-local array layout control</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/host-offloading.html">JAX Memories and Host Offloading</a></li>

<li class="toctree-l2"><a class="reference internal" href="multi_process.html">Introduction to multi-controller JAX (aka multi-process/multi-host JAX)</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed_data_loading.html">Distributed data loading</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">The Training Cookbook</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/autodiff_cookbook.html">The Autodiff Cookbook</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/Custom_derivative_rules_for_Python_code.html">Custom derivative rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/autodiff_remat.html">Control autodiff‚Äôs saved values with <code class="docutils literal notranslate"><span class="pre">jax.checkpoint</span></code> (aka <code class="docutils literal notranslate"><span class="pre">jax.remat</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="advanced-autodiff.html">Advanced automatic differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="errors.html">Errors</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="debugging.html">Introduction to debugging</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="debugging/print_breakpoint.html">Compiled prints and breakpoints</a></li>
<li class="toctree-l3"><a class="reference internal" href="debugging/checkify_guide.html">The <code class="docutils literal notranslate"><span class="pre">checkify</span></code> transformation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="debugging/flags.html">JAX debugging flags</a></li>
<li class="toctree-l2"><a class="reference internal" href="transfer_guard.html">Transfer guard</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytrees.html">Pytrees</a></li>
<li class="toctree-l2"><a class="reference internal" href="persistent_compilation_cache.html">Persistent compilation cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpu_performance_tips.html">GPU performance tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="profiling.html">Profiling computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="device_memory_profiling.html">Profiling device memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="array_refs.html"><code class="docutils literal notranslate"><span class="pre">ArrayRef</span></code>: mutable arrays for data plumbing and memory control</a></li>
<li class="toctree-l2"><a class="reference internal" href="external-callbacks.html">External callbacks</a></li>
<li class="toctree-l2"><a class="reference internal" href="ffi.html">Foreign function interface (FFI)</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradient-checkpointing.html">Gradient checkpointing with <code class="docutils literal notranslate"><span class="pre">jax.checkpoint</span></code> (<code class="docutils literal notranslate"><span class="pre">jax.remat</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="aot.html">Ahead-of-time lowering and compilation</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="export/index.html">Exporting and serialization</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="export/export.html">Exporting and serializing staged-out computations</a></li>
<li class="toctree-l3"><a class="reference internal" href="export/shape_poly.html">Shape polymorphism</a></li>
<li class="toctree-l3"><a class="reference internal" href="export/jax2tf.html">Interoperation with TensorFlow</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pallas/index.html">Pallas: a JAX kernel language</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="pallas/quickstart.html">Pallas Quickstart</a></li>
<li class="toctree-l3"><a class="reference internal" href="pallas/pipelining.html">Software Pipelining</a></li>
<li class="toctree-l3"><a class="reference internal" href="pallas/grid_blockspec.html">Grids and BlockSpecs</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pallas/tpu/index.html">Pallas TPU</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="pallas/tpu/details.html">Writing TPU kernels with Pallas</a></li>
<li class="toctree-l4"><a class="reference internal" href="pallas/tpu/pipelining.html">TPU Pipelining</a></li>
<li class="toctree-l4"><a class="reference internal" href="pallas/tpu/matmul.html">Matrix Multiplication</a></li>
<li class="toctree-l4"><a class="reference internal" href="pallas/tpu/sparse.html">Scalar Prefetch and Block-Sparse Computation</a></li>
<li class="toctree-l4"><a class="reference internal" href="pallas/tpu/distributed.html">Distributed Computing in Pallas for TPUs</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pallas/gpu/index.html">Pallas:Mosaic GPU</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="pallas/gpu/reference.html">Writing Mosaic GPU kernels with Pallas</a></li>
<li class="toctree-l4"><a class="reference internal" href="pallas/gpu/pipelining.html">Mosaic GPU Pipelining</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pallas/design/index.html">Pallas Design Notes</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="pallas/design/design.html">Pallas Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="pallas/design/async_note.html">Pallas Async Operations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="pallas/CHANGELOG.html">Pallas Changelog</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/neural_network_with_tfds_data.html">Training a simple neural network, with tensorflow/datasets data loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/Neural_Network_and_Data_Loading.html">Training a simple neural network, with PyTorch data loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/vmapped_log_probs.html">Autobatching for Bayesian inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/convolutions.html">Generalized convolutions in JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="xla_flags.html">XLA compiler flags</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax-primitives.html">JAX Internals: primitives</a></li>
<li class="toctree-l2"><a class="reference internal" href="jaxpr.html">JAX internals: The jaxpr language</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="contributor_guide.html">Developer notes</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="contributing.html">Contributing to JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="developer.html">Building from source</a></li>
<li class="toctree-l2"><a class="reference internal" href="investigating_a_regression.html">Investigating a regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="autodidax.html">Autodidax: JAX core from scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="autodidax2_part1.html">Autodidax2, part 1: JAX from scratch, again</a></li>

<li class="toctree-l2 has-children"><a class="reference internal" href="jep/index.html">JAX Enhancement Proposals (JEPs)</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="jep/263-prng.html">263: JAX PRNG Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/2026-custom-derivatives.html">2026: Custom JVP/VJP rules for JAX-transformable functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/4008-custom-vjp-update.html">4008: Custom VJP and `nondiff_argnums` update</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/4410-omnistaging.html">4410: Omnistaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/9263-typed-keys.html">9263: Typed keys &amp; pluggable RNGs</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/9407-type-promotion.html">9407: Design of Type Promotion Semantics for JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/9419-jax-versioning.html">9419: Jax and Jaxlib versioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/10657-sequencing-effects.html">10657: Sequencing side-effects in JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/11830-new-remat-checkpoint.html">11830: `jax.remat` / `jax.checkpoint` new implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/12049-type-annotations.html">12049: Type Annotation Roadmap for JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/14273-shard-map.html">14273: `shard_map` (`shmap`) for simple per-device code</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/15856-jex.html">15856: `jax.extend`, an extensions module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/17111-shmap-transpose.html">17111: Efficient transposition of `shard_map` (and other maps)</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/18137-numpy-scipy-scope.html">18137: Scope of JAX NumPy &amp; SciPy Wrappers</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/25516-effver.html">25516: Effort-based versioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/28661-jax-array-protocol.html">28661: Supporting the `__jax_array__` protocol</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="internals/index.html">JAX Internal Implementation Notes</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="internals/constants.html">Handling of closed-over constants</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="extensions.html">Extension guides</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="notebooks/Writing_custom_interpreters_in_Jax.html">Writing custom Jaxpr interpreters in JAX</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="jax.extend.html"><code class="docutils literal notranslate"><span class="pre">jax.extend</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="jax.extend.core.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.core</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.extend.linear_util.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.linear_util</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.extend.mlir.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.mlir</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.extend.random.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.random</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="building_on_jax.html">Building on JAX</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="notes.html">Notes</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="api_compatibility.html">API compatibility</a></li>
<li class="toctree-l2"><a class="reference internal" href="deprecation.html">Python and NumPy version support policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="async_dispatch.html">Asynchronous dispatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="concurrency.html">Concurrency</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpu_memory_allocation.html">GPU memory allocation</a></li>
<li class="toctree-l2"><a class="reference internal" href="rank_promotion_warning.html">Rank promotion warning</a></li>
<li class="toctree-l2"><a class="reference internal" href="type_promotion.html">Type promotion semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="default_dtypes.html">Default dtypes and the X64 flag</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="jax.html">Public API: <code class="docutils literal notranslate"><span class="pre">jax</span></code> package</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="jax.numpy.html"><code class="docutils literal notranslate"><span class="pre">jax.numpy</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.fft.html">jax.numpy.fft.fft</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.fft2.html">jax.numpy.fft.fft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.fftfreq.html">jax.numpy.fft.fftfreq</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.fftn.html">jax.numpy.fft.fftn</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.fftshift.html">jax.numpy.fft.fftshift</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.hfft.html">jax.numpy.fft.hfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.ifft.html">jax.numpy.fft.ifft</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.ifft2.html">jax.numpy.fft.ifft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.ifftn.html">jax.numpy.fft.ifftn</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.ifftshift.html">jax.numpy.fft.ifftshift</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.ihfft.html">jax.numpy.fft.ihfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.irfft.html">jax.numpy.fft.irfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.irfft2.html">jax.numpy.fft.irfft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.irfftn.html">jax.numpy.fft.irfftn</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.rfft.html">jax.numpy.fft.rfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.rfft2.html">jax.numpy.fft.rfft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.rfftfreq.html">jax.numpy.fft.rfftfreq</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.rfftn.html">jax.numpy.fft.rfftn</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="jax.scipy.html"><code class="docutils literal notranslate"><span class="pre">jax.scipy</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.scipy.stats.bernoulli.logpmf.html">jax.scipy.stats.bernoulli.logpmf</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.scipy.stats.bernoulli.pmf.html">jax.scipy.stats.bernoulli.pmf</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.scipy.stats.bernoulli.cdf.html">jax.scipy.stats.bernoulli.cdf</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.scipy.stats.bernoulli.ppf.html">jax.scipy.stats.bernoulli.ppf</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="jax.lax.html"><code class="docutils literal notranslate"><span class="pre">jax.lax</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.random.html"><code class="docutils literal notranslate"><span class="pre">jax.random</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.sharding.html"><code class="docutils literal notranslate"><span class="pre">jax.sharding</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.debug.html"><code class="docutils literal notranslate"><span class="pre">jax.debug</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.dlpack.html"><code class="docutils literal notranslate"><span class="pre">jax.dlpack</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.distributed.html"><code class="docutils literal notranslate"><span class="pre">jax.distributed</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.dtypes.html"><code class="docutils literal notranslate"><span class="pre">jax.dtypes</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.ffi.html"><code class="docutils literal notranslate"><span class="pre">jax.ffi</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.flatten_util.html"><code class="docutils literal notranslate"><span class="pre">jax.flatten_util</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.image.html"><code class="docutils literal notranslate"><span class="pre">jax.image</span></code> module</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="jax.nn.html"><code class="docutils literal notranslate"><span class="pre">jax.nn</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="jax.nn.initializers.html"><code class="docutils literal notranslate"><span class="pre">jax.nn.initializers</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="jax.ops.html"><code class="docutils literal notranslate"><span class="pre">jax.ops</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.profiler.html"><code class="docutils literal notranslate"><span class="pre">jax.profiler</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.ref.html"><code class="docutils literal notranslate"><span class="pre">jax.ref</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.stages.html"><code class="docutils literal notranslate"><span class="pre">jax.stages</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.test_util.html"><code class="docutils literal notranslate"><span class="pre">jax.test_util</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.tree.html"><code class="docutils literal notranslate"><span class="pre">jax.tree</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.tree_util.html"><code class="docutils literal notranslate"><span class="pre">jax.tree_util</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.typing.html"><code class="docutils literal notranslate"><span class="pre">jax.typing</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.export.html"><code class="docutils literal notranslate"><span class="pre">jax.export</span></code> module</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="jax.extend.html"><code class="docutils literal notranslate"><span class="pre">jax.extend</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="jax.extend.core.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.core</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.extend.linear_util.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.linear_util</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.extend.mlir.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.mlir</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.extend.random.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.random</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="jax.example_libraries.html"><code class="docutils literal notranslate"><span class="pre">jax.example_libraries</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="jax.example_libraries.optimizers.html"><code class="docutils literal notranslate"><span class="pre">jax.example_libraries.optimizers</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.example_libraries.stax.html"><code class="docutils literal notranslate"><span class="pre">jax.example_libraries.stax</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="jax.experimental.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.checkify.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.checkify</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.compilation_cache.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.compilation_cache</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.custom_dce.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.custom_dce</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.custom_partitioning.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.custom_partitioning</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.jet.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.jet</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.key_reuse.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.key_reuse</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.mesh_utils.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.mesh_utils</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.multihost_utils.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.multihost_utils</span></code> module</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="jax.experimental.pallas.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pallas</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-22"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="jax.experimental.pallas.mosaic_gpu.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pallas.mosaic_gpu</span></code> module</a></li>
<li class="toctree-l4"><a class="reference internal" href="jax.experimental.pallas.triton.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pallas.triton</span></code> module</a></li>
<li class="toctree-l4"><a class="reference internal" href="jax.experimental.pallas.tpu.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pallas.tpu</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.pjit.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pjit</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.serialize_executable.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.serialize_executable</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.shard_map.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.shard_map</span></code> module</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="jax.experimental.sparse.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.sparse</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-23"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.BCOO.html">jax.experimental.sparse.BCOO</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_broadcast_in_dim.html">jax.experimental.sparse.bcoo_broadcast_in_dim</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_concatenate.html">jax.experimental.sparse.bcoo_concatenate</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_dot_general.html">jax.experimental.sparse.bcoo_dot_general</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_dot_general_sampled.html">jax.experimental.sparse.bcoo_dot_general_sampled</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_dynamic_slice.html">jax.experimental.sparse.bcoo_dynamic_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_extract.html">jax.experimental.sparse.bcoo_extract</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_fromdense.html">jax.experimental.sparse.bcoo_fromdense</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_gather.html">jax.experimental.sparse.bcoo_gather</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_multiply_dense.html">jax.experimental.sparse.bcoo_multiply_dense</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_multiply_sparse.html">jax.experimental.sparse.bcoo_multiply_sparse</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_update_layout.html">jax.experimental.sparse.bcoo_update_layout</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_reduce_sum.html">jax.experimental.sparse.bcoo_reduce_sum</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_reshape.html">jax.experimental.sparse.bcoo_reshape</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_slice.html">jax.experimental.sparse.bcoo_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_sort_indices.html">jax.experimental.sparse.bcoo_sort_indices</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_squeeze.html">jax.experimental.sparse.bcoo_squeeze</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_sum_duplicates.html">jax.experimental.sparse.bcoo_sum_duplicates</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_todense.html">jax.experimental.sparse.bcoo_todense</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_transpose.html">jax.experimental.sparse.bcoo_transpose</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="jax.lib.html"><code class="docutils literal notranslate"><span class="pre">jax.lib</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.addressable_shards.html">jax.Array.addressable_shards</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.all.html">jax.Array.all</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.any.html">jax.Array.any</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.argmax.html">jax.Array.argmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.argmin.html">jax.Array.argmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.argpartition.html">jax.Array.argpartition</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.argsort.html">jax.Array.argsort</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.astype.html">jax.Array.astype</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.at.html">jax.Array.at</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.choose.html">jax.Array.choose</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.clip.html">jax.Array.clip</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.compress.html">jax.Array.compress</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.committed.html">jax.Array.committed</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.conj.html">jax.Array.conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.conjugate.html">jax.Array.conjugate</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.copy.html">jax.Array.copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.copy_to_host_async.html">jax.Array.copy_to_host_async</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.cumprod.html">jax.Array.cumprod</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.cumsum.html">jax.Array.cumsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.device.html">jax.Array.device</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.diagonal.html">jax.Array.diagonal</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.dot.html">jax.Array.dot</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.dtype.html">jax.Array.dtype</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.flat.html">jax.Array.flat</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.flatten.html">jax.Array.flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.global_shards.html">jax.Array.global_shards</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.imag.html">jax.Array.imag</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.is_fully_addressable.html">jax.Array.is_fully_addressable</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.is_fully_replicated.html">jax.Array.is_fully_replicated</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.item.html">jax.Array.item</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.itemsize.html">jax.Array.itemsize</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.max.html">jax.Array.max</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.mean.html">jax.Array.mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.min.html">jax.Array.min</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.nbytes.html">jax.Array.nbytes</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.ndim.html">jax.Array.ndim</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.nonzero.html">jax.Array.nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.prod.html">jax.Array.prod</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.ptp.html">jax.Array.ptp</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.ravel.html">jax.Array.ravel</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.real.html">jax.Array.real</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.repeat.html">jax.Array.repeat</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.reshape.html">jax.Array.reshape</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.round.html">jax.Array.round</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.searchsorted.html">jax.Array.searchsorted</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.shape.html">jax.Array.shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.sharding.html">jax.Array.sharding</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.size.html">jax.Array.size</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.sort.html">jax.Array.sort</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.squeeze.html">jax.Array.squeeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.std.html">jax.Array.std</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.sum.html">jax.Array.sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.swapaxes.html">jax.Array.swapaxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.take.html">jax.Array.take</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.to_device.html">jax.Array.to_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.trace.html">jax.Array.trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.transpose.html">jax.Array.transpose</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.var.html">jax.Array.var</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.view.html">jax.Array.view</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.T.html">jax.Array.T</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.mT.html">jax.Array.mT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="about.html">About the project</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently asked questions (FAQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Change log</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary of terms</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="config_options.html">Configuration Options</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="advanced_guides.html" class="nav-link">Resources and Advanced Guides</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">The...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/jax-ml/jax" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/the-training-cookbook.rst" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.rst</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>The Training Cookbook</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#device-mesh-and-shardings">Device Mesh and Shardings</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#device-mesh">Device Mesh</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-state-initialization">Train State Initialization</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-initialization">Parameter Initialization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizer-initialization">Optimizer Initialization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-train-step-functional-transformations">The Train Step (Functional Transformations)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-forward-pass">Model Forward Pass</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-and-optimizer-update">Gradient and Optimizer Update</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-training-loop">The Training Loop</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#efficiency-via-asynchronous-dispatch">Efficiency via Asynchronous Dispatch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-mistakes">Common Mistakes</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#requesting-device-to-host-transfers">Requesting device-to-host transfers</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interrupting-the-accelerator">Interrupting the accelerator</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-loading">Data Loading</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#achieving-high-performance">Achieving High Performance</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-parallel">Data Parallel</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fully-sharded-data-parallel-fsdp">Fully-Sharded Data Parallel (FSDP)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-parallel">Tensor Parallel</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="the-training-cookbook">
<h1>The Training Cookbook<a class="headerlink" href="#the-training-cookbook" title="Link to this heading">#</a></h1>
<p>Traditionally, machine learning codebases rely on libraries to perform much of the bookkeeping and parameter wrangling necessary for training large, complex models. While convenient, these libraries can abstract the key functionality and core APIs offered in JAX. The purpose of this cookbook, therefore, is to demonstrate best practices (or ‚Äúrecipes‚Äù) for writing simple yet high-performance machine learning training code directly in JAX. Following the patterns documented below will prepare your machine learning workloads to maximally leverage our compiler (XLA) for performance and tractability. Most training scripts adhere roughly to the following structure:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train_loop</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">):</span>
  <span class="n">record_writer</span> <span class="o">=</span> <span class="n">RecordWriter</span><span class="p">()</span>
  <span class="n">train_state</span> <span class="o">=</span> <span class="n">init_train_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
  <span class="n">train_state</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">ref</span><span class="o">.</span><span class="n">array_ref</span><span class="p">,</span> <span class="n">train_state</span><span class="p">)</span>
  <span class="n">batch</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">get_dataset_on_device</span><span class="p">(</span><span class="n">config</span><span class="p">))</span>
  <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_train_steps</span><span class="p">):</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">train_state</span><span class="p">,</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
    <span class="n">record_writer</span><span class="p">({</span><span class="s2">&quot;step&quot;</span><span class="p">:</span> <span class="n">step</span><span class="p">}</span> <span class="o">|</span> <span class="n">metrics</span><span class="p">)</span>
</pre></div>
</div>
<p>For each line of code above, we will explain the best practices and showcase the core technologies we have assembled to empower you to write simple, yet unbelievably performant code in JAX. The code above is a segment of a self-contained, completely functional <a class="reference external" href="https://github.com/jax-ml/jax/blob/main/docs/the-training-cookbook.py">companion script</a> in which we initialize a <a class="reference external" href="https://arxiv.org/abs/170.03762">Vaswani et al. (2017)</a> Transformer decoder, define the training loss for next-token prediction, and <a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam optimizer</a>, in pure JAX. The code therein is suited to TPUs, CPUs, and GPUs, as well as single- and multi-host systems. For that reason, we use the terms <em>device</em> or <em>accelerator</em> to refer interchangeably to the hardware JAX is primarily performing arithmetic on‚Äîwhether it be a TPU, GPU, or CPU‚Äîand <em>host system</em> to refer to operations performed exclusively using the host CPU. In this guide, there are many aspects of the JAX APIs we will gloss over for the sake of expediency. These are available for you to peruse at your leisure in our API documentation. However, there is a central JAX concept that one must confront in detail for much of what follows to cohere.</p>
<section id="device-mesh-and-shardings">
<h2>Device Mesh and Shardings<a class="headerlink" href="#device-mesh-and-shardings" title="Link to this heading">#</a></h2>
<p>JAX employs the <a class="reference external" href="https://en.wikipedia.org/wiki/Single_program,_multiple_data">Single Program, Multiple Data (SPMD)</a> model of parallelism. This means we write a single program that runs on multiple devices, using annotations to specify which part of the data each device is responsible for. The two primary concepts for this are the <a class="reference internal" href="jax.sharding.html#jax.sharding.Mesh" title="jax.sharding.Mesh"><code class="xref py py-class docutils literal notranslate"><span class="pre">jax.sharding.Mesh</span></code></a> and <code class="xref py py-class docutils literal notranslate"><span class="pre">jax.P</span></code>.</p>
<section id="device-mesh">
<h3>Device Mesh<a class="headerlink" href="#device-mesh" title="Link to this heading">#</a></h3>
<p>A <a class="reference internal" href="jax.sharding.html#jax.sharding.Mesh" title="jax.sharding.Mesh"><code class="xref py py-class docutils literal notranslate"><span class="pre">jax.sharding.Mesh</span></code></a> is an arrangement of all our accelerators into a NumPy <code class="docutils literal notranslate"><span class="pre">ndarray</span></code>, together with string labels for the axes of the device array. The reason for using an array is that this allows for a very convenient annotation for how arrays should be partitioned across devices. For this introduction, we will use the notation of an ordered dictionary <a class="footnote-reference brackets" href="#ordered" id="id1" role="doc-noteref"><span class="fn-bracket">[</span>1<span class="fn-bracket">]</span></a>, so that <code class="docutils literal notranslate"><span class="pre">{&quot;x&quot;:</span> <span class="pre">2,</span> <span class="pre">&quot;y&quot;:</span> <span class="pre">4}</span></code> refers to a device mesh of shape <code class="docutils literal notranslate"><span class="pre">(2,</span> <span class="pre">4)</span></code> with labeled axes <code class="docutils literal notranslate"><span class="pre">&quot;x&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;y&quot;</span></code>. To shard an array <code class="docutils literal notranslate"><span class="pre">param</span></code>, we decorate it with a <code class="xref py py-class docutils literal notranslate"><span class="pre">jax.P</span></code>, which is a tuple of <code class="docutils literal notranslate"><span class="pre">str</span> <span class="pre">|</span> <span class="pre">None</span></code> elements of the same length as the dimensions of the array. The <code class="docutils literal notranslate"><span class="pre">jax.P</span></code> specifies which axes of our array are to be sharded over which axes of devices. A more thorough account of the notation of shardings and sharded computations is available in <a class="reference internal" href="sharded-computation.html#sharded-computation"><span class="std std-ref">Introduction to parallel programming</span></a>. Some common sharding strategies such as data parallel, fully sharded data parallel, and basic tensor parallelism will be covered in <a class="reference internal" href="#achieving-high-performance"><span class="std std-ref">Achieving High Performance</span></a>.</p>
<div class="admonition-example admonition">
<p class="admonition-title">Example</p>
<p>Suppose we have a device mesh of <code class="docutils literal notranslate"><span class="pre">{&quot;x&quot;:</span> <span class="pre">2,</span> <span class="pre">&quot;y&quot;:</span> <span class="pre">4}</span></code> and an array <code class="docutils literal notranslate"><span class="pre">param</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(32,</span> <span class="pre">64,</span> <span class="pre">64,</span> <span class="pre">128)</span></code>. If we shard this array with <cite>jax.P(None, ‚Äúx‚Äù, ‚Äúy‚Äù, None) `, we end up with shards of size ``(32, 32, 16, 128)`</cite> distributed across the devices. The <code class="docutils literal notranslate"><span class="pre">None</span></code> indicates that an axis should not be sharded. JAX implicitly broadcasts trailing axes, so an identical sharding can be achieved more concisely with <cite>jax.P(None, ‚Äúx‚Äù, ‚Äúy‚Äù)</cite>. As a result, the shorthand for a fully replicated array (of any dimension) is <cite>jax.P()</cite>.</p>
</div>
<div class="admonition-example admonition">
<p class="admonition-title">Example</p>
<p>More advanced mesh geometries are convenient when aligned with the communication hierarchy of our devices. Host-to-host communication is typically slower than accelerator-to-accelerator communication. Suppose we have two host machines, each with eight attached GPUs. One might arrange the devices into a mesh of <code class="docutils literal notranslate"><span class="pre">{&quot;host&quot;:</span> <span class="pre">2,</span> <span class="pre">&quot;gpu&quot;:</span> <span class="pre">8}</span></code>. Then we can shard a parameter as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">param</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">256</span><span class="p">,</span> <span class="mi">192</span><span class="p">),</span> <span class="n">out_sharding</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
</pre></div>
</div>
<p>The whole of <code class="docutils literal notranslate"><span class="pre">param</span></code> will be replicated twice, but within each host, it will be spread across the eight locally attached GPUs, with each GPU storing a shard of shape <code class="docutils literal notranslate"><span class="pre">(32,</span> <span class="pre">192)</span></code> in HBM. This is particularly useful for <a class="reference internal" href="#fsdp-sharding"><span class="std std-ref">Fully-Sharded Data Parallel (FSDP)</span></a>.</p>
</div>
</section>
</section>
<section id="train-state-initialization">
<h2>Train State Initialization<a class="headerlink" href="#train-state-initialization" title="Link to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">init_train_state</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">dot_dict</span><span class="p">:</span>
  <span class="n">train_state</span> <span class="o">=</span> <span class="n">dot_dict</span><span class="p">()</span>
<span class="hll">  <span class="n">train_state</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">init_param_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</span>  <span class="n">train_state</span><span class="o">.</span><span class="n">opt</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">init_adam_state</span><span class="p">,</span> <span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">train_state</span>
</pre></div>
</div>
<p>Before we can get started, the first thing we need to do is set up the train state. The train state encapsulates (unsurprisingly) all the <em>stateful</em> aspects of the training process. This typically includes, at a minimum, the model parameters and the optimizer state. The way we have structured this function (though you may choose to do otherwise) is to:</p>
<ol class="arabic simple">
<li><p>Create a series of nested dictionaries to house the model parameters, and then</p></li>
<li><p><a class="reference internal" href="_autosummary/jax.tree.map.html#jax.tree.map" title="jax.tree.map"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.tree.map()</span></code></a> over those parameters to produce a similar set of nested dictionaries to house the accompanying optimizer states. (More on this <a class="reference external" href="#optimizer-initialization">below</a>.)</p></li>
</ol>
<section id="parameter-initialization">
<h3>Parameter Initialization<a class="headerlink" href="#parameter-initialization" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">init_train_state</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">dot_dict</span><span class="p">:</span>
  <span class="n">train_state</span> <span class="o">=</span> <span class="n">dot_dict</span><span class="p">()</span>
<span class="hll">  <span class="n">train_state</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">init_param_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</span>  <span class="n">train_state</span><span class="o">.</span><span class="n">opt</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">init_adam_state</span><span class="p">,</span> <span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">train_state</span>
</pre></div>
</div>
<p>To initialize our parameters, we build a series of nested dictionaries that correspond to the semantic sections of the neural network. If we were using a layer-based library such as PyTorch or Flax, these might correspond to neural network layers. For this example, we could, in fact, get by with a completely flattened dictionary, but the nested approach is convenient both for working with some of the APIs in JAX and for structuring our code.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">init_param_state</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">dot_dict</span><span class="p">:</span>
  <span class="n">root_key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">param_seed</span><span class="p">)</span>
  <span class="n">key</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">ft</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">fold_in</span><span class="p">,</span> <span class="n">root_key</span><span class="p">),</span> <span class="n">it</span><span class="o">.</span><span class="n">count</span><span class="p">())</span>
  <span class="n">zero_init</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
  <span class="n">he_init</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">he_normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="n">dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">dtype</span>

  <span class="n">params</span> <span class="o">=</span> <span class="n">dot_dict</span><span class="p">(</span>
    <span class="n">pos_embed</span><span class="o">=</span><span class="n">zero_init</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">key</span><span class="p">),</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">pos_embed</span><span class="p">),</span>
    <span class="n">layers</span><span class="o">=</span><span class="n">dot_dict</span><span class="p">(),</span>
  <span class="p">)</span>
  <span class="n">params</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">he_init</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">key</span><span class="p">),</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">embed</span><span class="p">)</span>
  <span class="n">params</span><span class="o">.</span><span class="n">linear_in</span> <span class="o">=</span> <span class="n">dot_dict</span><span class="p">(</span>
    <span class="n">kernel</span><span class="o">=</span><span class="n">he_init</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">key</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">in_kernel</span><span class="p">),</span>
    <span class="n">bias</span><span class="o">=</span><span class="n">zero_init</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">key</span><span class="p">),</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,),</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">in_bias</span><span class="p">),</span>
  <span class="p">)</span>
  <span class="n">params</span><span class="o">.</span><span class="n">linear_out</span> <span class="o">=</span> <span class="n">dot_dict</span><span class="p">(</span>
    <span class="n">kernel</span><span class="o">=</span><span class="n">he_init</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">key</span><span class="p">),</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">out_kernel</span><span class="p">),</span>
  <span class="p">)</span>
  <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
    <span class="n">qkv_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
    <span class="n">out_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>
    <span class="n">params</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">dot_dict</span><span class="p">(</span>
      <span class="n">attention</span><span class="o">=</span><span class="n">dot_dict</span><span class="p">(</span>
        <span class="n">qkv</span><span class="o">=</span><span class="n">he_init</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">key</span><span class="p">),</span> <span class="n">qkv_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">att_qkv</span><span class="p">),</span>
        <span class="n">out</span><span class="o">=</span><span class="n">he_init</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">key</span><span class="p">),</span> <span class="n">out_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">att_out</span><span class="p">),</span>
      <span class="p">),</span>
      <span class="n">mlp</span><span class="o">=</span><span class="n">dot_dict</span><span class="p">(</span>
        <span class="n">in_kernel</span><span class="o">=</span><span class="n">he_init</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">key</span><span class="p">),</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">mlp_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">mlp_in</span><span class="p">),</span>
        <span class="n">out_kernel</span><span class="o">=</span><span class="n">he_init</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">key</span><span class="p">),</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">mlp_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">mlp_out</span><span class="p">),</span>
      <span class="p">),</span>
    <span class="p">)</span>
  <span class="k">return</span> <span class="n">params</span>
</pre></div>
</div>
<p>Our <code class="docutils literal notranslate"><span class="pre">get_param_state</span></code> function makes use of the <code class="docutils literal notranslate"><span class="pre">constant</span></code> and <code class="docutils literal notranslate"><span class="pre">he_normal</span></code> factories provided in <a class="reference internal" href="jax.nn.initializers.html#module-jax.nn.initializers" title="jax.nn.initializers"><code class="xref py py-mod docutils literal notranslate"><span class="pre">jax.nn.initializers</span></code></a>. These factories return an <em>initializer</em>, which is a function conforming to the following protocol:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">Initializer</span><span class="p">(</span><span class="n">Protocol</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">out_sharding</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">:</span>
        <span class="o">...</span>
</pre></div>
</div>
<p>The functional flavor of JAX requires explicit handling of all stochasticity (viz. <a class="reference internal" href="random-numbers.html#pseudorandom-numbers"><span class="std std-ref">Pseudorandom numbers</span></a>), so we set up a little iterator that yields PRNG keys. Then, to build our parameters, we initialize them at their respective positions in the <code class="docutils literal notranslate"><span class="pre">params</span></code> nested dictionary, supplying the parameter shape, dtype, and sharding from the <code class="docutils literal notranslate"><span class="pre">Config</span></code> class.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>By specifying the shardings here, we initialize each shard of each parameter directly on the correct device in the device mesh where it needs to be, preventing the need for needless host-to-device transfers or, in the case of a model that does not fit in system memory, avoiding out-of-memory errors.</p>
</div>
</section>
<section id="optimizer-initialization">
<h3>Optimizer Initialization<a class="headerlink" href="#optimizer-initialization" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">init_train_state</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">dot_dict</span><span class="p">:</span>
  <span class="n">train_state</span> <span class="o">=</span> <span class="n">dot_dict</span><span class="p">()</span>
  <span class="n">train_state</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">init_param_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="hll">  <span class="n">train_state</span><span class="o">.</span><span class="n">opt</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">init_adam_state</span><span class="p">,</span> <span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
</span>  <span class="k">return</span> <span class="n">train_state</span>
</pre></div>
</div>
<p>When it comes to setting up the optimizer state, things are a little less straightforward than when we built the model parameters. The <a class="reference external" href="https://arxiv.org/abs/1412.6980">Adam optimizer</a> requires that, for each parameter, we keep track of three optimization states: <code class="docutils literal notranslate"><span class="pre">mu</span></code>, <code class="docutils literal notranslate"><span class="pre">nu</span></code>, and <code class="docutils literal notranslate"><span class="pre">count</span></code>. The simplest of these is <code class="docutils literal notranslate"><span class="pre">count</span></code>, which stores the number of training steps we have performed. This is just a scalar used to de-bias the Adam updates. The <code class="docutils literal notranslate"><span class="pre">mu</span></code> and <code class="docutils literal notranslate"><span class="pre">nu</span></code> states will be arrays of the same shape, dtype, and sharding as the accompanying parameter <code class="docutils literal notranslate"><span class="pre">param</span></code> <a class="footnote-reference brackets" href="#zeros-like" id="id3" role="doc-noteref"><span class="fn-bracket">[</span>2<span class="fn-bracket">]</span></a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">init_adam_state</span><span class="p">(</span><span class="n">param</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">dot_dict</span><span class="p">:</span>
  <span class="n">adam_state</span> <span class="o">=</span> <span class="n">dot_dict</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="n">nu</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="n">count</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">adam_state</span>
</pre></div>
</div>
<p>When we use <a class="reference internal" href="_autosummary/jax.tree.map.html#jax.tree.map" title="jax.tree.map"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.tree.map()</span></code></a>, it iterates over the items in <code class="docutils literal notranslate"><span class="pre">train_state.params</span></code>. For each parameter, it creates a corresponding Adam state, resulting in a new nested dictionary that mirrors the structure of <code class="docutils literal notranslate"><span class="pre">train_state.params</span></code>. Each leaf in this new structure contains the optimizer state for the corresponding parameter.</p>
</section>
</section>
<section id="the-train-step-functional-transformations">
<h2>The Train Step (Functional Transformations)<a class="headerlink" href="#the-train-step-functional-transformations" title="Link to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train_step</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">,</span> <span class="n">train_state</span><span class="p">:</span> <span class="n">dot_dict</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
  <span class="k">def</span><span class="w"> </span><span class="nf">loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model_apply</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;observed_ids&quot;</span><span class="p">])</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;target_ids&quot;</span><span class="p">],</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">labels</span> <span class="o">*</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

  <span class="n">params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">ref</span><span class="o">.</span><span class="n">get</span><span class="p">,</span> <span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
  <span class="n">loss</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)(</span><span class="n">params</span><span class="p">)</span>
  <span class="n">jax</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">ft</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">adam_update</span><span class="p">,</span> <span class="n">config</span><span class="p">),</span> <span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">train_state</span><span class="o">.</span><span class="n">opt</span><span class="p">)</span>
  <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;train_loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">}</span>
  <span class="k">return</span> <span class="n">metrics</span>
</pre></div>
</div>
<p>The train step is where we calculate the gradient of the model with respect to the current parameters and use the gradient, together with the optimizer, to update the parameters. To do this in JAX, we define the forward pass of the model, then we leverage JAX‚Äôs functional transformations to automatically generate the backward pass, which we use to calculate the gradients and perform the update.</p>
<section id="model-forward-pass">
<h3>Model Forward Pass<a class="headerlink" href="#model-forward-pass" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">model_apply</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">dot_dict</span><span class="p">,</span> <span class="n">tokens</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">:</span>
  <span class="n">out</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">tokens</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">act_seq</span><span class="p">)</span>
  <span class="n">out</span> <span class="o">+=</span> <span class="n">params</span><span class="o">.</span><span class="n">pos_embed</span>
  <span class="k">del</span> <span class="n">tokens</span>

  <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
    <span class="n">block</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span>
    <span class="n">att_skip</span> <span class="o">=</span> <span class="n">out</span>  <span class="c1"># 1 billion dollars in venture capital funding please</span>
    <span class="n">qkv</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bsd,3dkh-&gt;bs3kh&quot;</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">qkv</span><span class="p">,</span> <span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">act_att</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dot_product_attention</span><span class="p">(</span><span class="n">qkv</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">qkv</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">qkv</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">:],</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bskh,khd-&gt;bsd&quot;</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">out</span><span class="p">,</span> <span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">act_seq</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">+=</span> <span class="n">att_skip</span>
    <span class="n">out</span> <span class="o">*=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>

    <span class="n">mlp_skip</span> <span class="o">=</span> <span class="n">out</span>  <span class="c1"># machine learning circa 1986</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bsd,dh-&gt;bsh&quot;</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">mlp</span><span class="o">.</span><span class="n">in_kernel</span><span class="p">,</span> <span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">act_hidden</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bsh,hd-&gt;bsd&quot;</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">mlp</span><span class="o">.</span><span class="n">out_kernel</span><span class="p">,</span> <span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">act_seq</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">+=</span> <span class="n">mlp_skip</span>
    <span class="n">out</span> <span class="o">*=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>

  <span class="n">logits</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bsd,dl-&gt;bsl&quot;</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">params</span><span class="o">.</span><span class="n">linear_out</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> <span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">act_seq</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">logits</span>
</pre></div>
</div>
<p>The model‚Äôs forward pass is mostly unremarkable, aside from the <code class="docutils literal notranslate"><span class="pre">out_sharding</span></code> annotations we have supplied. These annotations declare what the result-sharding should be after the operation executes. The compiler uses these activation shardings, together with the parameter shardings we supplied when we <a class="reference external" href="#parameter-initialization">initialized the model</a>, to dynamically insert <a class="reference external" href="https://en.wikipedia.org/wiki/Collective_operation">communication collectives</a> that ferry parameters and activations alike between devices. By choosing a good sharding strategy, we can achieve highly performant training (and inference) code. We will cover some standard strategies that serve most use cases in the section titled <a class="reference internal" href="#achieving-high-performance"><span class="std std-ref">Achieving High Performance</span></a>. For a detailed discussion of the principles underpinning the design of sharding strategies, see <a class="reference external" href="https://jax-ml.github.io/scaling-book/">The Scaling Cookbook</a>.</p>
</section>
<section id="gradient-and-optimizer-update">
<h3>Gradient and Optimizer Update<a class="headerlink" href="#gradient-and-optimizer-update" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train_step</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">,</span> <span class="n">train_state</span><span class="p">:</span> <span class="n">dot_dict</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="hll">  <span class="k">def</span><span class="w"> </span><span class="nf">loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
</span><span class="hll">    <span class="n">logits</span> <span class="o">=</span> <span class="n">model_apply</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;observed_ids&quot;</span><span class="p">])</span>
</span><span class="hll">    <span class="n">labels</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;target_ids&quot;</span><span class="p">],</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span>
</span><span class="hll">    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">labels</span> <span class="o">*</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span>
  <span class="n">params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">ref</span><span class="o">.</span><span class="n">get</span><span class="p">,</span> <span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
  <span class="n">loss</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)(</span><span class="n">params</span><span class="p">)</span>
  <span class="n">jax</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">ft</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">adam_update</span><span class="p">,</span> <span class="n">config</span><span class="p">),</span> <span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">train_state</span><span class="o">.</span><span class="n">opt</span><span class="p">)</span>
  <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;train_loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">}</span>
  <span class="k">return</span> <span class="n">metrics</span>
</pre></div>
</div>
<p>In order to calculate the gradient, we define the training loss. This is a function of the parameters that returns a scalar which summarizes how well our model, with the current <code class="docutils literal notranslate"><span class="pre">train_state</span></code> parameters, is explaining the data.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">loss</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)(</span><span class="n">params</span><span class="p">)</span>
</pre></div>
</div>
<p>By supplying this function to <a class="reference internal" href="_autosummary/jax.value_and_grad.html#jax.value_and_grad" title="jax.value_and_grad"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.value_and_grad()</span></code></a>, we transform it into a function that returns both the scalar value and the gradient of <code class="docutils literal notranslate"><span class="pre">loss_fn</span></code> evaluated at <code class="docutils literal notranslate"><span class="pre">params</span></code> (the <em>value</em> and <em>grad</em>). Since we have defined our parameters in terms of a series of nested dictionaries, the gradient will also be a series of nested dictionaries, mirroring the parameters. Recall that, unlike the parameters, the optimizer states contain some extra, deeper nested dictionaries corresponding to the optimizer state per parameter. Take a moment, before reading the explanation, to ponder what the semantics of the following function call might be:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">jax</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">ft</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">adam_update</span><span class="p">,</span> <span class="n">config</span><span class="p">),</span> <span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">train_state</span><span class="o">.</span><span class="n">opt</span><span class="p">)</span>
</pre></div>
</div>
<p>Examining the call signature of the function <code class="docutils literal notranslate"><span class="pre">adam_apply</span></code> gives us a hint:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">adam_update</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">,</span> <span class="n">param</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">ArrayRef</span><span class="p">,</span> <span class="n">grad</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span> <span class="n">adam_state</span><span class="p">:</span> <span class="n">dot_dict</span><span class="p">):</span>
  <span class="n">adam_state</span><span class="o">.</span><span class="n">mu</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">config</span><span class="o">.</span><span class="n">beta_1</span><span class="p">)</span> <span class="o">*</span> <span class="n">adam_state</span><span class="o">.</span><span class="n">mu</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">+</span> <span class="n">config</span><span class="o">.</span><span class="n">beta_1</span> <span class="o">*</span> <span class="n">grad</span>
  <span class="n">adam_state</span><span class="o">.</span><span class="n">nu</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">config</span><span class="o">.</span><span class="n">beta_2</span><span class="p">)</span> <span class="o">*</span> <span class="n">adam_state</span><span class="o">.</span><span class="n">nu</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">+</span> <span class="n">config</span><span class="o">.</span><span class="n">beta_2</span> <span class="o">*</span> <span class="n">grad</span><span class="o">**</span><span class="mi">2</span>
  <span class="n">adam_state</span><span class="o">.</span><span class="n">count</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

  <span class="n">mu_hat</span> <span class="o">=</span> <span class="n">adam_state</span><span class="o">.</span><span class="n">mu</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">config</span><span class="o">.</span><span class="n">beta_1</span> <span class="o">**</span> <span class="n">adam_state</span><span class="o">.</span><span class="n">count</span><span class="p">[</span><span class="o">...</span><span class="p">])</span>
  <span class="n">nu_hat</span> <span class="o">=</span> <span class="n">adam_state</span><span class="o">.</span><span class="n">nu</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">config</span><span class="o">.</span><span class="n">beta_2</span> <span class="o">**</span> <span class="n">adam_state</span><span class="o">.</span><span class="n">count</span><span class="p">[</span><span class="o">...</span><span class="p">])</span>
  <span class="n">param</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">-=</span> <span class="n">config</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">mu_hat</span> <span class="o">/</span> <span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">nu_hat</span> <span class="o">+</span> <span class="n">config</span><span class="o">.</span><span class="n">eps_root</span><span class="p">)</span> <span class="o">+</span> <span class="n">config</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
</pre></div>
</div>
<p>Because <code class="docutils literal notranslate"><span class="pre">train_state.params</span></code> is the first argument, <a class="reference internal" href="_autosummary/jax.tree.map.html#jax.tree.map" title="jax.tree.map"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.tree.map()</span></code></a> uses its tree structure to guide the mapping process.[#prefix_tree]_ This means that <code class="docutils literal notranslate"><span class="pre">train_state.opt</span></code> is traversed only as deep as the leaves of <code class="docutils literal notranslate"><span class="pre">train_state.params</span></code>. The optimizer state for each parameter is therefore passed in as a complete subtree, which allows us to easily access all relevant states (like <code class="docutils literal notranslate"><span class="pre">mu</span></code> and <code class="docutils literal notranslate"><span class="pre">nu</span></code>) for a given <code class="docutils literal notranslate"><span class="pre">param</span></code> inside <code class="docutils literal notranslate"><span class="pre">adam_apply</span></code>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If we wished to use different optimization algorithms and states on different parameters in our model (or freeze some parameters), we could achieve this by modifying the body of <code class="docutils literal notranslate"><span class="pre">adam_apply</span></code> and replacing <a class="reference internal" href="_autosummary/jax.tree.map.html#jax.tree.map" title="jax.tree.map"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.tree.map()</span></code></a> with <a class="reference internal" href="_autosummary/jax.tree_util.tree_map_with_path.html#jax.tree_util.tree_map_with_path" title="jax.tree_util.tree_map_with_path"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.tree_util.tree_map_with_path()</span></code></a>, which allows the operand function to customize its behavior depending on the parameter.</p>
</div>
</section>
</section>
<section id="the-training-loop">
<h2>The Training Loop<a class="headerlink" href="#the-training-loop" title="Link to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">train_loop</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">):</span>
  <span class="n">record_writer</span> <span class="o">=</span> <span class="n">RecordWriter</span><span class="p">()</span>
  <span class="n">train_state</span> <span class="o">=</span> <span class="n">init_train_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
  <span class="n">train_state</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">ref</span><span class="o">.</span><span class="n">array_ref</span><span class="p">,</span> <span class="n">train_state</span><span class="p">)</span>
  <span class="n">batch</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">get_dataset_on_device</span><span class="p">(</span><span class="n">config</span><span class="p">))</span>
  <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_train_steps</span><span class="p">):</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">train_state</span><span class="p">,</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
    <span class="n">record_writer</span><span class="p">({</span><span class="s2">&quot;step&quot;</span><span class="p">:</span> <span class="n">step</span><span class="p">}</span> <span class="o">|</span> <span class="n">metrics</span><span class="p">)</span>
</pre></div>
</div>
<p>During training, we have to orchestrate the flow of data between two key players: the host system and the accelerator. Ensuring smooth interplay between these systems is key to writing highly performant training code. The Python <a class="reference external" href="https://en.wikipedia.org/wiki/Global_interpreter_lock">GIL</a> would ordinarily pose a significant obstacle here, but to work around this, the paradigm of <a class="reference internal" href="async_dispatch.html#async-dispatch"><span class="std std-ref">Asynchronous Dispatch</span></a> adopted by JAX makes this orchestration easy to accomplish. But, in order to leverage this paradigm, we need to be mindful of how our code will be executed when structuring our training step.</p>
<section id="efficiency-via-asynchronous-dispatch">
<h3>Efficiency via Asynchronous Dispatch<a class="headerlink" href="#efficiency-via-asynchronous-dispatch" title="Link to this heading">#</a></h3>
<p>One of the most important tasks performed by the host system is to fetch data and place it on the accelerators so that the accelerators are never waiting for data. The time when accelerators are waiting idle between train steps is referred to as the <em>step bubble</em>. We can leverage asynchronous dispatch to minimize the step bubble. Let‚Äôs see how this works with our training loop, discarding, for the moment, the line concerning the <code class="docutils literal notranslate"><span class="pre">record_writer</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_train_steps</span><span class="p">):</span>
  <span class="n">metrics</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">train_state</span><span class="p">,</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
</pre></div>
</div>
<p>When this code executes, Python will first query the range iterator, get <code class="docutils literal notranslate"><span class="pre">step</span></code> (with value <code class="docutils literal notranslate"><span class="pre">0</span></code>), then call <code class="docutils literal notranslate"><span class="pre">next(batch)</span></code>, which will take some time to retrieve the batch. Then, <code class="docutils literal notranslate"><span class="pre">train_step</span></code> gets called. So far, nothing out of the ordinary.</p>
<p>What happens next is interesting. Because <a class="reference internal" href="_autosummary/jax.jit.html#jax.jit" title="jax.jit"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.jit()</span></code></a>-decorated calls are non-blocking, the call to <code class="docutils literal notranslate"><span class="pre">train_step</span></code> returns to the Python interpreter immediately. While the computation is enqueued on the accelerator, no work is actually performed yet. The Python loop continues, advancing the step counter and calling <code class="docutils literal notranslate"><span class="pre">next(batch)</span></code> for the <em>next</em> iteration. Once the second call to <code class="docutils literal notranslate"><span class="pre">train_step</span></code> is made, its inputs are now the mutated reference to <code class="docutils literal notranslate"><span class="pre">train_state</span></code> from the previous JIT call and a fresh batch of data. The runtime is clever and sees that in order to execute the second call to <code class="docutils literal notranslate"><span class="pre">train_step</span></code>, we first need to realize the <code class="docutils literal notranslate"><span class="pre">train_state</span></code> result of step <code class="docutils literal notranslate"><span class="pre">0</span></code> to perform the mutation. And so it fires off the computation for the first step, and, crucially, while this happens, <code class="docutils literal notranslate"><span class="pre">train_step</span></code>, once again, returns immediately, and the loop skips over again. Python now runs ahead until it encounters the <code class="docutils literal notranslate"><span class="pre">next(batch)</span></code> function at step 3, which proceeds to execute in Python, loading data, <em>while</em> the first train step is executing (for real this time). And just like that, we can simultaneously load data and perform math on the accelerator, without any traditional multiprocessing. <a class="footnote-reference brackets" href="#sleep" id="id4" role="doc-noteref"><span class="fn-bracket">[</span>4<span class="fn-bracket">]</span></a></p>
<pre  class="mermaid">
        ---
displayMode: compact
---
gantt
    title Synchronous Dispatch: No Overlap
    axisFormat %

    section Host
    next(batch) :gb0, 0, 1000s
    next(batch) :gb1, after ajc0, 1000s
    next(batch) :gb2, after ajc1, 1000s

    section Accelerator

    train_step 0 :ajc0, after gb0, 2000s
    train_step 1 :ajc1, after gb1, 2000s
    </pre><pre  class="mermaid">
        ---
displayMode: compact
---
gantt
    title JAX Asynchronous Dispatch: Host-Device Overlap
    axisFormat %

    section Host
    %% Task: id, name, start, duration_or_end
    next(batch) :gb0, 0, 1000s
    next(batch) :gb1, after gb0, 1000s
    next(batch) :gb2, after gb1, 1000s
    next(batch) :gb3, after jc0, 1000s
    next(batch) :gb4, after jc1, 1000s

    section Accelerator
    %% Task: id, name, start, duration_or_end
    train_step 0 :jc0, after gb1, 2000s
    train_step 1 :jc1, after jc0, 2000s
    train_step 2 :jc2, after jc1, 2000s
    </pre></section>
<section id="common-mistakes">
<h3>Common Mistakes<a class="headerlink" href="#common-mistakes" title="Link to this heading">#</a></h3>
<p>When writing asynchronous dispatch code in Python, there are two primary mistakes one should be wary of so as not to interrupt our careful orchestration of compute.</p>
<section id="requesting-device-to-host-transfers">
<h4>Requesting device-to-host transfers<a class="headerlink" href="#requesting-device-to-host-transfers" title="Link to this heading">#</a></h4>
<p>Up until now, we have ignored what happens to the variable <code class="docutils literal notranslate"><span class="pre">metrics</span></code>. Indeed, if this is left dangling, nothing will happen, and we will achieve good overlap just as advertised. However, more often than not, we would like to observe telemetry from our train step, such as the current loss, gradient statistics, and so on. Suppose we were to insert code such as:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">metrics</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">train_state</span><span class="p">,</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
<span class="nb">print</span><span class="p">({</span><span class="s2">&quot;step&quot;</span><span class="p">:</span> <span class="n">step</span><span class="p">}</span> <span class="o">|</span> <span class="n">metrics</span><span class="p">)</span>
</pre></div>
</div>
<p>Instead of the loop ticking over, <code class="docutils literal notranslate"><span class="pre">print</span></code> will incur a device-to-host transfer of whatever on-device arrays are in <code class="docutils literal notranslate"><span class="pre">metrics</span></code>. This interrupts the Python interpreter, and the code is forced to execute synchronously, producing a step bubble. The solution is slightly counterintuitive: at each step, we gather the telemetry for the <em>previous</em> step.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span><span class="w"> </span><span class="nc">RecordWriter</span><span class="p">:</span>
  <span class="n">prev_metrics</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cur_metrics</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prev_metrics</span><span class="p">,</span> <span class="n">log_metrics</span> <span class="o">=</span> <span class="n">cur_metrics</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prev_metrics</span>
    <span class="k">if</span> <span class="n">log_metrics</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span>
    <span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="n">it</span><span class="o">.</span><span class="n">starmap</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">,</span> <span class="n">log_metrics</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">sep</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>and</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">metrics</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">train_state</span><span class="p">,</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
</pre></div>
</div>
<p>A small helper function like this is essential to achieve good overlap and make the most of the resources of our host system and our accelerator. Of course, the simple <code class="docutils literal notranslate"><span class="pre">print</span></code> statement here can be swapped out for any Python operation that requests data from the accelerator.</p>
</section>
<section id="interrupting-the-accelerator">
<h4>Interrupting the accelerator<a class="headerlink" href="#interrupting-the-accelerator" title="Link to this heading">#</a></h4>
<p>The other common way in which we can waste spectacular amounts of cloud compute money is by unintentionally enqueuing math operations on the accelerator outside of the train step. Suppose we are using a cosine learning rate schedule.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">learning_rate</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">init_value</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="n">decay_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">):</span>
    <span class="n">cosine_decay</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">decay_steps</span><span class="p">)</span> <span class="o">/</span> <span class="n">decay_steps</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">init_value</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">cosine_decay</span>
</pre></div>
</div>
<p>A common pattern is to want to visualize the schedule alongside the other metrics we‚Äôre gathering. However, even if we use the clever <code class="docutils literal notranslate"><span class="pre">record_writer</span></code> class we defined earlier, the following code will create a bubble on the accelerator.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">metrics</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">train_state</span><span class="p">,</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
<span class="n">record_writer</span><span class="p">({</span><span class="s2">&quot;step&quot;</span><span class="p">:</span> <span class="n">step</span><span class="p">,</span> <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="n">learning_rate</span><span class="p">(</span><span class="n">step</span><span class="p">)}</span> <span class="o">|</span> <span class="n">metrics</span><span class="p">)</span>
</pre></div>
</div>
<p>This is because we have used <a class="reference internal" href="jax.numpy.html#module-jax.numpy" title="jax.numpy"><code class="xref py py-mod docutils literal notranslate"><span class="pre">jax.numpy</span></code></a> in our calculations. When <a class="reference internal" href="_autosummary/jax.numpy.minimum.html#jax.numpy.minimum" title="jax.numpy.minimum"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.numpy.minimum()</span></code></a> is called, the Python integer <code class="docutils literal notranslate"><span class="pre">step</span></code> is promoted to a <a class="reference internal" href="_autosummary/jax.Array.html#jax.Array" title="jax.Array"><code class="xref py py-class docutils literal notranslate"><span class="pre">jax.Array</span></code></a> and transferred to the accelerator (a host-to-device transfer). The calculation is now enqueued on the accelerator, outside our main <code class="docutils literal notranslate"><span class="pre">train_step</span></code>. To <code class="docutils literal notranslate"><span class="pre">print</span></code> the result, the value must be transferred back to the host (a device-to-host transfer). This round-trip forces the accelerator to synchronize with the host, and we have thrown away money by creating a performance bubble. The two ways to avoid this are to use NumPy for these calculations or to use the <a class="reference internal" href="_autosummary/jax.default_device.html#jax.default_device" title="jax.default_device"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.default_device()</span></code></a> context manager.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">metrics</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">train_state</span><span class="p">,</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
<span class="k">with</span> <span class="n">jax</span><span class="o">.</span><span class="n">default_device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">):</span>
  <span class="n">record_writer</span><span class="p">({</span><span class="s2">&quot;step&quot;</span><span class="p">:</span> <span class="n">step</span><span class="p">,</span> <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="n">learning_rate</span><span class="p">(</span><span class="n">step</span><span class="p">)}</span> <span class="o">|</span> <span class="n">metrics</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="data-loading">
<h3>Data Loading<a class="headerlink" href="#data-loading" title="Link to this heading">#</a></h3>
<p>In addition to overlapping the actual loading of the data (that is, retrieving it from network storage to the host), JAX also allows us to overlap the host-to-device transfer of the data itself with the computation of the train step. The special function <a class="reference internal" href="_autosummary/jax.device_put.html#jax.device_put" title="jax.device_put"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.device_put()</span></code></a> is carefully designed to be non-blocking, executing asynchronously, which makes it perfectly fine to use in the context of our train step. However, there is a more convenient function specifically designed for the task of loading data. In the following code, <code class="docutils literal notranslate"><span class="pre">dataset</span></code> is an ordinary Python iterator that yields a <code class="docutils literal notranslate"><span class="pre">dict</span></code> of batched data. By mapping over this iterator with <a class="reference internal" href="_autosummary/jax.make_array_from_process_local_data.html#jax.make_array_from_process_local_data" title="jax.make_array_from_process_local_data"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.make_array_from_process_local_data()</span></code></a>, we generate a new iterator. Yielding from this new iterator will generate data placed on the device, ready for consumption by our train step. Internally, it will <a class="reference internal" href="_autosummary/jax.tree.map.html#jax.tree.map" title="jax.tree.map"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.tree.map()</span></code></a> to create <a class="reference internal" href="_autosummary/jax.Array.html#jax.Array" title="jax.Array"><code class="xref py py-class docutils literal notranslate"><span class="pre">jax.Array</span></code></a> objects and queue them to be transferred to the device. Provided the data can be batched fast enough, on both TPUs and GPUs, these transfers will be overlapped with the train step computation.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">get_dataset_on_device</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">]]:</span>
  <span class="n">datset</span> <span class="o">=</span> <span class="n">get_dataset</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
  <span class="n">sharding</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">mesh_axis_names</span><span class="p">)</span>
  <span class="k">return</span> <span class="nb">map</span><span class="p">(</span><span class="n">ft</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">make_array_from_process_local_data</span><span class="p">,</span> <span class="n">sharding</span><span class="p">),</span> <span class="n">datset</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="achieving-high-performance">
<span id="id5"></span><h2>Achieving High Performance<a class="headerlink" href="#achieving-high-performance" title="Link to this heading">#</a></h2>
<p>In this section, we will describe the three primary forms of model parallelism that are useful for training. During training, <em>throughput</em> is of paramount importance; that is, we wish to maximize the average number of operations per second. This contrasts with inference, where the goal is to minimize <em>latency</em> by ensuring all the operations happen in as little time as possible. Keeping throughput in mind as our ultimate goal for training, this section introduces the three primary strategies for sharding during training. For each strategy, we outline the JAX shardings that implement it and describe the collectives involved so that when studying program traces, you‚Äôll have landmarks to look for to confirm that the program is behaving as expected. The sharding variables we define in the code blocks below correspond to their uses in the <a class="reference external" href="#train-state-initialization">initialization</a> and <a class="reference external" href="#model-forward-pass">model forward pass</a>. But in the companion script these and other aspects of the training code are set conveniently using the global <cite>Config</cite> class.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">register_static</span>
<span class="nd">@dataclass</span><span class="p">(</span><span class="n">kw_only</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">frozen</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Config</span><span class="p">:</span>
  <span class="n">mesh_axis_names</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,)</span>
  <span class="n">mesh_shape</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,)</span>
  <span class="n">seq_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span>

  <span class="n">num_train_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="o">**</span><span class="mi">6</span>
  <span class="n">host_batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">16</span>
  <span class="n">learning_rate</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-4</span>
  <span class="n">beta_1</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.9</span>
  <span class="n">beta_2</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.999</span>
  <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-8</span>
  <span class="n">eps_root</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span>

  <span class="n">param_seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">12738</span>
  <span class="n">num_layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span>
  <span class="n">embed_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span>
  <span class="n">mlp_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span> <span class="o">*</span> <span class="mi">4</span>
  <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2</span><span class="o">**</span><span class="mi">8</span>  <span class="c1"># uint8 ascii encoding</span>
  <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span>
  <span class="n">head_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">128</span>
  <span class="n">dtype</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;bfloat16&quot;</span>

  <span class="n">embed</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">pos_embed</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">att_qkv</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">att_out</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">mlp_in</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">mlp_out</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">)</span>
  <span class="n">in_kernel</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">in_bias</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
  <span class="n">out_kernel</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">out_bias</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>

  <span class="n">act_ids</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">)</span>
  <span class="n">act_seq</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">act_att</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
  <span class="n">act_hidden</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

  <span class="k">def</span><span class="w"> </span><span class="nf">__post_init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="n">mesh</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">make_mesh</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mesh_shape</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">mesh_axis_names</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mesh_shape</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">AxisType</span><span class="o">.</span><span class="n">Explicit</span><span class="p">,))</span>
    <span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">set_mesh</span><span class="p">(</span><span class="n">mesh</span><span class="p">)</span>
</pre></div>
</div>
<section id="data-parallel">
<h3>Data Parallel<a class="headerlink" href="#data-parallel" title="Link to this heading">#</a></h3>
<p>Data parallel is the most common and easy-to-understand form of parallelism. In this scheme, each accelerator stores a complete copy of the model parameters, and we shard activations along the batch axis to split the computation of the gradients. To compute the gradients, each accelerator performs an individual forward and backward pass. Then, before the parameters are updated, XLA inserts an <code class="docutils literal notranslate"><span class="pre">AllReduce</span></code> to share the updates and keep the models in sync.</p>
<p><em>Mesh:</em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mesh</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">Mesh</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">(),</span> <span class="p">(</span><span class="s1">&#39;devices&#39;</span><span class="p">,))</span>
</pre></div>
</div>
<p><em>Parameter Shardings:</em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pos_embed</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">att_qkv</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">att_out</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">mlp_in</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">mlp_out</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">in_kernel</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">in_bias</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
<span class="n">out_kernel</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">out_bias</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<p><em>Activation Shardings:</em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">act_ids</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;devices&quot;</span><span class="p">)</span>
<span class="n">act_seq</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;devices&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">act_att</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;devices&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">act_hidden</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;devices&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="fully-sharded-data-parallel-fsdp">
<span id="fsdp-sharding"></span><h3>Fully-Sharded Data Parallel (FSDP)<a class="headerlink" href="#fully-sharded-data-parallel-fsdp" title="Link to this heading">#</a></h3>
<p>The drawback of data-parallel sharding is that we have to keep multiple, full, redundant copies of the model parameters in HBM. This is a very performant strategy for small models, but since HBM is in short supply, we need to shard the model parameters as well. In the <em>Fully-Sharded Data Parallel (FSDP)</em> strategy, we shard both the model and the parameters. Now, as the forward pass happens, the parameters are, one-by-one, unsharded (via <code class="docutils literal notranslate"><span class="pre">AllGather</span></code>) into whole arrays before they are applied to the activations. This unsharding is brief and temporary, however, leading to a large saving in HBM. In the backward pass, each <code class="docutils literal notranslate"><span class="pre">AllGather</span></code> becomes a <code class="docutils literal notranslate"><span class="pre">ReduceScatter</span></code>. Then there is a final <code class="docutils literal notranslate"><span class="pre">ReduceScatter</span></code> at the optimizer update to synchronize gradients. Compared with Data parallelism, the total communication traffic is 50% highter, but we our HBM pressure is reduced by the size of the model divided by the number of devices.</p>
<p><em>Mesh:</em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mesh</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">Mesh</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">(),</span> <span class="p">(</span><span class="s1">&#39;fsdp&#39;</span><span class="p">,))</span>
</pre></div>
</div>
<p><em>Parameter Shardings:</em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pos_embed</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">att_qkv</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">att_out</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">mlp_in</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">mlp_out</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">)</span>
<span class="n">in_kernel</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">in_bias</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
<span class="n">out_kernel</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">out_bias</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<p><em>Activation Shardings:</em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">act_ids</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">)</span>
<span class="n">act_seq</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">act_att</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">act_hidden</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>While FSDP entails a great deal more communication than data parallel, in practice we are able to overlap the communication with the compute, thereby hiding it and achieving the same throughput at a drastically improved HBM budget.</p>
</div>
</section>
<section id="tensor-parallel">
<h3>Tensor Parallel<a class="headerlink" href="#tensor-parallel" title="Link to this heading">#</a></h3>
<p>If our model is large enough and structured appropriately, it becomes beneficial to partition the computation within a single example across our accelerators. Using a matrix multiplication as an example, we can spread the large matrix multiplications over two or four accelerators. This entails significantly more communication, and so this strategy only works for computations with a very high arithmetic intensity, such as extremely large matrix multiplications. With multi-head self-attention, we opt to shard along the heads with a replicated sequence axis, since this offers the most natural amount of parallelism. If the MLP is large enough we can also efficiently shard the matrix multiplications.</p>
<p><em>Mesh:</em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mesh</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">Mesh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">())</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">))</span>
</pre></div>
</div>
<p><em>Parameter Shardings:</em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pos_embed</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
<span class="n">att_qkv</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">att_out</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">mlp_in</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
<span class="n">mlp_out</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">)</span>
<span class="n">in_kernel</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">in_bias</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
<span class="n">out_kernel</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">out_bias</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<p><em>Activation Shardings:</em></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">act_ids</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">)</span>
<span class="n">act_seq</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">act_att</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">act_hidden</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
</pre></div>
</div>
<aside class="footnote-list brackets">
<aside class="footnote brackets" id="ordered" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">1</a><span class="fn-bracket">]</span></span>
<p>Of course, all dictionaries are order-preserving in modern Python, so this is somewhat redundant.</p>
</aside>
<aside class="footnote brackets" id="zeros-like" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">2</a><span class="fn-bracket">]</span></span>
<p>This is accomplished by using the <code class="docutils literal notranslate"><span class="pre">zeros_like</span></code> constructor, but we could have specified the sharding manually using the <code class="docutils literal notranslate"><span class="pre">devices</span></code> argument of many of the <a class="reference internal" href="jax.numpy.html#module-jax.numpy" title="jax.numpy"><code class="xref py py-mod docutils literal notranslate"><span class="pre">jax.numpy</span></code></a> functions.</p>
</aside>
<aside class="footnote brackets" id="prefix-tree" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span>3<span class="fn-bracket">]</span></span>
<p>We could have achieved the same behavior equivalently by ordering <code class="docutils literal notranslate"><span class="pre">grad</span></code> first.</p>
</aside>
<aside class="footnote brackets" id="sleep" role="doc-footnote">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">4</a><span class="fn-bracket">]</span></span>
<p>For the purposes of this explanation, you can think of <code class="docutils literal notranslate"><span class="pre">next(batch)</span></code> as just a sleep.</p>
</aside>
</aside>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="distributed_data_loading.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Distributed data loading</p>
      </div>
    </a>
    <a class="right-next"
       href="notebooks/autodiff_cookbook.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">The Autodiff Cookbook</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#device-mesh-and-shardings">Device Mesh and Shardings</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#device-mesh">Device Mesh</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-state-initialization">Train State Initialization</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-initialization">Parameter Initialization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizer-initialization">Optimizer Initialization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-train-step-functional-transformations">The Train Step (Functional Transformations)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-forward-pass">Model Forward Pass</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-and-optimizer-update">Gradient and Optimizer Update</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-training-loop">The Training Loop</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#efficiency-via-asynchronous-dispatch">Efficiency via Asynchronous Dispatch</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-mistakes">Common Mistakes</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#requesting-device-to-host-transfers">Requesting device-to-host transfers</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#interrupting-the-accelerator">Interrupting the accelerator</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-loading">Data Loading</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#achieving-high-performance">Achieving High Performance</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-parallel">Data Parallel</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fully-sharded-data-parallel-fsdp">Fully-Sharded Data Parallel (FSDP)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-parallel">Tensor Parallel</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The JAX authors
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2024, The JAX Authors.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>