
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>JAX Memories and Host Offloading &#8212; JAX  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css?v=17fd2dad" />
    <link rel="stylesheet" href="../_static/style.css" type="text/css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=30646c52"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/host-offloading';</script>
    <link rel="icon" href="../_static/favicon.png"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Introduction to multi-controller JAX (aka multi-process/multi-host JAX)" href="../multi_process.html" />
    <link rel="prev" title="Device-local array layout control" href="layout.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/jax_logo_250px.png" class="logo__image only-light" alt="JAX  documentation - Home"/>
    <script>document.write(`<img src="../_static/jax_logo_250px.png" class="logo__image only-dark" alt="JAX  documentation - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="thinking_in_jax.html">Quickstart: How to think in JAX</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Common_Gotchas_in_JAX.html">üî™ JAX - The Sharp Bits üî™</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../jax-101.html">JAX 101</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../jit-compilation.html">Just-in-time compilation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../automatic-vectorization.html">Automatic vectorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../automatic-differentiation.html">Automatic differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../working-with-pytrees.html">Working with pytrees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../random-numbers.html">Pseudorandom numbers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../control-flow.html">Control flow and logical operators with JIT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stateful-computations.html">Stateful computations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../sharded-computation.html">Introduction to parallel programming</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Resources, guides, and references</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../key-concepts.html">Key concepts</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../advanced_guides.html">Resources and Advanced Guides</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="Distributed_arrays_and_automatic_parallelization.html">Distributed arrays and automatic parallelization</a></li>
<li class="toctree-l2"><a class="reference internal" href="explicit-sharding.html">Explicit sharding (a.k.a. ‚Äúsharding in types‚Äù)</a></li>
<li class="toctree-l2"><a class="reference internal" href="shard_map.html">Manual parallelism with <code class="docutils literal notranslate"><span class="pre">shard_map</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="layout.html">Device-local array layout control</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">JAX Memories and Host Offloading</a></li>

<li class="toctree-l2"><a class="reference internal" href="../multi_process.html">Introduction to multi-controller JAX (aka multi-process/multi-host JAX)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../distributed_data_loading.html">Distributed data loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../the-training-cookbook.html">The Training Cookbook</a></li>
<li class="toctree-l2"><a class="reference internal" href="autodiff_cookbook.html">The Autodiff Cookbook</a></li>
<li class="toctree-l2"><a class="reference internal" href="Custom_derivative_rules_for_Python_code.html">Custom derivative rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="autodiff_remat.html">Control autodiff‚Äôs saved values with <code class="docutils literal notranslate"><span class="pre">jax.checkpoint</span></code> (aka <code class="docutils literal notranslate"><span class="pre">jax.remat</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced-autodiff.html">Advanced automatic differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../errors.html">Errors</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../debugging.html">Introduction to debugging</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../debugging/print_breakpoint.html">Compiled prints and breakpoints</a></li>
<li class="toctree-l3"><a class="reference internal" href="../debugging/checkify_guide.html">The <code class="docutils literal notranslate"><span class="pre">checkify</span></code> transformation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../debugging/flags.html">JAX debugging flags</a></li>
<li class="toctree-l2"><a class="reference internal" href="../transfer_guard.html">Transfer guard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytrees.html">Pytrees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../persistent_compilation_cache.html">Persistent compilation cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu_performance_tips.html">GPU performance tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="../profiling.html">Profiling computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../device_memory_profiling.html">Profiling device memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../array_refs.html"><code class="docutils literal notranslate"><span class="pre">ArrayRef</span></code>: mutable arrays for data plumbing and memory control</a></li>
<li class="toctree-l2"><a class="reference internal" href="../external-callbacks.html">External callbacks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ffi.html">Foreign function interface (FFI)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gradient-checkpointing.html">Gradient checkpointing with <code class="docutils literal notranslate"><span class="pre">jax.checkpoint</span></code> (<code class="docutils literal notranslate"><span class="pre">jax.remat</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aot.html">Ahead-of-time lowering and compilation</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../export/index.html">Exporting and serialization</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../export/export.html">Exporting and serializing staged-out computations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../export/shape_poly.html">Shape polymorphism</a></li>
<li class="toctree-l3"><a class="reference internal" href="../export/jax2tf.html">Interoperation with TensorFlow</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../pallas/index.html">Pallas: a JAX kernel language</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../pallas/quickstart.html">Pallas Quickstart</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pallas/pipelining.html">Software Pipelining</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pallas/grid_blockspec.html">Grids and BlockSpecs</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../pallas/tpu/index.html">Pallas TPU</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../pallas/tpu/details.html">Writing TPU kernels with Pallas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../pallas/tpu/pipelining.html">TPU Pipelining</a></li>
<li class="toctree-l4"><a class="reference internal" href="../pallas/tpu/matmul.html">Matrix Multiplication</a></li>
<li class="toctree-l4"><a class="reference internal" href="../pallas/tpu/sparse.html">Scalar Prefetch and Block-Sparse Computation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../pallas/tpu/distributed.html">Distributed Computing in Pallas for TPUs</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../pallas/gpu/index.html">Pallas:Mosaic GPU</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../pallas/gpu/reference.html">Writing Mosaic GPU kernels with Pallas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../pallas/gpu/pipelining.html">Mosaic GPU Pipelining</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../pallas/design/index.html">Pallas Design Notes</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../pallas/design/design.html">Pallas Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="../pallas/design/async_note.html">Pallas Async Operations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../pallas/CHANGELOG.html">Pallas Changelog</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="neural_network_with_tfds_data.html">Training a simple neural network, with tensorflow/datasets data loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="Neural_Network_and_Data_Loading.html">Training a simple neural network, with PyTorch data loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="vmapped_log_probs.html">Autobatching for Bayesian inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="convolutions.html">Generalized convolutions in JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../xla_flags.html">XLA compiler flags</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax-primitives.html">JAX Internals: primitives</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jaxpr.html">JAX internals: The jaxpr language</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../contributor_guide.html">Developer notes</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html">Contributing to JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../developer.html">Building from source</a></li>
<li class="toctree-l2"><a class="reference internal" href="../investigating_a_regression.html">Investigating a regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autodidax.html">Autodidax: JAX core from scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autodidax2_part1.html">Autodidax2, part 1: JAX from scratch, again</a></li>

<li class="toctree-l2 has-children"><a class="reference internal" href="../jep/index.html">JAX Enhancement Proposals (JEPs)</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../jep/263-prng.html">263: JAX PRNG Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/2026-custom-derivatives.html">2026: Custom JVP/VJP rules for JAX-transformable functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/4008-custom-vjp-update.html">4008: Custom VJP and `nondiff_argnums` update</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/4410-omnistaging.html">4410: Omnistaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/9263-typed-keys.html">9263: Typed keys &amp; pluggable RNGs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/9407-type-promotion.html">9407: Design of Type Promotion Semantics for JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/9419-jax-versioning.html">9419: Jax and Jaxlib versioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/10657-sequencing-effects.html">10657: Sequencing side-effects in JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/11830-new-remat-checkpoint.html">11830: `jax.remat` / `jax.checkpoint` new implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/12049-type-annotations.html">12049: Type Annotation Roadmap for JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/14273-shard-map.html">14273: `shard_map` (`shmap`) for simple per-device code</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/15856-jex.html">15856: `jax.extend`, an extensions module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/17111-shmap-transpose.html">17111: Efficient transposition of `shard_map` (and other maps)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/18137-numpy-scipy-scope.html">18137: Scope of JAX NumPy &amp; SciPy Wrappers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/25516-effver.html">25516: Effort-based versioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/28661-jax-array-protocol.html">28661: Supporting the `__jax_array__` protocol</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../internals/index.html">JAX Internal Implementation Notes</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../internals/constants.html">Handling of closed-over constants</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../extensions.html">Extension guides</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="Writing_custom_interpreters_in_Jax.html">Writing custom Jaxpr interpreters in JAX</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../jax.extend.html"><code class="docutils literal notranslate"><span class="pre">jax.extend</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../jax.extend.core.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.core</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.extend.linear_util.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.linear_util</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.extend.mlir.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.mlir</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.extend.random.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.random</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../building_on_jax.html">Building on JAX</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../notes.html">Notes</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../api_compatibility.html">API compatibility</a></li>
<li class="toctree-l2"><a class="reference internal" href="../deprecation.html">Python and NumPy version support policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../async_dispatch.html">Asynchronous dispatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../concurrency.html">Concurrency</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu_memory_allocation.html">GPU memory allocation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rank_promotion_warning.html">Rank promotion warning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../type_promotion.html">Type promotion semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../default_dtypes.html">Default dtypes and the X64 flag</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../jax.html">Public API: <code class="docutils literal notranslate"><span class="pre">jax</span></code> package</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../jax.numpy.html"><code class="docutils literal notranslate"><span class="pre">jax.numpy</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.fft.html">jax.numpy.fft.fft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.fft2.html">jax.numpy.fft.fft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.fftfreq.html">jax.numpy.fft.fftfreq</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.fftn.html">jax.numpy.fft.fftn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.fftshift.html">jax.numpy.fft.fftshift</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.hfft.html">jax.numpy.fft.hfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.ifft.html">jax.numpy.fft.ifft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.ifft2.html">jax.numpy.fft.ifft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.ifftn.html">jax.numpy.fft.ifftn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.ifftshift.html">jax.numpy.fft.ifftshift</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.ihfft.html">jax.numpy.fft.ihfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.irfft.html">jax.numpy.fft.irfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.irfft2.html">jax.numpy.fft.irfft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.irfftn.html">jax.numpy.fft.irfftn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.rfft.html">jax.numpy.fft.rfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.rfft2.html">jax.numpy.fft.rfft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.rfftfreq.html">jax.numpy.fft.rfftfreq</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.rfftn.html">jax.numpy.fft.rfftn</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../jax.scipy.html"><code class="docutils literal notranslate"><span class="pre">jax.scipy</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.scipy.stats.bernoulli.logpmf.html">jax.scipy.stats.bernoulli.logpmf</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.scipy.stats.bernoulli.pmf.html">jax.scipy.stats.bernoulli.pmf</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.scipy.stats.bernoulli.cdf.html">jax.scipy.stats.bernoulli.cdf</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.scipy.stats.bernoulli.ppf.html">jax.scipy.stats.bernoulli.ppf</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../jax.lax.html"><code class="docutils literal notranslate"><span class="pre">jax.lax</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.random.html"><code class="docutils literal notranslate"><span class="pre">jax.random</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.sharding.html"><code class="docutils literal notranslate"><span class="pre">jax.sharding</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.debug.html"><code class="docutils literal notranslate"><span class="pre">jax.debug</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.dlpack.html"><code class="docutils literal notranslate"><span class="pre">jax.dlpack</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.distributed.html"><code class="docutils literal notranslate"><span class="pre">jax.distributed</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.dtypes.html"><code class="docutils literal notranslate"><span class="pre">jax.dtypes</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.ffi.html"><code class="docutils literal notranslate"><span class="pre">jax.ffi</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.flatten_util.html"><code class="docutils literal notranslate"><span class="pre">jax.flatten_util</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.image.html"><code class="docutils literal notranslate"><span class="pre">jax.image</span></code> module</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../jax.nn.html"><code class="docutils literal notranslate"><span class="pre">jax.nn</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../jax.nn.initializers.html"><code class="docutils literal notranslate"><span class="pre">jax.nn.initializers</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../jax.ops.html"><code class="docutils literal notranslate"><span class="pre">jax.ops</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.profiler.html"><code class="docutils literal notranslate"><span class="pre">jax.profiler</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.ref.html"><code class="docutils literal notranslate"><span class="pre">jax.ref</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.stages.html"><code class="docutils literal notranslate"><span class="pre">jax.stages</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.test_util.html"><code class="docutils literal notranslate"><span class="pre">jax.test_util</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.tree.html"><code class="docutils literal notranslate"><span class="pre">jax.tree</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.tree_util.html"><code class="docutils literal notranslate"><span class="pre">jax.tree_util</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.typing.html"><code class="docutils literal notranslate"><span class="pre">jax.typing</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.export.html"><code class="docutils literal notranslate"><span class="pre">jax.export</span></code> module</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../jax.extend.html"><code class="docutils literal notranslate"><span class="pre">jax.extend</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../jax.extend.core.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.core</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.extend.linear_util.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.linear_util</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.extend.mlir.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.mlir</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.extend.random.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.random</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../jax.example_libraries.html"><code class="docutils literal notranslate"><span class="pre">jax.example_libraries</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../jax.example_libraries.optimizers.html"><code class="docutils literal notranslate"><span class="pre">jax.example_libraries.optimizers</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.example_libraries.stax.html"><code class="docutils literal notranslate"><span class="pre">jax.example_libraries.stax</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../jax.experimental.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../jax.experimental.checkify.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.checkify</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.experimental.compilation_cache.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.compilation_cache</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.experimental.custom_dce.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.custom_dce</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.experimental.custom_partitioning.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.custom_partitioning</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.experimental.jet.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.jet</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.experimental.key_reuse.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.key_reuse</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.experimental.mesh_utils.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.mesh_utils</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.experimental.multihost_utils.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.multihost_utils</span></code> module</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../jax.experimental.pallas.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pallas</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-22"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../jax.experimental.pallas.mosaic_gpu.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pallas.mosaic_gpu</span></code> module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../jax.experimental.pallas.triton.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pallas.triton</span></code> module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../jax.experimental.pallas.tpu.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pallas.tpu</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../jax.experimental.pjit.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pjit</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.experimental.serialize_executable.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.serialize_executable</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.experimental.shard_map.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.shard_map</span></code> module</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../jax.experimental.sparse.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.sparse</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-23"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.BCOO.html">jax.experimental.sparse.BCOO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_broadcast_in_dim.html">jax.experimental.sparse.bcoo_broadcast_in_dim</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_concatenate.html">jax.experimental.sparse.bcoo_concatenate</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_dot_general.html">jax.experimental.sparse.bcoo_dot_general</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_dot_general_sampled.html">jax.experimental.sparse.bcoo_dot_general_sampled</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_dynamic_slice.html">jax.experimental.sparse.bcoo_dynamic_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_extract.html">jax.experimental.sparse.bcoo_extract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_fromdense.html">jax.experimental.sparse.bcoo_fromdense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_gather.html">jax.experimental.sparse.bcoo_gather</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_multiply_dense.html">jax.experimental.sparse.bcoo_multiply_dense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_multiply_sparse.html">jax.experimental.sparse.bcoo_multiply_sparse</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_update_layout.html">jax.experimental.sparse.bcoo_update_layout</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_reduce_sum.html">jax.experimental.sparse.bcoo_reduce_sum</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_reshape.html">jax.experimental.sparse.bcoo_reshape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_slice.html">jax.experimental.sparse.bcoo_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_sort_indices.html">jax.experimental.sparse.bcoo_sort_indices</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_squeeze.html">jax.experimental.sparse.bcoo_squeeze</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_sum_duplicates.html">jax.experimental.sparse.bcoo_sum_duplicates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_todense.html">jax.experimental.sparse.bcoo_todense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_transpose.html">jax.experimental.sparse.bcoo_transpose</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../jax.lib.html"><code class="docutils literal notranslate"><span class="pre">jax.lib</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.addressable_shards.html">jax.Array.addressable_shards</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.all.html">jax.Array.all</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.any.html">jax.Array.any</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.argmax.html">jax.Array.argmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.argmin.html">jax.Array.argmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.argpartition.html">jax.Array.argpartition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.argsort.html">jax.Array.argsort</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.astype.html">jax.Array.astype</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.at.html">jax.Array.at</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.choose.html">jax.Array.choose</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.clip.html">jax.Array.clip</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.compress.html">jax.Array.compress</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.committed.html">jax.Array.committed</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.conj.html">jax.Array.conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.conjugate.html">jax.Array.conjugate</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.copy.html">jax.Array.copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.copy_to_host_async.html">jax.Array.copy_to_host_async</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.cumprod.html">jax.Array.cumprod</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.cumsum.html">jax.Array.cumsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.device.html">jax.Array.device</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.diagonal.html">jax.Array.diagonal</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.dot.html">jax.Array.dot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.dtype.html">jax.Array.dtype</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.flat.html">jax.Array.flat</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.flatten.html">jax.Array.flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.global_shards.html">jax.Array.global_shards</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.imag.html">jax.Array.imag</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.is_fully_addressable.html">jax.Array.is_fully_addressable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.is_fully_replicated.html">jax.Array.is_fully_replicated</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.item.html">jax.Array.item</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.itemsize.html">jax.Array.itemsize</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.max.html">jax.Array.max</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.mean.html">jax.Array.mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.min.html">jax.Array.min</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.nbytes.html">jax.Array.nbytes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.ndim.html">jax.Array.ndim</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.nonzero.html">jax.Array.nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.prod.html">jax.Array.prod</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.ptp.html">jax.Array.ptp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.ravel.html">jax.Array.ravel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.real.html">jax.Array.real</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.repeat.html">jax.Array.repeat</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.reshape.html">jax.Array.reshape</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.round.html">jax.Array.round</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.searchsorted.html">jax.Array.searchsorted</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.shape.html">jax.Array.shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.sharding.html">jax.Array.sharding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.size.html">jax.Array.size</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.sort.html">jax.Array.sort</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.squeeze.html">jax.Array.squeeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.std.html">jax.Array.std</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.sum.html">jax.Array.sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.swapaxes.html">jax.Array.swapaxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.take.html">jax.Array.take</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.to_device.html">jax.Array.to_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.trace.html">jax.Array.trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.transpose.html">jax.Array.transpose</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.var.html">jax.Array.var</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.view.html">jax.Array.view</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.T.html">jax.Array.T</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.mT.html">jax.Array.mT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../about.html">About the project</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently asked questions (FAQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Change log</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary of terms</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../config_options.html">Configuration Options</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../advanced_guides.html" class="nav-link">Resources and Advanced Guides</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">JAX...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/jax-ml/jax" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notebooks/host-offloading.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>JAX Memories and Host Offloading</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">JAX Memories and Host Offloading</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-blocks-for-offloading">Building Blocks for Offloading</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#namedsharding-and-memory-kinds">NamedSharding and Memory Kinds</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-placement-with-device-put">Data Placement with device_put</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#output-sharding-controls">Output Sharding Controls</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#device-output-sharding">Device Output Sharding</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#host-output-sharding">Host Output Sharding</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-offloading">Activation Offloading</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checkpoint-names">Checkpoint Names</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checkpoint-policies">Checkpoint Policies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-activation-offloading">Summary of Activation Offloading</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-offloading">Parameter Offloading</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-placement-for-computation">Parameter Placement for Computation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-initialization-with-host-offloading">Parameter Initialization with Host Offloading</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-analysis">Memory Analysis</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#key-optimizations">Key Optimizations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#results">Results</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-of-parameter-offloading">Limitations of Parameter Offloading</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizer-state-offloading">Optimizer State Offloading</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-implementation">Basic Implementation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-sharding-and-memory-kinds">Setting Up Sharding and Memory Kinds</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-and-training-step-implementation">Model and Training Step Implementation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#running-and-comparing-results">Running and Comparing Results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tools-for-host-offloading">Tools for Host Offloading</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="jax-memories-and-host-offloading">
<span id="host-offloading"></span><h1>JAX Memories and Host Offloading<a class="headerlink" href="#jax-memories-and-host-offloading" title="Link to this heading">#</a></h1>
<!--* freshness: { reviewed: '2025-04-10' } *-->
<p>This tutorial provides a practical introduction to host offloading techniques in JAX, focusing on:</p>
<ul class="simple">
<li><p>Activation offloading</p></li>
<li><p>Parameter offloading</p></li>
<li><p>Optimizer state offloading</p></li>
</ul>
<p>By applying offloading strategies, developers can better manage memory resources and reduce memory pressure on devices. To implement these strategies effectively, understanding JAX‚Äôs core mechanisms for data placement and movement is essential.</p>
<section id="building-blocks-for-offloading">
<h2>Building Blocks for Offloading<a class="headerlink" href="#building-blocks-for-offloading" title="Link to this heading">#</a></h2>
<p>JAX provides several key components for controlling where and how data are stored and moved between the host and the device memory. The following sections explore:</p>
<ul class="simple">
<li><p>How to specify data distribution with sharding</p></li>
<li><p>How to control memory placement between host and device</p></li>
<li><p>How to manage data movement in jitted functions</p></li>
</ul>
<section id="namedsharding-and-memory-kinds">
<h3>NamedSharding and Memory Kinds<a class="headerlink" href="#namedsharding-and-memory-kinds" title="Link to this heading">#</a></h3>
<p><a class="reference internal" href="../jax.sharding.html#jax.sharding.NamedSharding" title="jax.sharding.NamedSharding"><code class="xref py py-class docutils literal notranslate"><span class="pre">NamedSharding</span></code></a> defines how data are distributed across devices. It includes:</p>
<ul class="simple">
<li><p>Basic data distribution configuration</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">memory_kind</span></code> parameter for specifying memory type (<code class="docutils literal notranslate"><span class="pre">device</span></code> or <code class="docutils literal notranslate"><span class="pre">pinned_host</span></code>)</p></li>
<li><p>By default, <code class="docutils literal notranslate"><span class="pre">memory_kind</span></code> is set to <code class="docutils literal notranslate"><span class="pre">device</span></code> memory</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">with_memory_kind</span></code> method for creating new sharding with modified memory type</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">jax</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">jnp</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">jax.sharding</span><span class="w"> </span><span class="kn">import</span> <span class="n">Mesh</span><span class="p">,</span> <span class="n">NamedSharding</span><span class="p">,</span> <span class="n">PartitionSpec</span> <span class="k">as</span> <span class="n">P</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Create mesh</span>
<span class="c1"># 1x1 mesh represents a single device with two named dimensions (x and y)</span>
<span class="n">mesh</span> <span class="o">=</span> <span class="n">Mesh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">))</span>

<span class="c1"># Device sharding - partitions data along x and y dimensions</span>
<span class="n">s_dev</span> <span class="o">=</span> <span class="n">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">P</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">,</span> <span class="s1">&#39;y&#39;</span><span class="p">),</span> <span class="n">memory_kind</span><span class="o">=</span><span class="s2">&quot;device&quot;</span><span class="p">)</span>

<span class="c1"># Host sharding - same partitioning but in pinned host memory</span>
<span class="n">s_host</span> <span class="o">=</span> <span class="n">s_dev</span><span class="o">.</span><span class="n">with_memory_kind</span><span class="p">(</span><span class="s1">&#39;pinned_host&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">s_dev</span><span class="p">)</span>   <span class="c1"># Shows device memory sharding</span>
<span class="nb">print</span><span class="p">(</span><span class="n">s_host</span><span class="p">)</span>  <span class="c1"># Shows pinned host memory sharding</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>NamedSharding(mesh=Mesh(&#39;x&#39;: 1, &#39;y&#39;: 1), spec=PartitionSpec(&#39;x&#39;, &#39;y&#39;), memory_kind=device)
NamedSharding(mesh=Mesh(&#39;x&#39;: 1, &#39;y&#39;: 1), spec=PartitionSpec(&#39;x&#39;, &#39;y&#39;), memory_kind=pinned_host)
</pre></div>
</div>
</div>
</div>
</section>
<section id="data-placement-with-device-put">
<h3>Data Placement with device_put<a class="headerlink" href="#data-placement-with-device-put" title="Link to this heading">#</a></h3>
<p><a class="reference internal" href="../_autosummary/jax.device_put.html#jax.device_put" title="jax.device_put"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.device_put()</span></code></a> is a function that explicitly transfers arrays to a specified memory location according to a sharding specification.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create a 2x4 array</span>
<span class="n">arr</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mf">8.0</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>

<span class="c1"># Move arrays to different memory locations based on sharding objects</span>
<span class="n">arr_host</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">s_host</span><span class="p">)</span>  <span class="c1"># Places in pinned host memory</span>
<span class="n">arr_dev</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">s_dev</span><span class="p">)</span>    <span class="c1"># Places in device memory</span>

<span class="c1"># Verify memory locations</span>
<span class="nb">print</span><span class="p">(</span><span class="n">arr_host</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">memory_kind</span><span class="p">)</span>  <span class="c1"># Output: pinned_host</span>
<span class="nb">print</span><span class="p">(</span><span class="n">arr_dev</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">memory_kind</span><span class="p">)</span>   <span class="c1"># Output: device</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>pinned_host
device
</pre></div>
</div>
</div>
</div>
</section>
<section id="output-sharding-controls">
<h3>Output Sharding Controls<a class="headerlink" href="#output-sharding-controls" title="Link to this heading">#</a></h3>
<p>Shardings determine how data is split across devices. JAX provides <code class="docutils literal notranslate"><span class="pre">out_shardings</span></code> to control how output arrays are partitioned when leaving a jitted function.</p>
<p>Key Features:</p>
<ul class="simple">
<li><p>Can differ from input sharding</p></li>
<li><p>Allows different memory kinds for outputs</p></li>
</ul>
<p>Examples:</p>
<section id="device-output-sharding">
<h4>Device Output Sharding<a class="headerlink" href="#device-output-sharding" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="p">,</span> <span class="n">out_shardings</span><span class="o">=</span><span class="n">s_dev</span><span class="p">)</span>
<span class="n">out_dev</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">arr_host</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Result value of H2D: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">out_dev</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Result value of H2D: 
 [[0. 1. 2. 3.]
 [4. 5. 6. 7.]]
</pre></div>
</div>
</div>
</div>
<p>Moving data from host to device memory when needed for computation is the essence of host offloading. Use <a class="reference internal" href="../_autosummary/jax.device_put.html#jax.device_put" title="jax.device_put"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.device_put()</span></code></a> to perform this transfer in this example to optimize performance.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Instead of the lambda function, add_func can be defined explicitly</span>
<span class="c1"># move data to device before computation</span>
<span class="k">def</span><span class="w"> </span><span class="nf">add_func</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>  <span class="c1"># Move data to device and add one</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">s_dev</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="mi">1</span>

<span class="n">f</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">add_func</span><span class="p">,</span> <span class="n">out_shardings</span><span class="o">=</span><span class="n">s_dev</span><span class="p">)</span>
<span class="n">out_dev</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">arr_host</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Result value of H2D and add 1 in device memory: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">out_dev</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Result value of H2D and add 1 in device memory: 
 [[1. 2. 3. 4.]
 [5. 6. 7. 8.]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="host-output-sharding">
<h4>Host Output Sharding<a class="headerlink" href="#host-output-sharding" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">out_shardings</span><span class="o">=</span><span class="n">s_dev</span><span class="p">)</span>
<span class="n">out_host</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">arr_host</span><span class="p">)</span>      <span class="c1"># Input arrays in the device memory while output arrays in the host memory</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Result value of D2H: </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">out_host</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Result value of D2H: 
 [[0. 1. 2. 3.]
 [4. 5. 6. 7.]]
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="activation-offloading">
<h2>Activation Offloading<a class="headerlink" href="#activation-offloading" title="Link to this heading">#</a></h2>
<p>Before diving into activation offloading, let‚Äôs first take a look at the baseline code.</p>
<p>This code implements a simple neural network with 10 layers, each consisting of two linear transformations. The code demonstrates basic memory usage patterns and provides a foundation for comparing offloading optimization techniques.</p>
<p>Key components:</p>
<ul class="simple">
<li><p>Each layer consists of two sequential linear operations:</p>
<ol class="arabic simple">
<li><p>First multiplication: <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">&#64;</span> <span class="pre">w1</span></code></p></li>
<li><p>Second multiplication: <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">&#64;</span> <span class="pre">w2</span></code></p></li>
</ol>
</li>
<li><p>10-layer network using JAX‚Äôs scan operation</p></li>
<li><p>Memory usage analysis</p></li>
<li><p>Gradient computation with JIT compilation</p></li>
</ul>
<p>To analyze memory usage in JAX, the <a class="reference internal" href="../jax.stages.html#jax.stages.Compiled.memory_analysis" title="jax.stages.Compiled.memory_analysis"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.stages.Compiled.memory_analysis()</span></code></a> method can be used on a compiled function. This provides detailed statistics about memory consumption during computation. The key metrics include temporary memory size, argument size, output size, and alias size. To calculate the total memory usage, sum the temporary, argument, and output sizes, then subtract the alias size to avoid double-counting the same memory multiple times. This provides a summarized view of how the device memory is utilized across different aspects of the computation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize input and weights with small values (0.0001)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.001</span>  <span class="c1"># Input matrix: 256 x 256</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.001</span> <span class="c1"># 10 layers of 256 x 1024 matrices</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.001</span> <span class="c1"># 10 layers of 1024 x 256 matrices</span>

<span class="k">def</span><span class="w"> </span><span class="nf">two_layers</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
  <span class="c1"># Simple two-layer linear transformation</span>
  <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span> <span class="o">=</span> <span class="n">w</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w1</span>
  <span class="k">return</span> <span class="n">y</span> <span class="o">@</span> <span class="n">w2</span><span class="p">,</span> <span class="kc">None</span>

<span class="k">def</span><span class="w"> </span><span class="nf">scanned</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="c1"># Applies the layer function 10 times using JAX&#39;s scan operation</span>
  <span class="c1"># Input: w (tuple of weight matrices), x (input matrix)</span>
  <span class="c1"># Output: sum of the final layer&#39;s output</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span><span class="n">two_layers</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

<span class="c1"># Compile and compute gradients of the scanned function</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">scanned</span><span class="p">))</span>  <span class="c1"># Apply JIT compilation to gradient computation</span>

<span class="c1"># Analyze memory usage</span>
<span class="n">compiled_step</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">lower</span><span class="p">((</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">),</span> <span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="n">compiled_stats</span> <span class="o">=</span> <span class="n">compiled_step</span><span class="o">.</span><span class="n">memory_analysis</span><span class="p">()</span>

<span class="k">if</span> <span class="n">compiled_stats</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
  <span class="c1"># Calculate total memory usage including temporary storage, arguments, and outputs</span>
  <span class="c1"># Subtract alias size to avoid double-counting memory shared between different components</span>
  <span class="n">total</span> <span class="o">=</span> <span class="n">compiled_stats</span><span class="o">.</span><span class="n">temp_size_in_bytes</span> <span class="o">+</span> <span class="n">compiled_stats</span><span class="o">.</span><span class="n">argument_size_in_bytes</span> \
      <span class="o">+</span> <span class="n">compiled_stats</span><span class="o">.</span><span class="n">output_size_in_bytes</span> <span class="o">-</span> <span class="n">compiled_stats</span><span class="o">.</span><span class="n">alias_size_in_bytes</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Temp size: </span><span class="si">{</span><span class="n">compiled_stats</span><span class="o">.</span><span class="n">temp_size_in_bytes</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Argument size: </span><span class="si">{</span><span class="n">compiled_stats</span><span class="o">.</span><span class="n">argument_size_in_bytes</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total size: </span><span class="si">{</span><span class="n">total</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>

<span class="c1"># Execute the function and print sample results</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">f</span><span class="p">((</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">),</span> <span class="nb">input</span><span class="p">)</span>     <span class="c1"># Execute the function with weights and input</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample of results: &quot;</span><span class="p">,</span> <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Temp size: 17.25 MB
Argument size: 20.25 MB
Total size: 57.50 MB
Sample of results:  [3.8312336e-07 3.8312336e-07 3.8312336e-07 3.8312336e-07 3.8312336e-07]
</pre></div>
</div>
</div>
</div>
<p>The detailed coverage of activation offloading can be found in the <a class="reference internal" href="../gradient-checkpointing.html#gradient-checkpointing"><span class="std std-ref">Gradient checkpointing with jax.checkpoint (jax.remat)</span></a> tutorial. Activation offloading helps manage memory by moving intermediate activations to host memory after the forward pass, and bringing them back to device memory during the backward pass when needed for gradient computation.</p>
<p>To implement activation offloading effectively, it is important to understand checkpoint names and policies. Here‚Äôs how they work in a simple example:</p>
<section id="checkpoint-names">
<h3>Checkpoint Names<a class="headerlink" href="#checkpoint-names" title="Link to this heading">#</a></h3>
<p>The <code class="xref py py-func docutils literal notranslate"><span class="pre">checkpoint_name()</span></code> function allows labeling activations for memory management during computation. Here‚Äôs a simple example that a checkpoint name <code class="docutils literal notranslate"><span class="pre">x</span></code> is specified.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">jax.ad_checkpoint</span><span class="w"> </span><span class="kn">import</span> <span class="n">checkpoint_name</span>

<span class="k">def</span><span class="w"> </span><span class="nf">layer_name</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
  <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span> <span class="o">=</span> <span class="n">w</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">checkpoint_name</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s2">&quot;x&quot;</span><span class="p">)</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w1</span>
  <span class="k">return</span> <span class="n">y</span> <span class="o">@</span> <span class="n">w2</span><span class="p">,</span> <span class="kc">None</span>
</pre></div>
</div>
</div>
</div>
<p>The checkpoint name helps the system decide whether to:</p>
<ul class="simple">
<li><p>Keep the activation in device memory or</p></li>
<li><p>Offload it to host memory during computation</p></li>
</ul>
<p>This pattern is common in neural networks, where multiple transformations are applied sequentially to input data.</p>
</section>
<section id="checkpoint-policies">
<h3>Checkpoint Policies<a class="headerlink" href="#checkpoint-policies" title="Link to this heading">#</a></h3>
<p>This checkpoint policy implements a memory management strategy that optimizes memory usage during computation. It manages memory by handling intermediate values through three strategies:</p>
<ol class="arabic simple">
<li><p>Recomputing during backward pass (default behavior)</p></li>
<li><p>Storing on device</p></li>
<li><p>Offloading to host memory after forward pass and loading back during backward pass</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">jax</span><span class="w"> </span><span class="kn">import</span> <span class="n">checkpoint_policies</span> <span class="k">as</span> <span class="n">cp</span>

<span class="n">policy</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">save_and_offload_only_these_names</span><span class="p">(</span>
    <span class="n">names_which_can_be_saved</span><span class="o">=</span><span class="p">[],</span>          <span class="c1"># No values stored on device</span>
    <span class="n">names_which_can_be_offloaded</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span>   <span class="c1"># Offload activations labeled &quot;x&quot;</span>
    <span class="n">offload_src</span><span class="o">=</span><span class="s2">&quot;device&quot;</span><span class="p">,</span>                 <span class="c1"># Move from device memory</span>
    <span class="n">offload_dst</span><span class="o">=</span><span class="s2">&quot;pinned_host&quot;</span>             <span class="c1"># To pinned host memory</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><a class="reference internal" href="../_autosummary/jax.lax.scan.html#jax.lax.scan" title="jax.lax.scan"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.lax.scan()</span></code></a> is commonly used in JAX for handling sequential operations (like RNNs or transformers). It can be integrated with JAX‚Äôs rematerialization to process sequential data.</p>
<p>Key components:</p>
<ul class="simple">
<li><p><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.remat()</span></code> creates a rematerialized version of the layer function using <code class="xref py py-func docutils literal notranslate"><span class="pre">jax.remat()</span></code> and applies the checkpoint policy to the layer function</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">prevent_cse=False</span></code> enables XLA‚Äôs common subexpression elimination for better performance</p></li>
<li><p><a class="reference internal" href="../_autosummary/jax.lax.scan.html#jax.lax.scan" title="jax.lax.scan"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.lax.scan()</span></code></a> iterates the rematerialized layer along an axis</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">scanned</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="n">remat_layer</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">remat</span><span class="p">(</span><span class="n">layer_name</span><span class="p">,</span>
                          <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span>     <span class="c1"># Use our offloading policy</span>
                          <span class="n">prevent_cse</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># Allow CSE optimizations</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span><span class="n">remat_layer</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

<span class="c1"># Initialize input and weights with small values (0.0001)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.001</span>  <span class="c1"># Input matrix: 256 x 256</span>
<span class="n">w1</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.001</span> <span class="c1"># 10 layers of 256 x 1024 matrices</span>
<span class="n">w2</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.001</span> <span class="c1"># 10 layers of 1024 x 256 matrices</span>

<span class="c1"># Compile and compute gradients of the scanned function</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">scanned</span><span class="p">))</span>  <span class="c1"># Apply JIT compilation to gradient computation</span>

<span class="c1"># Analyze memory usage</span>
<span class="n">compiled_step</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">lower</span><span class="p">((</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">),</span> <span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="n">compiled_stats</span> <span class="o">=</span> <span class="n">compiled_step</span><span class="o">.</span><span class="n">memory_analysis</span><span class="p">()</span>

<span class="k">if</span> <span class="n">compiled_stats</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
  <span class="n">total</span> <span class="o">=</span> <span class="n">compiled_stats</span><span class="o">.</span><span class="n">temp_size_in_bytes</span> <span class="o">+</span> <span class="n">compiled_stats</span><span class="o">.</span><span class="n">argument_size_in_bytes</span> \
      <span class="o">+</span> <span class="n">compiled_stats</span><span class="o">.</span><span class="n">output_size_in_bytes</span> <span class="o">-</span> <span class="n">compiled_stats</span><span class="o">.</span><span class="n">alias_size_in_bytes</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Temp size: </span><span class="si">{</span><span class="n">compiled_stats</span><span class="o">.</span><span class="n">temp_size_in_bytes</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Argument size: </span><span class="si">{</span><span class="n">compiled_stats</span><span class="o">.</span><span class="n">argument_size_in_bytes</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total size: </span><span class="si">{</span><span class="n">total</span><span class="o">/</span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>

<span class="n">result_activation</span> <span class="o">=</span> <span class="n">f</span><span class="p">((</span><span class="n">w1</span><span class="p">,</span> <span class="n">w2</span><span class="p">),</span> <span class="nb">input</span><span class="p">)</span>     <span class="c1"># Execute the function with weights and input</span>
<span class="c1"># Verify numerical correctness</span>
<span class="n">are_close</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span>
    <span class="n">result_activation</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>    <span class="c1"># Result from activation offloading only</span>
    <span class="n">result</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>         <span class="c1"># Result from both activation and parameter offloading</span>
    <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">atol</span><span class="o">=</span><span class="mf">1e-5</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Results match within tolerance: </span><span class="si">{</span><span class="n">are_close</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample of results: &quot;</span><span class="p">,</span> <span class="n">result_activation</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Temp size: 6.50 MB
Argument size: 20.25 MB
Total size: 46.75 MB
Results match within tolerance: True
Sample of results:  [3.8312336e-07 3.8312336e-07 3.8312336e-07 3.8312336e-07 3.8312336e-07]
</pre></div>
</div>
</div>
</div>
<p>Activation offloading reduces temporary memory usage from 17.25 MB to 6.5 MB while input and output argument sizes remain the same. Totally 10.75 MB is saved. It is achieved by offloading activation <code class="docutils literal notranslate"><span class="pre">x</span></code> to host memory after the forward pass and loading it back to device memory before the backward pass.</p>
</section>
<section id="summary-of-activation-offloading">
<h3>Summary of Activation Offloading<a class="headerlink" href="#summary-of-activation-offloading" title="Link to this heading">#</a></h3>
<p>Activation offloading provides a powerful way to manage memory in large computations by:</p>
<ul class="simple">
<li><p>Using checkpoint names to mark specific activations</p></li>
<li><p>Applying policies to control where and how activations are stored</p></li>
<li><p>Supporting common JAX patterns like scan operations</p></li>
<li><p>Moving selected activations to host memory when device memory is under budget</p></li>
</ul>
<p>This approach is particularly useful when working with large models that would otherwise exceed device memory capacity.</p>
</section>
</section>
<section id="parameter-offloading">
<h2>Parameter Offloading<a class="headerlink" href="#parameter-offloading" title="Link to this heading">#</a></h2>
<p>Model parameters (also known as weights) can be offloaded to the host memory to optimize device memory usage during initialization. This is achieved by using <a class="reference internal" href="../_autosummary/jax.jit.html#jax.jit" title="jax.jit"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.jit()</span></code></a> with a sharding strategy that specifies host memory kind.</p>
<p>While parameter offloading and activation offloading are distinct memory optimization techniques, the following example demonstrates parameter offloading built upon the activation offloading implementation shown earlier.</p>
<section id="parameter-placement-for-computation">
<h3>Parameter Placement for Computation<a class="headerlink" href="#parameter-placement-for-computation" title="Link to this heading">#</a></h3>
<p>Different from the earlier <code class="docutils literal notranslate"><span class="pre">layer</span></code> function, <a class="reference internal" href="../_autosummary/jax.device_put.html#jax.device_put" title="jax.device_put"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.device_put()</span></code></a> is applied to move parameter <code class="docutils literal notranslate"><span class="pre">w1</span></code> and <code class="docutils literal notranslate"><span class="pre">w2</span></code> to the device before the  matrix multiplications. This ensures the parameters are available on the device for both forward and backward passes.</p>
<p>Note that the activation offloading implementation remains unchanged, using the same:</p>
<ul class="simple">
<li><p>Checkpoint name <code class="docutils literal notranslate"><span class="pre">&quot;x&quot;</span></code></p></li>
<li><p>Checkpoint policy</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scanned</span></code> function combining <code class="xref py py-func docutils literal notranslate"><span class="pre">jax.remat()</span></code> and <a class="reference internal" href="../_autosummary/jax.lax.scan.html#jax.lax.scan" title="jax.lax.scan"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.lax.scan()</span></code></a></p></li>
</ul>
</section>
<section id="parameter-initialization-with-host-offloading">
<h3>Parameter Initialization with Host Offloading<a class="headerlink" href="#parameter-initialization-with-host-offloading" title="Link to this heading">#</a></h3>
<p>During the initialization, parameter <code class="docutils literal notranslate"><span class="pre">w1</span></code> and <code class="docutils literal notranslate"><span class="pre">w2</span></code> are placed on host memory before being passed to the <a class="reference internal" href="../_autosummary/jax.jit.html#jax.jit" title="jax.jit"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.jit()</span></code></a> function <code class="docutils literal notranslate"><span class="pre">f</span></code>, while keeping the <code class="docutils literal notranslate"><span class="pre">input</span></code> variable on the device.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Hybrid version: Both activation and parameter offloading</span>
<span class="k">def</span><span class="w"> </span><span class="nf">hybrid_layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
  <span class="c1"># Move model parameters w1 and w2 to device memory via device_put</span>
  <span class="n">w1</span><span class="p">,</span> <span class="n">w2</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">s_dev</span><span class="p">),</span> <span class="n">w</span><span class="p">)</span>
  <span class="n">x</span> <span class="o">=</span> <span class="n">checkpoint_name</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="s2">&quot;x&quot;</span><span class="p">)</span>  <span class="c1"># Offload activation x to host memory</span>
  <span class="n">y</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w1</span>
  <span class="k">return</span> <span class="n">y</span> <span class="o">@</span> <span class="n">w2</span><span class="p">,</span> <span class="kc">None</span>

<span class="k">def</span><span class="w"> </span><span class="nf">hybrid_scanned</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="n">remat_layer</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">remat</span><span class="p">(</span><span class="n">hybrid_layer</span><span class="p">,</span>     <span class="c1"># Use hybrid_layer instead of layer</span>
                          <span class="n">policy</span><span class="o">=</span><span class="n">policy</span><span class="p">,</span>     <span class="c1"># Use offloading policy</span>
                          <span class="n">prevent_cse</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="c1"># Allow CSE optimizations</span>
  <span class="n">result</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span><span class="n">remat_layer</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

<span class="c1"># Move model parameters w1 and w2 to the host via device_put</span>
<span class="c1"># Initialize input and weights with small values (0.0001)</span>
<span class="n">wh1</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span><span class="n">w1</span><span class="p">,</span> <span class="n">s_host</span><span class="p">)</span>
<span class="n">wh2</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span><span class="n">w2</span><span class="p">,</span> <span class="n">s_host</span><span class="p">)</span>

<span class="c1"># Compile and compute gradients of the scanned function</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">hybrid_scanned</span><span class="p">))</span>  <span class="c1"># Apply JIT compilation to gradient computation</span>

<span class="c1"># Analyze memory usage</span>
<span class="n">compiled_step</span> <span class="o">=</span> <span class="n">f</span><span class="o">.</span><span class="n">lower</span><span class="p">((</span><span class="n">wh1</span><span class="p">,</span> <span class="n">wh2</span><span class="p">),</span> <span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="n">compiled_stats</span> <span class="o">=</span> <span class="n">compiled_step</span><span class="o">.</span><span class="n">memory_analysis</span><span class="p">()</span>

<span class="k">if</span> <span class="n">compiled_stats</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
  <span class="n">total</span> <span class="o">=</span> <span class="n">compiled_stats</span><span class="o">.</span><span class="n">temp_size_in_bytes</span> <span class="o">+</span> <span class="n">compiled_stats</span><span class="o">.</span><span class="n">argument_size_in_bytes</span> \
      <span class="o">+</span> <span class="n">compiled_stats</span><span class="o">.</span><span class="n">output_size_in_bytes</span> <span class="o">-</span> <span class="n">compiled_stats</span><span class="o">.</span><span class="n">alias_size_in_bytes</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Temp size: </span><span class="si">{</span><span class="n">compiled_stats</span><span class="o">.</span><span class="n">temp_size_in_bytes</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Argument size: </span><span class="si">{</span><span class="n">compiled_stats</span><span class="o">.</span><span class="n">argument_size_in_bytes</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total size: </span><span class="si">{</span><span class="n">total</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>

<span class="n">result_both</span> <span class="o">=</span> <span class="n">f</span><span class="p">((</span><span class="n">wh1</span><span class="p">,</span> <span class="n">wh2</span><span class="p">),</span> <span class="nb">input</span><span class="p">)</span> <span class="c1"># Execute with both activation and parameter offloading</span>

<span class="c1"># Verify numerical correctness</span>
<span class="n">are_close</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span>
    <span class="n">result_activation</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>    <span class="c1"># Result from activation offloading only</span>
    <span class="n">result_both</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>         <span class="c1"># Result from both activation and parameter offloading</span>
    <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
    <span class="n">atol</span><span class="o">=</span><span class="mf">1e-5</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Results match within tolerance: </span><span class="si">{</span><span class="n">are_close</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Temp size: 4.75 MB
Argument size: 0.25 MB
Total size: 25.00 MB
Results match within tolerance: True
</pre></div>
</div>
</div>
</div>
<p>This implementation demonstrates how offloading model parameters together with activation offloading to host memory can significantly reduce device memory usage.</p>
</section>
<section id="memory-analysis">
<h3>Memory Analysis<a class="headerlink" href="#memory-analysis" title="Link to this heading">#</a></h3>
<p><strong>Baseline Memory Usage:</strong></p>
<ul class="simple">
<li><p>Input tensor: 0.25 MB (256 √ó 256 √ó 4 bytes)</p></li>
<li><p>Model parameters (w1, w2): 10 MB each (256 √ó 1024 √ó 4 bytes ‚âà 1 MB per layer √ó 10 layers)</p></li>
</ul>
<p><strong>Memory Usage Comparison:</strong></p>
<ul class="simple">
<li><p>Argument size without parameter offloading: 20.25 MB (0.25 + 10 + 10)</p></li>
<li><p>Argument size with parameter offloading: 0.25 MB (only input remains)</p></li>
<li><p>Temporary memory without activation offloading: 17.25 MB</p></li>
<li><p>Temporary memory with activation offloading: 6.50 MB</p></li>
<li><p>Temporary memory with activation and parameter offloading: 4.75 MB</p></li>
</ul>
<section id="key-optimizations">
<h4>Key Optimizations<a class="headerlink" href="#key-optimizations" title="Link to this heading">#</a></h4>
<ol class="arabic simple">
<li><p><strong>Parameter Offloading</strong>: Moving parameters (w1, w2) to host memory reduces argument size by 20 MB (from 20.25 MB to 0.25 MB).</p></li>
<li><p><strong>Activation Offloading</strong>: Moving activations to host memory reduces temporary memory usage by 10.75 MB (from 17.25 to 6.50 MB).</p></li>
<li><p><strong>Hybrid Strategy</strong>: The rematerialization of activation offloading helps avoid keeping weights on the device and reduce temporary memory usage by 1.75 MB (from 6.50 MB to 4.75 MB). Without it, JAX would be eager to keep the on-device copies of the weights alive for the backward pass.</p></li>
</ol>
</section>
<section id="results">
<h4>Results<a class="headerlink" href="#results" title="Link to this heading">#</a></h4>
<p><strong>Total Memory Savings</strong>: 33.5 MB (20 MB + 10.75 MB + 1.75 MB)</p>
<p>This hybrid approach demonstrates that parameter and activation offloading work synergistically to achieve significant memory reductions while maintaining computational correctness.</p>
</section>
</section>
<section id="limitations-of-parameter-offloading">
<h3>Limitations of Parameter Offloading<a class="headerlink" href="#limitations-of-parameter-offloading" title="Link to this heading">#</a></h3>
<p><a class="reference internal" href="../_autosummary/jax.lax.scan.html#jax.lax.scan" title="jax.lax.scan"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.lax.scan()</span></code></a> is crucial for effective parameter management. Using an explicit for loop would cause parameters to continuously occupy device memory, resulting in the same memory usage as without parameter offloading. While <a class="reference internal" href="../_autosummary/jax.lax.scan.html#jax.lax.scan" title="jax.lax.scan"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.lax.scan()</span></code></a> allows specifying the scan axis, parameter offloading currently works only when scanning over axis 0. Scanning over other axes generates a <code class="docutils literal notranslate"><span class="pre">transpose</span></code> operation during compilation before returning parameters to the device, which is expensive and not supported on all platforms.</p>
<p>The offloading performance can vary for different device types. It may degrade performance due to memory transfers between host and device, so it‚Äôs important to consider this trade-off when designing your optimization strategy.</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="optimizer-state-offloading">
<h1>Optimizer State Offloading<a class="headerlink" href="#optimizer-state-offloading" title="Link to this heading">#</a></h1>
<p>Optimizer state offloading is a memory management technique that stores optimizer states in host memory instead of device memory. This approach is particularly useful when optimizer states are large, as it reduces device memory usage.</p>
<p>A basic JAX implementation using the Adam optimizer can serve as a starting point, where all tensors are stored on the device. This will serve as a reference implementation before introducing optimizer state offloading.</p>
<section id="basic-implementation">
<h2>Basic Implementation<a class="headerlink" href="#basic-implementation" title="Link to this heading">#</a></h2>
<p>This section, let‚Äôs implement a simple model with the Adam optimizer. This implementation helps establish the baseline behavior before exploring optimizer state offloading. It is particularly useful for understanding memory patterns in large-scale neural network training.</p>
<p>In the code example below, a neural network training loop is included to use JAX and Optax‚Äôs Adam optimizer. The network consists of four linear layers with GELU activation functions, processing large matrices of size 7168x7168. The training process involves:</p>
<ul class="simple">
<li><p>Forward pass: The input flows through four layers, each applying a linear transformation followed by GELU activation</p></li>
<li><p>Loss computation: Calculates mean squared error between output and input, plus L2 regularization</p></li>
<li><p>Backward pass: Computes gradients using automatic differentiation</p></li>
<li><p>Optimization step: Updates parameters using Adam optimizer with gradient clipping</p></li>
</ul>
<p>The code uses JIT compilation to optimize performance and includes memory usage analysis to monitor the computational resources required during training. The memory analysis provides insights into temporary memory usage, argument sizes, and total memory consumption during the optimization step.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">optax</span>

<span class="n">DIM</span> <span class="o">=</span> <span class="mi">7168</span>

<span class="c1"># Initialize data and parameter w1, w2, w3 and w4</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">DIM</span><span class="p">,</span> <span class="n">DIM</span><span class="p">))</span>
<span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="sa">f</span><span class="s1">&#39;w</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">DIM</span><span class="p">,</span> <span class="n">DIM</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)}</span>

<span class="c1"># Initialize optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span>
    <span class="n">optax</span><span class="o">.</span><span class="n">clip_by_global_norm</span><span class="p">(</span><span class="mf">1.0</span><span class="p">),</span>
    <span class="n">optax</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">opt_state</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">gelu</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
  <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="n">jnp</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">+</span> <span class="mf">0.044715</span> <span class="o">*</span> <span class="n">x</span><span class="o">**</span><span class="mi">3</span><span class="p">)))</span>

<span class="k">def</span><span class="w"> </span><span class="nf">single_layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w</span>

<span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">gelu</span><span class="p">(</span><span class="n">single_layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">params</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;w</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">]))</span>
  <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span><span class="w"> </span><span class="nf">compute_loss</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
  <span class="n">outputs</span> <span class="o">=</span> <span class="n">forward</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
  <span class="n">loss</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">outputs</span> <span class="o">-</span> <span class="n">inputs</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
  <span class="n">l2_reg</span> <span class="o">=</span> <span class="mf">0.001</span> <span class="o">*</span> <span class="nb">sum</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">w</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree_util</span><span class="o">.</span><span class="n">tree_leaves</span><span class="p">(</span><span class="n">params</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">loss</span> <span class="o">+</span> <span class="n">l2_reg</span>

<span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
  <span class="n">grads</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">inputs</span><span class="p">))(</span><span class="n">params</span><span class="p">)</span>
  <span class="n">updates</span><span class="p">,</span> <span class="n">new_opt_state</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">optax</span><span class="o">.</span><span class="n">apply_updates</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">updates</span><span class="p">),</span> <span class="n">new_opt_state</span>

<span class="c1"># JIT compile the step function with proper sharding</span>
<span class="n">step</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="n">donate_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>

<span class="c1"># Run a optimization step</span>
<span class="n">new_params</span><span class="p">,</span> <span class="n">new_opt_state</span> <span class="o">=</span> <span class="n">step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>

<span class="c1"># Analyze memory usage</span>
<span class="n">compiled_step</span> <span class="o">=</span> <span class="n">step</span><span class="o">.</span><span class="n">lower</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="n">compiled_stats</span> <span class="o">=</span> <span class="n">compiled_step</span><span class="o">.</span><span class="n">memory_analysis</span><span class="p">()</span>

<span class="k">if</span> <span class="n">compiled_stats</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
  <span class="n">total</span> <span class="o">=</span> <span class="n">compiled_stats</span><span class="o">.</span><span class="n">temp_size_in_bytes</span> <span class="o">+</span> <span class="n">compiled_stats</span><span class="o">.</span><span class="n">argument_size_in_bytes</span> \
      <span class="o">+</span> <span class="n">compiled_stats</span><span class="o">.</span><span class="n">output_size_in_bytes</span> <span class="o">-</span> <span class="n">compiled_stats</span><span class="o">.</span><span class="n">alias_size_in_bytes</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Temp size: </span><span class="si">{</span><span class="n">compiled_stats</span><span class="o">.</span><span class="n">temp_size_in_bytes</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Argument size: </span><span class="si">{</span><span class="n">compiled_stats</span><span class="o">.</span><span class="n">argument_size_in_bytes</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total size: </span><span class="si">{</span><span class="n">total</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Temp size: 2.11 GB
Argument size: 2.49 GB
Total size: 4.59 GB
</pre></div>
</div>
</div>
</div>
<p>Optimizer state offloading can be implemented as follows.</p>
</section>
<section id="setting-up-sharding-and-memory-kinds">
<h2>Setting Up Sharding and Memory Kinds<a class="headerlink" href="#setting-up-sharding-and-memory-kinds" title="Link to this heading">#</a></h2>
<p><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.sharding.SingleDeivceSharding()</span></code> is adopted to simplify the shardings for both device and host memory kinds. During the model state initialization, move the optimizer state to the host using <code class="xref py py-func docutils literal notranslate"><span class="pre">device_put()</span></code>.</p>
</section>
<section id="model-and-training-step-implementation">
<h2>Model and Training Step Implementation<a class="headerlink" href="#model-and-training-step-implementation" title="Link to this heading">#</a></h2>
<p>Next, define the model architecture, loss function, and training step. The key addition here is moving the optimizer state to device memory via <code class="xref py py-func docutils literal notranslate"><span class="pre">device_put()</span></code> at the beginning of each training step, as it‚Äôs needed for the parameter update on the device.</p>
</section>
<section id="running-and-comparing-results">
<h2>Running and Comparing Results<a class="headerlink" href="#running-and-comparing-results" title="Link to this heading">#</a></h2>
<p>After setting up the sharding, the optimizer state is moved to host memory and the step function is run with <a class="reference internal" href="../_autosummary/jax.jit.html#jax.jit" title="jax.jit"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.jit()</span></code></a>.</p>
<p>The JIT compilation of the step function uses several important parameters:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">donate_argnums=(0,)</span></code>: Indicates that the first argument (parameters) can be modified in-place, allowing JAX to reuse its memory</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">out_shardings</span></code>: Specifies how output tensors should be sharded across the mesh (devices and hosts)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create sharding specifications for device and host memory</span>
<span class="n">s_dev</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">SingleDeviceSharding</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">memory_kind</span><span class="o">=</span><span class="s2">&quot;device&quot;</span><span class="p">)</span>
<span class="n">s_host</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">SingleDeviceSharding</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">memory_kind</span><span class="o">=</span><span class="s2">&quot;pinned_host&quot;</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
  <span class="n">grads</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">p</span><span class="p">:</span> <span class="n">compute_loss</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">inputs</span><span class="p">))(</span><span class="n">params</span><span class="p">)</span>
  <span class="n">opt_state</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span><span class="n">opt_state</span><span class="p">,</span> <span class="n">s_dev</span><span class="p">)</span>
  <span class="n">updates</span><span class="p">,</span> <span class="n">new_opt_state</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">grads</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
  <span class="n">new_params</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">apply_updates</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">updates</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">new_params</span><span class="p">,</span> <span class="n">new_opt_state</span>

<span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="sa">f</span><span class="s1">&#39;w</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">DIM</span><span class="p">,</span> <span class="n">DIM</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">5</span><span class="p">)}</span>
<span class="n">opt_state</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>

<span class="c1"># Initialize optimizer</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optax</span><span class="o">.</span><span class="n">chain</span><span class="p">(</span>
  <span class="n">optax</span><span class="o">.</span><span class="n">clip_by_global_norm</span><span class="p">(</span><span class="mf">1.0</span><span class="p">),</span>
  <span class="n">optax</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Optimizer state is placed on the host during initialization</span>
<span class="n">opt_state</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span><span class="n">opt_state</span><span class="p">,</span> <span class="n">s_host</span><span class="p">)</span>

<span class="c1"># JIT compile the step function with proper sharding and memory optimization</span>
<span class="n">step</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span>
  <span class="n">step</span><span class="p">,</span>
  <span class="n">donate_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,),</span>
  <span class="n">out_shardings</span><span class="o">=</span><span class="p">(</span><span class="n">s_dev</span><span class="p">,</span> <span class="n">s_host</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Run an optimization step</span>
<span class="n">new_params</span><span class="p">,</span> <span class="n">offload_opt_state</span> <span class="o">=</span> <span class="n">step</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>

<span class="c1"># Analyze memory usage</span>
<span class="n">compiled_step</span> <span class="o">=</span> <span class="n">step</span><span class="o">.</span><span class="n">lower</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">opt_state</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>
<span class="n">compiled_stats</span> <span class="o">=</span> <span class="n">compiled_step</span><span class="o">.</span><span class="n">memory_analysis</span><span class="p">()</span>
<span class="k">if</span> <span class="n">compiled_stats</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
  <span class="n">total</span> <span class="o">=</span> <span class="n">compiled_stats</span><span class="o">.</span><span class="n">temp_size_in_bytes</span> <span class="o">+</span> <span class="n">compiled_stats</span><span class="o">.</span><span class="n">argument_size_in_bytes</span> \
      <span class="o">+</span> <span class="n">compiled_stats</span><span class="o">.</span><span class="n">output_size_in_bytes</span> <span class="o">-</span> <span class="n">compiled_stats</span><span class="o">.</span><span class="n">alias_size_in_bytes</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Temp size: </span><span class="si">{</span><span class="n">compiled_stats</span><span class="o">.</span><span class="n">temp_size_in_bytes</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Argument size: </span><span class="si">{</span><span class="n">compiled_stats</span><span class="o">.</span><span class="n">argument_size_in_bytes</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> MB&quot;</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Total size: </span><span class="si">{</span><span class="n">total</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mi">1024</span><span class="o">**</span><span class="mi">3</span><span class="p">)</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2"> GB&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Temp size: 1.91 GB
Argument size: 0.96 MB
Total size: 2.87 GB
</pre></div>
</div>
</div>
</div>
<p>This implementation demonstrates how to:</p>
<ol class="arabic simple">
<li><p>Set up sharding specifications for <code class="docutils literal notranslate"><span class="pre">device</span></code> and <code class="docutils literal notranslate"><span class="pre">pinned_host</span></code></p></li>
<li><p>Move optimizer states between host and device memory via <a class="reference internal" href="../_autosummary/jax.device_put.html#jax.device_put" title="jax.device_put"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.device_put()</span></code></a></p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">out_shardings</span></code> to ensure proper memory placement</p></li>
<li><p>Show the memory usage</p></li>
</ol>
<p>This implementation demonstrates how offloading optimizer state to host memory can reduce device memory usage through a trade-off between argument size and temporary memory.</p>
<p>Memory Analysis:</p>
<ol class="arabic simple">
<li><p>Argument Size Reduction:</p>
<ul class="simple">
<li><p>The optimizer states are arguments of the <a class="reference internal" href="../_autosummary/jax.jit.html#jax.jit" title="jax.jit"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.jit()</span></code></a> function</p></li>
<li><p>By offloading these states to host memory, the argument size on device is reduced</p></li>
</ul>
</li>
<li><p>Temporary Memory Impact:</p>
<ul class="simple">
<li><p>Offloading increases temporary memory usage</p></li>
<li><p>This is because outputs of optimizer states need memory buffers before being copied to host</p></li>
<li><p>The memory live ranges for these temporary buffers are extended due to the host-device transfers</p></li>
</ul>
</li>
<li><p>Latency Hiding Scheduling:</p>
<ul class="simple">
<li><p>JAX uses XLA‚Äôs latency hiding scheduling to overlap computation with host-device transfers</p></li>
<li><p>The overlapping can cause tensors to have larger live ranges, which increases memory pressure on the device</p></li>
<li><p>This adaptive behavior helps maintain stable memory usage while still providing some performance benefits</p></li>
</ul>
</li>
<li><p>Memory Trade-off:</p>
<ul class="simple">
<li><p>Total memory size with offloading: 2.87 GB</p></li>
<li><p>Total memory size without offloading: 4.59 GB</p></li>
<li><p>Net memory saving: 1.72 GB</p></li>
</ul>
</li>
</ol>
<p>while offloading increases temporary memory usage, the reduction in argument size more than compensates for this increase, resulting in an overall reduction in device memory usage.</p>
<p>Note: The optimizer states can be compared for numerical equivalence using <code class="docutils literal notranslate"><span class="pre">jax.tree_util.tree_map</span></code> and <code class="docutils literal notranslate"><span class="pre">jnp.allclose</span></code>, but this verification step is omitted here for brevity.</p>
</section>
<section id="tools-for-host-offloading">
<h2>Tools for Host Offloading<a class="headerlink" href="#tools-for-host-offloading" title="Link to this heading">#</a></h2>
<p><a class="reference internal" href="../jax.stages.html#jax.stages.Compiled.memory_analysis" title="jax.stages.Compiled.memory_analysis"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.stages.Compiled.memory_analysis()</span></code></a> API is utilized above to get memory usage information. For device memory analysis, refer to :doc:<code class="docutils literal notranslate"><span class="pre">device_memory_profiling</span></code>. The profiling tools described in <a class="reference external" href="https://docs.python.org/3/c-api/init.html#profiling" title="(in Python v3.13)"><span>Profiling and Tracing</span></a> can help measure memory savings and performance impact from host offloading.</p>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="layout.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Device-local array layout control</p>
      </div>
    </a>
    <a class="right-next"
       href="../multi_process.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Introduction to multi-controller JAX (aka multi-process/multi-host JAX)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">JAX Memories and Host Offloading</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-blocks-for-offloading">Building Blocks for Offloading</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#namedsharding-and-memory-kinds">NamedSharding and Memory Kinds</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-placement-with-device-put">Data Placement with device_put</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#output-sharding-controls">Output Sharding Controls</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#device-output-sharding">Device Output Sharding</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#host-output-sharding">Host Output Sharding</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-offloading">Activation Offloading</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checkpoint-names">Checkpoint Names</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checkpoint-policies">Checkpoint Policies</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-activation-offloading">Summary of Activation Offloading</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-offloading">Parameter Offloading</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-placement-for-computation">Parameter Placement for Computation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parameter-initialization-with-host-offloading">Parameter Initialization with Host Offloading</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#memory-analysis">Memory Analysis</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#key-optimizations">Key Optimizations</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#results">Results</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#limitations-of-parameter-offloading">Limitations of Parameter Offloading</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizer-state-offloading">Optimizer State Offloading</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-implementation">Basic Implementation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#setting-up-sharding-and-memory-kinds">Setting Up Sharding and Memory Kinds</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-and-training-step-implementation">Model and Training Step Implementation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#running-and-comparing-results">Running and Comparing Results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tools-for-host-offloading">Tools for Host Offloading</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The JAX authors
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2024, The JAX Authors.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>