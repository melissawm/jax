
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Scalar Prefetch and Block-Sparse Computation &#8212; JAX  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/style.css?v=537c1ddb" />
    <link rel="stylesheet" href="../../_static/style.css" type="text/css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=30646c52"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'pallas/tpu/sparse';</script>
    <link rel="icon" href="../../_static/favicon.png"/>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Distributed Computing in Pallas for TPUs" href="distributed.html" />
    <link rel="prev" title="Matrix Multiplication" href="matmul.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/jax_logo_250px.png" class="logo__image only-light" alt="JAX  documentation - Home"/>
    <script>document.write(`<img src="../../_static/jax_logo_250px.png" class="logo__image only-dark" alt="JAX  documentation - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/thinking_in_jax.html">Quickstart: How to think in JAX</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/Common_Gotchas_in_JAX.html">üî™ JAX - The Sharp Bits üî™</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials.html">Tutorials</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../jit-compilation.html">Just-in-time compilation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../automatic-vectorization.html">Automatic vectorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../automatic-differentiation.html">Automatic differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../tracing.html">Tracing</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../random-numbers.html">Pseudorandom numbers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../stateful-computations.html">Stateful computations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../control-flow.html">Control flow and logical operators with JIT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../pytrees.html">Pytrees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../intro_parallel.html">Introduction to Parallel Programming with JAX</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Resources and Advanced Guides</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../key-concepts.html">Key concepts</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../../advanced_guide.html">Resources and Advanced Guides</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/autodiff_cookbook.html">The Autodiff Cookbook</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Custom_derivative_rules_for_Python_code.html">Custom derivative rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/autodiff_remat.html">Control autodiff‚Äôs saved values with <code class="docutils literal notranslate"><span class="pre">jax.checkpoint</span></code> (aka <code class="docutils literal notranslate"><span class="pre">jax.remat</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../advanced-autodiff.html">Advanced automatic differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../custom_pytrees.html">Custom pytrees and initialization with unexpected values</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../errors.html">Errors</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../debugging.html">Introduction to debugging</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../debugging/print_breakpoint.html">Compiled prints and breakpoints</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../debugging/checkify_guide.html">The <code class="docutils literal notranslate"><span class="pre">checkify</span></code> transformation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../debugging/flags.html">JAX debugging flags</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../debugging/flags.html">JAX debugging flags</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../transfer_guard.html">Transfer guard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../persistent_compilation_cache.html">Persistent compilation cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../buffer_donation.html">Buffer donation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../gpu_performance_tips.html">GPU performance tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../benchmarking.html">Benchmarking JAX code</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../profiling.html">Profiling computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../device_memory_profiling.html">Profiling device memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Distributed_arrays_and_automatic_parallelization.html">Distributed arrays and automatic parallelization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/explicit-sharding.html">Explicit sharding (a.k.a. ‚Äúsharding in types‚Äù)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/shard_map.html">Manual parallelism with <code class="docutils literal notranslate"><span class="pre">shard_map</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/host-offloading.html">JAX Memories and Host Offloading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../multi_process.html">Introduction to multi-controller JAX (aka multi-process/multi-host JAX)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../distributed_data_loading.html">Distributed data loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../external-callbacks.html">External callbacks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ffi.html">Foreign function interface (FFI)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../gradient-checkpointing.html">Gradient checkpointing with <code class="docutils literal notranslate"><span class="pre">jax.checkpoint</span></code> (<code class="docutils literal notranslate"><span class="pre">jax.remat</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aot.html">Ahead-of-time lowering and compilation</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../export/index.html">Exporting and serialization</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../export/export.html">Exporting and serializing staged-out computations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../export/shape_poly.html">Shape polymorphism</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../export/jax2tf.html">Interoperation with TensorFlow</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../type_promotion.html">Type promotion semantics</a></li>
<li class="toctree-l2 current active has-children"><a class="reference internal" href="../index.html">Pallas: a JAX kernel language</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../quickstart.html">Pallas Quickstart</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pipelining.html">Software Pipelining</a></li>
<li class="toctree-l3"><a class="reference internal" href="../grid_blockspec.html">Grids and BlockSpecs</a></li>
<li class="toctree-l3 current active has-children"><a class="reference internal" href="index.html">Pallas TPU</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="details.html">Writing TPU kernels with Pallas</a></li>
<li class="toctree-l4"><a class="reference internal" href="pipelining.html">TPU Pipelining</a></li>
<li class="toctree-l4"><a class="reference internal" href="matmul.html">Matrix Multiplication</a></li>
<li class="toctree-l4 current active"><a class="current reference internal" href="#">Scalar Prefetch and Block-Sparse Computation</a></li>
<li class="toctree-l4"><a class="reference internal" href="distributed.html">Distributed Computing in Pallas for TPUs</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../gpu/index.html">Pallas:Mosaic GPU</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../gpu/reference.html">Writing Mosaic GPU kernels with Pallas</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../design/index.html">Pallas Design Notes</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../design/design.html">Pallas Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="../design/async_note.html">Pallas Async Operations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../CHANGELOG.html">Pallas Changelog</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/neural_network_with_tfds_data.html">Training a simple neural network, with tensorflow/datasets data loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Neural_Network_and_Data_Loading.html">Training a simple neural network, with PyTorch data loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/vmapped_log_probs.html">Autobatching for Bayesian inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/convolutions.html">Generalized convolutions in JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../xla_flags.html">List of XLA compiler flags</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sharded-computation.html">Introduction to parallel programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax-primitives.html">JAX Internals: primitives</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jaxpr.html">JAX internals: The jaxpr language</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../contributor_guide.html">Developer notes</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../contributing.html">Contributing to JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../developer.html">Building from source</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../investigating_a_regression.html">Investigating a regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../autodidax.html">Autodidax: JAX core from scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../autodidax2_part1.html">Autodidax2, part 1: JAX from scratch, again</a></li>

<li class="toctree-l2 has-children"><a class="reference internal" href="../../jep/index.html">JAX Enhancement Proposals (JEPs)</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../jep/263-prng.html">263: JAX PRNG Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/2026-custom-derivatives.html">2026: Custom JVP/VJP rules for JAX-transformable functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/4008-custom-vjp-update.html">4008: Custom VJP and `nondiff_argnums` update</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/4410-omnistaging.html">4410: Omnistaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/9263-typed-keys.html">9263: Typed keys &amp; pluggable RNGs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/9407-type-promotion.html">9407: Design of Type Promotion Semantics for JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/9419-jax-versioning.html">9419: Jax and Jaxlib versioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/10657-sequencing-effects.html">10657: Sequencing side-effects in JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/11830-new-remat-checkpoint.html">11830: `jax.remat` / `jax.checkpoint` new implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/12049-type-annotations.html">12049: Type Annotation Roadmap for JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/14273-shard-map.html">14273: `shard_map` (`shmap`) for simple per-device code</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/15856-jex.html">15856: `jax.extend`, an extensions module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/17111-shmap-transpose.html">17111: Efficient transposition of `shard_map` (and other maps)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/18137-numpy-scipy-scope.html">18137: Scope of JAX NumPy &amp; SciPy Wrappers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/25516-effver.html">25516: Effort-based versioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/28661-jax-array-protocol.html">28661: Supporting the `__jax_array__` protocol</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../extensions.html">Extension guides</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Writing_custom_interpreters_in_Jax.html">Writing custom Jaxpr interpreters in JAX</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../jax.extend.html"><code class="docutils literal notranslate"><span class="pre">jax.extend</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../jax.extend.core.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.core</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.extend.linear_util.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.linear_util</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.extend.mlir.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.mlir</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.extend.random.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.random</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../building_on_jax.html">Building on JAX</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../notes.html">Notes</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../api_compatibility.html">API compatibility</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../deprecation.html">Python and NumPy version support policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../async_dispatch.html">Asynchronous dispatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concurrency.html">Concurrency</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../gpu_memory_allocation.html">GPU memory allocation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../rank_promotion_warning.html">Rank promotion warning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../default_dtypes.html">Default dtypes and the X64 flag</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../jax.html">Public API: <code class="docutils literal notranslate"><span class="pre">jax</span></code> package</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../jax.numpy.html"><code class="docutils literal notranslate"><span class="pre">jax.numpy</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.fft.html">jax.numpy.fft.fft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.fft2.html">jax.numpy.fft.fft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.fftfreq.html">jax.numpy.fft.fftfreq</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.fftn.html">jax.numpy.fft.fftn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.fftshift.html">jax.numpy.fft.fftshift</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.hfft.html">jax.numpy.fft.hfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.ifft.html">jax.numpy.fft.ifft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.ifft2.html">jax.numpy.fft.ifft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.ifftn.html">jax.numpy.fft.ifftn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.ifftshift.html">jax.numpy.fft.ifftshift</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.ihfft.html">jax.numpy.fft.ihfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.irfft.html">jax.numpy.fft.irfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.irfft2.html">jax.numpy.fft.irfft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.irfftn.html">jax.numpy.fft.irfftn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.rfft.html">jax.numpy.fft.rfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.rfft2.html">jax.numpy.fft.rfft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.rfftfreq.html">jax.numpy.fft.rfftfreq</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.rfftn.html">jax.numpy.fft.rfftn</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../jax.scipy.html"><code class="docutils literal notranslate"><span class="pre">jax.scipy</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.scipy.stats.bernoulli.logpmf.html">jax.scipy.stats.bernoulli.logpmf</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.scipy.stats.bernoulli.pmf.html">jax.scipy.stats.bernoulli.pmf</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.scipy.stats.bernoulli.cdf.html">jax.scipy.stats.bernoulli.cdf</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.scipy.stats.bernoulli.ppf.html">jax.scipy.stats.bernoulli.ppf</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.lax.html"><code class="docutils literal notranslate"><span class="pre">jax.lax</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.random.html"><code class="docutils literal notranslate"><span class="pre">jax.random</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.sharding.html"><code class="docutils literal notranslate"><span class="pre">jax.sharding</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.debug.html"><code class="docutils literal notranslate"><span class="pre">jax.debug</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.dlpack.html"><code class="docutils literal notranslate"><span class="pre">jax.dlpack</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.distributed.html"><code class="docutils literal notranslate"><span class="pre">jax.distributed</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.dtypes.html"><code class="docutils literal notranslate"><span class="pre">jax.dtypes</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.ffi.html"><code class="docutils literal notranslate"><span class="pre">jax.ffi</span></code> module</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../jax.flatten_util.html"><code class="docutils literal notranslate"><span class="pre">jax.flatten_util</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.image.html"><code class="docutils literal notranslate"><span class="pre">jax.image</span></code> module</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../jax.nn.html"><code class="docutils literal notranslate"><span class="pre">jax.nn</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../jax.nn.initializers.html"><code class="docutils literal notranslate"><span class="pre">jax.nn.initializers</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.ops.html"><code class="docutils literal notranslate"><span class="pre">jax.ops</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.profiler.html"><code class="docutils literal notranslate"><span class="pre">jax.profiler</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.stages.html"><code class="docutils literal notranslate"><span class="pre">jax.stages</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.test_util.html"><code class="docutils literal notranslate"><span class="pre">jax.test_util</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.tree.html"><code class="docutils literal notranslate"><span class="pre">jax.tree</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.tree_util.html"><code class="docutils literal notranslate"><span class="pre">jax.tree_util</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.typing.html"><code class="docutils literal notranslate"><span class="pre">jax.typing</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.export.html"><code class="docutils literal notranslate"><span class="pre">jax.export</span></code> module</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../jax.extend.html"><code class="docutils literal notranslate"><span class="pre">jax.extend</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../jax.extend.core.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.core</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.extend.linear_util.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.linear_util</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.extend.mlir.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.mlir</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.extend.random.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.random</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../jax.example_libraries.html"><code class="docutils literal notranslate"><span class="pre">jax.example_libraries</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../jax.example_libraries.optimizers.html"><code class="docutils literal notranslate"><span class="pre">jax.example_libraries.optimizers</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.example_libraries.stax.html"><code class="docutils literal notranslate"><span class="pre">jax.example_libraries.stax</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../jax.experimental.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../jax.experimental.checkify.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.checkify</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.experimental.compilation_cache.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.compilation_cache</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.experimental.custom_dce.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.custom_dce</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.experimental.custom_partitioning.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.custom_partitioning</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.experimental.jet.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.jet</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.experimental.key_reuse.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.key_reuse</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.experimental.mesh_utils.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.mesh_utils</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.experimental.multihost_utils.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.multihost_utils</span></code> module</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../jax.experimental.pallas.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pallas</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../jax.experimental.pallas.mosaic_gpu.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pallas.mosaic_gpu</span></code> module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../jax.experimental.pallas.triton.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pallas.triton</span></code> module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../jax.experimental.pallas.tpu.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pallas.tpu</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.experimental.pjit.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pjit</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.experimental.serialize_executable.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.serialize_executable</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.experimental.shard_map.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.shard_map</span></code> module</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../jax.experimental.sparse.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.sparse</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-22"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.BCOO.html">jax.experimental.sparse.BCOO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_broadcast_in_dim.html">jax.experimental.sparse.bcoo_broadcast_in_dim</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_concatenate.html">jax.experimental.sparse.bcoo_concatenate</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_dot_general.html">jax.experimental.sparse.bcoo_dot_general</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_dot_general_sampled.html">jax.experimental.sparse.bcoo_dot_general_sampled</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_dynamic_slice.html">jax.experimental.sparse.bcoo_dynamic_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_extract.html">jax.experimental.sparse.bcoo_extract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_fromdense.html">jax.experimental.sparse.bcoo_fromdense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_gather.html">jax.experimental.sparse.bcoo_gather</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_multiply_dense.html">jax.experimental.sparse.bcoo_multiply_dense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_multiply_sparse.html">jax.experimental.sparse.bcoo_multiply_sparse</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_update_layout.html">jax.experimental.sparse.bcoo_update_layout</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_reduce_sum.html">jax.experimental.sparse.bcoo_reduce_sum</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_reshape.html">jax.experimental.sparse.bcoo_reshape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_slice.html">jax.experimental.sparse.bcoo_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_sort_indices.html">jax.experimental.sparse.bcoo_sort_indices</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_squeeze.html">jax.experimental.sparse.bcoo_squeeze</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_sum_duplicates.html">jax.experimental.sparse.bcoo_sum_duplicates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_todense.html">jax.experimental.sparse.bcoo_todense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_transpose.html">jax.experimental.sparse.bcoo_transpose</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.lib.html"><code class="docutils literal notranslate"><span class="pre">jax.lib</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.addressable_shards.html">jax.Array.addressable_shards</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.all.html">jax.Array.all</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.any.html">jax.Array.any</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.argmax.html">jax.Array.argmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.argmin.html">jax.Array.argmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.argpartition.html">jax.Array.argpartition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.argsort.html">jax.Array.argsort</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.astype.html">jax.Array.astype</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.at.html">jax.Array.at</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.choose.html">jax.Array.choose</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.clip.html">jax.Array.clip</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.compress.html">jax.Array.compress</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.committed.html">jax.Array.committed</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.conj.html">jax.Array.conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.conjugate.html">jax.Array.conjugate</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.copy.html">jax.Array.copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.copy_to_host_async.html">jax.Array.copy_to_host_async</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.cumprod.html">jax.Array.cumprod</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.cumsum.html">jax.Array.cumsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.device.html">jax.Array.device</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.diagonal.html">jax.Array.diagonal</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.dot.html">jax.Array.dot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.dtype.html">jax.Array.dtype</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.flat.html">jax.Array.flat</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.flatten.html">jax.Array.flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.global_shards.html">jax.Array.global_shards</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.imag.html">jax.Array.imag</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.is_fully_addressable.html">jax.Array.is_fully_addressable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.is_fully_replicated.html">jax.Array.is_fully_replicated</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.item.html">jax.Array.item</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.itemsize.html">jax.Array.itemsize</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.max.html">jax.Array.max</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.mean.html">jax.Array.mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.min.html">jax.Array.min</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.nbytes.html">jax.Array.nbytes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.ndim.html">jax.Array.ndim</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.nonzero.html">jax.Array.nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.prod.html">jax.Array.prod</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.ptp.html">jax.Array.ptp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.ravel.html">jax.Array.ravel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.real.html">jax.Array.real</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.repeat.html">jax.Array.repeat</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.reshape.html">jax.Array.reshape</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.round.html">jax.Array.round</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.searchsorted.html">jax.Array.searchsorted</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.shape.html">jax.Array.shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.sharding.html">jax.Array.sharding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.size.html">jax.Array.size</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.sort.html">jax.Array.sort</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.squeeze.html">jax.Array.squeeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.std.html">jax.Array.std</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.sum.html">jax.Array.sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.swapaxes.html">jax.Array.swapaxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.take.html">jax.Array.take</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.to_device.html">jax.Array.to_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.trace.html">jax.Array.trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.transpose.html">jax.Array.transpose</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.var.html">jax.Array.var</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.view.html">jax.Array.view</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.T.html">jax.Array.T</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.mT.html">jax.Array.mT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../about.html">About the project</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../faq.html">Frequently asked questions (FAQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changelog.html">Change log</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary.html">Glossary of terms</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../config_options.html">Configuration Options</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
        <div class="header-article-item">





<nav aria-label="Breadcrumb">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../advanced_guide.html" class="nav-link">Resources and Advanced Guides</a></li>
    
    
    <li class="breadcrumb-item"><i class="fa-solid fa-ellipsis"></i></li>
    
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">Pallas TPU</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Scalar...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/jax-ml/jax" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/pallas/tpu/sparse.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Scalar Prefetch and Block-Sparse Computation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-block-indexing-with-scalar-prefetch">Dynamic Block Indexing with Scalar Prefetch</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-block-dynamic-slice-with-scalar-prefetch">Example: Block Dynamic Slice with Scalar Prefetch</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sparse-kernels-representing-sparse-data">Sparse Kernels: Representing Sparse Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-sparse-dense-matrix-multiplication">Example: Sparse @ Dense Matrix Multiplication</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sparse-access-patterns-on-dense-data">Sparse Access Patterns on Dense Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-dense-dense-matrix-multiplication-with-a-block-sparse-output-mask">Example: Dense @ Dense Matrix Multiplication with a Block-Sparse Output Mask</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="scalar-prefetch-and-block-sparse-computation">
<span id="pallas-scalar-prefetch-guide"></span><h1>Scalar Prefetch and Block-Sparse Computation<a class="headerlink" href="#scalar-prefetch-and-block-sparse-computation" title="Link to this heading">#</a></h1>
<p>In this tutorial, we will cover the basics of block-sparse computing in Pallas. Sparse computation is a major reason to write custom Pallas kernels over simply using JAX/XLA, since it is generally difficult to express programs that perform a dynamic amount of computation in XLA due to static array shapes. In this tutorial we will learn how to use the scalar prefetch feature of Pallas in order to write block-sparse kernels that can dynamically skip over computation and blocks of memory.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">timeit</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">jnp</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">lax</span>
<span class="kn">from</span> <span class="nn">jax.experimental</span> <span class="kn">import</span> <span class="n">checkify</span>
<span class="kn">from</span> <span class="nn">jax.experimental</span> <span class="kn">import</span> <span class="n">pallas</span> <span class="k">as</span> <span class="n">pl</span>
<span class="kn">from</span> <span class="nn">jax.experimental.pallas</span> <span class="kn">import</span> <span class="n">tpu</span> <span class="k">as</span> <span class="n">pltpu</span>

<span class="k">assert</span> <span class="s2">&quot;TPU&quot;</span> <span class="ow">in</span> <span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">device_kind</span><span class="p">,</span> <span class="s2">&quot;Please run this notebook with TPU devices.&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Running on&quot;</span><span class="p">,</span> <span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">device_kind</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Running on TPU v5 lite
</pre></div>
</div>
</div>
</div>
<section id="dynamic-block-indexing-with-scalar-prefetch">
<h2>Dynamic Block Indexing with Scalar Prefetch<a class="headerlink" href="#dynamic-block-indexing-with-scalar-prefetch" title="Link to this heading">#</a></h2>
<p>We will be exploiting the ‚Äúscalar prefetch‚Äù feature of Pallas to enable us to write sparse kernels. Scalar prefetch allows you to pass in a small amount of data into SMEM (‚Äúscalar memory‚Äù) that is loaded before the start of the pipeline (‚Äúprefetch‚Äù). Because this data is loaded before the pipeline, it is available for use in the <code class="docutils literal notranslate"><span class="pre">index_map</span></code> for each BlockSpec, allowing you to perform data-dependent indexing calculations. The main goal of this tutorial is to go over common programming patterns that utilize this feature.</p>
<p>To use scalar prefetch, use <code class="docutils literal notranslate"><span class="pre">pltpu.PrefetchScalarGridSpec</span></code> in place of the standard <code class="docutils literal notranslate"><span class="pre">pl.GridSpec</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PrefetchScalarGridSpec</span><span class="p">:</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
    <span class="n">num_scalar_prefetch</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">grid</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
    <span class="n">in_specs</span><span class="p">:</span> <span class="n">PyTree</span><span class="p">[</span><span class="n">BlockSpec</span><span class="p">],</span>
    <span class="n">out_specs</span><span class="p">:</span> <span class="n">PyTree</span><span class="p">[</span><span class="n">BlockSpec</span><span class="p">],</span>
    <span class="n">scratch_shapes</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="n">MemorySpace</span><span class="p">,</span> <span class="o">...</span><span class="p">]):</span>
      <span class="o">...</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">num_scalar_prefetch</span></code> parameter indicates the number of scalar prefetch values. When this is set to a non-zero value, it changes the call signature of the kernel and index maps to expect additional prefetch values. The prefetch <code class="docutils literal notranslate"><span class="pre">Ref</span></code>s passed in to the <code class="docutils literal notranslate"><span class="pre">index_map</span></code> and kernel are all allocated in SMEM and are not partitioned into blocks as they do not have a BlockSpec defined. Moreover, the order of arguments to both <code class="docutils literal notranslate"><span class="pre">index_map</span></code> and kernel are always fixed and described below:</p>
<ul class="simple">
<li><p>Each <code class="docutils literal notranslate"><span class="pre">BlockSpec</span></code>‚Äôs <code class="docutils literal notranslate"><span class="pre">index_map</span></code> now expects the prefetch <code class="docutils literal notranslate"><span class="pre">Ref</span></code>s to come after the grid indices:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">index_map</span><span class="p">(</span><span class="o">*</span><span class="n">grid_indices</span><span class="p">,</span> <span class="o">*</span><span class="n">prefetch_refs</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The user-defined kernel expects prefetch <code class="docutils literal notranslate"><span class="pre">Ref</span></code>s to come before the input <code class="docutils literal notranslate"><span class="pre">Ref</span></code>s. Additionally, the scratch refs come after the output <code class="docutils literal notranslate"><span class="pre">Ref</span></code>s.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">kernel</span><span class="p">(</span><span class="o">*</span><span class="n">prefetch_refs</span><span class="p">,</span> <span class="o">*</span><span class="n">input_refs</span><span class="p">,</span> <span class="o">*</span><span class="n">output_refs</span><span class="p">,</span> <span class="o">*</span><span class="n">scratch_refs</span><span class="p">):</span>
    <span class="o">...</span>
</pre></div>
</div>
<ul class="simple">
<li><p>When calling a new kernel using <code class="docutils literal notranslate"><span class="pre">pallas_call</span></code>, the function returned by <code class="docutils literal notranslate"><span class="pre">pallas_call</span></code> also expects the scalar prefetch arguments to come before the inputs, e.g.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">kernel</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">pallas_call</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">kernel</span><span class="p">(</span><span class="o">*</span><span class="n">prefetch_args</span><span class="p">,</span> <span class="o">*</span><span class="n">input_args</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="example-block-dynamic-slice-with-scalar-prefetch">
<h2>Example: Block Dynamic Slice with Scalar Prefetch<a class="headerlink" href="#example-block-dynamic-slice-with-scalar-prefetch" title="Link to this heading">#</a></h2>
<p>Let‚Äôs begin with a basic example that demonstrates how to use the scalar prefetch feature. We will implement a block-aligned dynamic slice kernel which simply extracts a block out of larger array based on user-specified indices:</p>
<ol class="arabic simple">
<li><p>Outside of the kernel, we compute the block index to extract as: <code class="docutils literal notranslate"><span class="pre">block_idx</span> <span class="pre">=</span> <span class="pre">(start[0]</span> <span class="pre">//</span> <span class="pre">size[0],</span> <span class="pre">start[1]</span> <span class="pre">//</span> <span class="pre">size[1])</span></code></p></li>
<li><p>We pass <code class="docutils literal notranslate"><span class="pre">block_idx</span></code> as a scalar prefetch argument into <code class="docutils literal notranslate"><span class="pre">pallas_call</span></code>.</p></li>
<li><p>In our index map, we use the block index to select the corresponding block by returning <code class="docutils literal notranslate"><span class="pre">(block_idx[0],</span> <span class="pre">block_idx[1])</span></code>.</p></li>
</ol>
<p>Of course, this kernel is limited in that our slice sizes must fit inside of a kernel block (limited by VMEM size) and we can only start on size-aligned indices. A more advanced kernel would decouple the kernel block size with the slice size and allow non-aligned start indices.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">dynamic_slice_kernel</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">x_ref</span><span class="p">,</span> <span class="n">o_ref</span><span class="p">):</span>
  <span class="k">del</span> <span class="n">indices</span>
  <span class="n">o_ref</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">x_ref</span><span class="p">[</span><span class="o">...</span><span class="p">]</span>

<span class="nd">@checkify</span><span class="o">.</span><span class="n">checkify</span>
<span class="nd">@functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnums</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
<span class="k">def</span> <span class="nf">block_dynamic_slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">starts</span><span class="p">,</span> <span class="n">sizes</span><span class="p">):</span>
  <span class="n">grid_spec</span> <span class="o">=</span> <span class="n">pltpu</span><span class="o">.</span><span class="n">PrefetchScalarGridSpec</span><span class="p">(</span>
      <span class="n">num_scalar_prefetch</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
      <span class="n">grid</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
      <span class="n">in_specs</span><span class="o">=</span><span class="p">[</span><span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">(</span>
          <span class="n">sizes</span><span class="p">,</span>
          <span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">block_idx</span><span class="p">:</span> <span class="p">(</span><span class="n">block_idx</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">block_idx</span><span class="p">[</span><span class="mi">1</span><span class="p">]))],</span>
      <span class="n">out_specs</span><span class="o">=</span><span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">(</span><span class="n">sizes</span><span class="p">,</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">_</span><span class="p">:</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)),</span>
  <span class="p">)</span>

  <span class="n">kernel</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">pallas_call</span><span class="p">(</span>
    <span class="n">dynamic_slice_kernel</span><span class="p">,</span>
    <span class="n">grid_spec</span><span class="o">=</span><span class="n">grid_spec</span><span class="p">,</span>
    <span class="n">out_shape</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">sizes</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
  <span class="p">)</span>
  <span class="c1"># Checkify inserts a runtime assert that starts are divisible by block size.</span>
  <span class="n">checkify</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="n">starts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">%</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Starts must be divisible by size.&quot;</span><span class="p">)</span>
  <span class="n">checkify</span><span class="o">.</span><span class="n">check</span><span class="p">(</span><span class="n">starts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Starts must be divisible by size.&quot;</span><span class="p">)</span>
  <span class="n">block_idx</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">starts</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">//</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">starts</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">//</span> <span class="n">sizes</span><span class="p">[</span><span class="mi">1</span><span class="p">]])</span>
  <span class="k">return</span> <span class="n">kernel</span><span class="p">(</span><span class="n">block_idx</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

<span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">512</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">shape</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">int32</span><span class="p">),</span> <span class="n">shape</span><span class="p">)</span>
<span class="n">err</span><span class="p">,</span> <span class="n">result</span> <span class="o">=</span> <span class="n">block_dynamic_slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">starts</span><span class="o">=</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span> <span class="n">sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="n">err</span><span class="o">.</span><span class="n">throw</span><span class="p">()</span>
<span class="n">ref</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">dynamic_slice</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">start_indices</span><span class="o">=</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">256</span><span class="p">),</span> <span class="n">slice_sizes</span><span class="o">=</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">128</span><span class="p">))</span>
<span class="n">diff</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">result</span> <span class="o">-</span> <span class="n">ref</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Error |result - lax.dynamic_slice| =&quot;</span><span class="p">,</span> <span class="n">diff</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Error |result - lax.dynamic_slice| = 0
</pre></div>
</div>
</div>
</div>
</section>
<section id="sparse-kernels-representing-sparse-data">
<h2>Sparse Kernels: Representing Sparse Data<a class="headerlink" href="#sparse-kernels-representing-sparse-data" title="Link to this heading">#</a></h2>
<p>Before we dive into implementing sparse kernels, let‚Äôs first review how sparse matrices are represented. While there are several popular formats for storing sparse matrices, we will be following a blocked variant of the coordinate-list format (COO) in which we will store a matrix as a list of <code class="docutils literal notranslate"><span class="pre">(block_index,</span> <span class="pre">block_data)</span></code> pairs. All blocks that are not explicitly stored in the list are assumed to be zero, meaning we can save a significant amount of memory if there are many zero blocks in the matrix.</p>
<p>The following figure demonstrates how we convert a 4x4 dense matrix (left) into a block-COO format (right) with a block size of 2x2. Note that in the sparse format, we can avoid explicitly storing the upper-right block which consists of all zero elements.</p>
<p><img alt="block_coo" src="../../_images/block_coo.svg" /></p>
<p>We will use the following helper function to sample a block-sparse matrix. It returns a dense matrix used for checking our results, as well as a list of block data and indices for each axis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">generate_block_sparse_mat</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">blk_M</span><span class="p">,</span> <span class="n">blk_N</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Returns a sampled matrix and its block-sparse representation.</span>

<span class="sd">  Args:</span>
<span class="sd">    key: RNG Key.</span>
<span class="sd">    M: Major array dimension.</span>
<span class="sd">    N: Minor array dimension.</span>
<span class="sd">    blk_M: Block size along M dimension.</span>
<span class="sd">    blk_N: Block size along N dimension.</span>
<span class="sd">    p: Probability that a block will be non-zero.</span>
<span class="sd">    dtype: dtype of the sampled matrix.</span>

<span class="sd">  Returns:</span>
<span class="sd">    dense_mat: A (M, N) dense sampled array.</span>
<span class="sd">    block_data: A (num_blocks, blk_M, blk_N) array of data blocks representing</span>
<span class="sd">      the non-zero blocks of the matrix.</span>
<span class="sd">    indices_i: A (num_blocks,) array of block indices for the first axis.</span>
<span class="sd">    indices_j: A (num_blocks,) array of block indices for the second axis.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">mask_key</span><span class="p">,</span> <span class="n">blocks_key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
  <span class="n">num_blocks</span> <span class="o">=</span> <span class="p">(</span><span class="n">M</span> <span class="o">//</span> <span class="n">blk_M</span><span class="p">,</span> <span class="n">N</span> <span class="o">//</span> <span class="n">blk_N</span><span class="p">)</span>
  <span class="c1"># We first sample a block mask, denoting which blocks are nonzero.</span>
  <span class="n">block_mask</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">bernoulli</span><span class="p">(</span><span class="n">mask_key</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">num_blocks</span><span class="p">)</span>
  <span class="n">num_blocks</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">block_mask</span><span class="p">)</span>
  <span class="n">indices</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">block_mask</span><span class="p">)</span>
  <span class="c1"># For each non-zero block, we sample a block of random values.</span>
  <span class="n">block_data</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">blocks_key</span><span class="p">,</span>
                                  <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">,</span> <span class="n">blk_M</span><span class="p">,</span> <span class="n">blk_N</span><span class="p">),</span>
                                  <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
  <span class="c1"># For checking purposes, create the dense version of the sparse matrix.</span>
  <span class="n">dense_mat</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">blk</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">):</span>
    <span class="n">idx_i</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">blk</span><span class="p">]</span>
    <span class="n">idx_j</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="mi">1</span><span class="p">][</span><span class="n">blk</span><span class="p">]</span>
    <span class="n">slice_i</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="n">idx_i</span> <span class="o">*</span> <span class="n">blk_M</span><span class="p">,</span> <span class="p">(</span><span class="n">idx_i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">blk_M</span><span class="p">)</span>
    <span class="n">slice_j</span> <span class="o">=</span> <span class="nb">slice</span><span class="p">(</span><span class="n">idx_j</span> <span class="o">*</span> <span class="n">blk_N</span><span class="p">,</span> <span class="p">(</span><span class="n">idx_j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">blk_N</span><span class="p">)</span>
    <span class="n">dense_mat</span> <span class="o">=</span> <span class="n">dense_mat</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">slice_i</span><span class="p">,</span> <span class="n">slice_j</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">block_data</span><span class="p">[</span><span class="n">blk</span><span class="p">])</span>
  <span class="k">return</span> <span class="n">dense_mat</span><span class="p">,</span> <span class="n">block_data</span><span class="p">,</span> <span class="n">indices</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">indices</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="example-sparse-dense-matrix-multiplication">
<h2>Example: Sparse &#64; Dense Matrix Multiplication<a class="headerlink" href="#example-sparse-dense-matrix-multiplication" title="Link to this heading">#</a></h2>
<p>In our first example, we will multiply a sparse LHS matrix with a dense RHS matrix to produce a dense output.</p>
<p>We will structure our kernel grid with 2 loops - the outer loop over the columns of the RHS/output, and inner loop over the sparse blocks of the LHS. During each inner loop iteration, we load one block from the LHS and lookup the corresponding block on in the RHS using the block index of the contracting dimension (K). We multiply the two blocks together and accumulate into the correct output block. One outer loop iteration will compute a result for an entire column as depicted by the following diagram:</p>
<p><img alt="sparse_matmul" src="../../_images/sparse_matmul.svg" /></p>
<p>It is important that we group the block indices by row (e.g. <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">0,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">3]</span></code>) before we pass them into the kernel for two reasons. First, in our kernel we need to know when to initially zero-out the accumulator in the output ref, and it is easy to do so if the row indices are grouped. Second, the pipelining logic for Pallas does not allow us to re-visit blocks in the output <code class="docutils literal notranslate"><span class="pre">Ref</span></code> on non-consecutive iterations, and therefore we need to do all accumulation into an output block in consecutive kernel iterations. This is because the pipeline emitter will realize that we are loading the same output block on consecutive iterations and keep the block in VMEM. When we change output block Pallas will finally store the output into HBM and assume we never touch it again. Failure to access output blocks consecutively will result in incorrect values even though the kernel is otherwise logically correct.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">M</span> <span class="o">=</span> <span class="n">N</span> <span class="o">=</span> <span class="n">K</span> <span class="o">=</span> <span class="mi">16384</span>
<span class="n">blk_M</span> <span class="o">=</span> <span class="n">blk_N</span> <span class="o">=</span> <span class="n">blk_K</span> <span class="o">=</span> <span class="mi">512</span>


<span class="k">def</span> <span class="nf">dsd_kernel</span><span class="p">(</span><span class="n">idxs_i_ref</span><span class="p">,</span> <span class="n">idxs_k_ref</span><span class="p">,</span> <span class="c1"># Scalar prefetch inputs.</span>
               <span class="n">x_ref</span><span class="p">,</span> <span class="n">y_ref</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">o_ref</span><span class="p">,</span> <span class="c1"># Kernel inputs.</span>
               <span class="n">accum_scratch</span><span class="p">,</span>
               <span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;A DSD (Dense = Sparse @ Dense) matmul kernel.&quot;&quot;&quot;</span>
  <span class="k">del</span> <span class="n">idxs_k_ref</span>
  <span class="n">blk_idx</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
  <span class="n">is_start</span> <span class="o">=</span> <span class="n">blk_idx</span> <span class="o">==</span> <span class="mi">0</span>
  <span class="n">changed_blocks</span> <span class="o">=</span> <span class="p">(</span><span class="n">idxs_i_ref</span><span class="p">[</span><span class="n">blk_idx</span><span class="p">]</span> <span class="o">!=</span> <span class="n">idxs_i_ref</span><span class="p">[</span><span class="n">jnp</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">blk_idx</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">)])</span>
  <span class="nd">@pl</span><span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">is_start</span> <span class="o">|</span> <span class="n">changed_blocks</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">_</span><span class="p">():</span>
    <span class="n">accum_scratch</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">accum_scratch</span><span class="p">)</span>
  <span class="n">accum_scratch</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">+=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_ref</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:],</span> <span class="n">y_ref</span><span class="p">[</span><span class="o">...</span><span class="p">],</span> <span class="n">preferred_element_type</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>

  <span class="n">next_block_change</span> <span class="o">=</span> <span class="p">(</span><span class="n">idxs_i_ref</span><span class="p">[</span><span class="n">blk_idx</span><span class="p">]</span> <span class="o">!=</span> <span class="n">idxs_i_ref</span><span class="p">[</span><span class="n">jnp</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">blk_idx</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_blocks</span><span class="p">)])</span>
  <span class="n">is_end</span> <span class="o">=</span> <span class="n">blk_idx</span> <span class="o">==</span> <span class="p">(</span><span class="n">num_blocks</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
  <span class="nd">@pl</span><span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">is_end</span> <span class="o">|</span> <span class="n">next_block_change</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">_</span><span class="p">():</span>
    <span class="n">o_ref</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">accum_scratch</span><span class="p">[</span><span class="o">...</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">o_ref</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">x_map</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">blk_idx</span><span class="p">,</span> <span class="n">blk_idxs_i</span><span class="p">,</span> <span class="n">blk_idxs_k</span><span class="p">):</span>
  <span class="k">del</span> <span class="n">j</span><span class="p">,</span> <span class="n">blk_idxs_i</span><span class="p">,</span> <span class="n">blk_idxs_k</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">blk_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">y_map</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">blk_idx</span><span class="p">,</span> <span class="n">blk_idxs_i</span><span class="p">,</span> <span class="n">blk_idxs_k</span><span class="p">):</span>
  <span class="k">del</span> <span class="n">blk_idxs_i</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">blk_idxs_k</span><span class="p">[</span><span class="n">blk_idx</span><span class="p">],</span> <span class="n">j</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">o_map</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">blk_idx</span><span class="p">,</span> <span class="n">blk_idxs_i</span><span class="p">,</span> <span class="n">blk_idxs_k</span><span class="p">):</span>
  <span class="k">del</span> <span class="n">blk_idxs_k</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">blk_idxs_i</span><span class="p">[</span><span class="n">blk_idx</span><span class="p">],</span> <span class="n">j</span><span class="p">)</span>

<span class="p">(</span><span class="n">X_dense</span><span class="p">,</span> <span class="n">X_blocks</span><span class="p">,</span> <span class="n">indices_i</span><span class="p">,</span> <span class="n">indices_k</span><span class="p">)</span> <span class="o">=</span> <span class="n">generate_block_sparse_mat</span><span class="p">(</span>
    <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">blk_M</span><span class="p">,</span> <span class="n">blk_K</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
<span class="n">num_blocks</span> <span class="o">=</span> <span class="n">X_blocks</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
<span class="n">zeros</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
<span class="n">out_shape</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>

<span class="n">grid_spec</span> <span class="o">=</span> <span class="n">pltpu</span><span class="o">.</span><span class="n">PrefetchScalarGridSpec</span><span class="p">(</span>
    <span class="n">num_scalar_prefetch</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="c1"># Note that while num_blocks is static here, Pallas does support</span>
    <span class="c1"># dynamic grid sizes.</span>
    <span class="n">grid</span><span class="o">=</span><span class="p">(</span><span class="n">N</span> <span class="o">//</span> <span class="n">blk_N</span><span class="p">,</span> <span class="n">num_blocks</span><span class="p">),</span>
    <span class="n">in_specs</span><span class="o">=</span><span class="p">[</span><span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">blk_M</span><span class="p">,</span> <span class="n">blk_K</span><span class="p">),</span> <span class="n">x_map</span><span class="p">),</span>
              <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="n">blk_K</span><span class="p">,</span> <span class="n">blk_N</span><span class="p">),</span> <span class="n">y_map</span><span class="p">),</span>
              <span class="c1"># Placeholder for a zeros-array used by input_output_aliases.</span>
              <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="n">blk_M</span><span class="p">,</span> <span class="n">blk_N</span><span class="p">),</span> <span class="n">o_map</span><span class="p">),</span>
              <span class="p">],</span>
    <span class="n">out_specs</span><span class="o">=</span><span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="n">blk_M</span><span class="p">,</span> <span class="n">blk_N</span><span class="p">),</span> <span class="n">o_map</span><span class="p">),</span>
    <span class="n">scratch_shapes</span><span class="o">=</span><span class="p">[</span><span class="n">pltpu</span><span class="o">.</span><span class="n">VMEM</span><span class="p">((</span><span class="n">blk_M</span><span class="p">,</span> <span class="n">blk_N</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)]</span>
<span class="p">)</span>
<span class="n">kernel</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">pallas_call</span><span class="p">(</span>
  <span class="n">dsd_kernel</span><span class="p">,</span>
  <span class="n">grid_spec</span><span class="o">=</span><span class="n">grid_spec</span><span class="p">,</span>
  <span class="n">out_shape</span><span class="o">=</span><span class="n">out_shape</span><span class="p">,</span>
  <span class="c1"># We use input-output aliases to zero-out o_ref for blocks that we never</span>
  <span class="c1"># visit. By passing in an array of zeros we avoid having o_ref start with</span>
  <span class="c1"># uninitialized values.</span>
  <span class="n">input_output_aliases</span><span class="o">=</span><span class="p">{</span><span class="mi">4</span><span class="p">:</span> <span class="mi">0</span><span class="p">},</span>  <span class="c1"># Map zeros to o_ref.</span>
<span class="p">)</span>
<span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">indices_i</span><span class="p">,</span> <span class="n">indices_k</span><span class="p">,</span> <span class="n">X_blocks</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">zeros</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">kernel</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

<span class="n">ref</span> <span class="o">=</span> <span class="n">X_dense</span> <span class="o">@</span> <span class="n">Y</span>
<span class="n">diff</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">ref</span> <span class="o">-</span> <span class="n">result</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;mean |result - ref|:&#39;</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">diff</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>mean |result - ref|: 0
</pre></div>
</div>
</div>
</div>
<p>We can do a quick benchmark to compare the performance of our sparse kernel compared to a dense matmul in JAX. On a TPU v5e chip, this kernel achieves a roughly ~6x speed increase compared to the theoretical 10x from the sparsity factor.</p>
<p>There are a few main tips for performance here, mainly centered around reducing the communication overhead between HBM/VMEM:</p>
<ul class="simple">
<li><p>Using <code class="docutils literal notranslate"><span class="pre">dtype=jnp.bfloat16</span></code> is critical for performance since it reduces memory bandwidth by half.</p></li>
<li><p>Using larger block sizes also helps, since matrix multiply is an <span class="math notranslate nohighlight">\(O(N^3)\)</span> compute and <span class="math notranslate nohighlight">\(O(N^2)\)</span> memory operation. As <span class="math notranslate nohighlight">\(N\)</span> grows larger, the kernel becomes compute-bound. However, a counter-argument to this in practice is that smaller block sizes also enables data to be more sparse, so this is a parameter that should be selected carefully.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Benchmark Sparse Pallas kernel vs reference JAX implementation</span>

<span class="k">def</span> <span class="nf">benchmark</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">ntrials</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="c1"># Compile function first</span>
    <span class="n">jax</span><span class="o">.</span><span class="n">block_until_ready</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>
    <span class="c1"># Time function</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">timeit</span><span class="o">.</span><span class="n">timeit</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">block_until_ready</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)),</span>
                           <span class="n">number</span><span class="o">=</span><span class="n">ntrials</span><span class="p">)</span>
    <span class="n">time</span> <span class="o">=</span> <span class="n">result</span> <span class="o">/</span> <span class="n">ntrials</span>
    <span class="k">return</span> <span class="n">time</span>
  <span class="k">return</span> <span class="n">run</span>


<span class="n">n_trials</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">pallas_impl</span> <span class="o">=</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">kernel</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
<span class="n">time</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span><span class="n">pallas_impl</span><span class="p">,</span> <span class="n">n_trials</span><span class="p">)(</span><span class="n">indices_i</span><span class="p">,</span> <span class="n">indices_k</span><span class="p">,</span> <span class="n">X_blocks</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">zeros</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sparse Kernel: </span><span class="si">%.3f</span><span class="s2"> ms (avg over </span><span class="si">%d</span><span class="s2"> trials)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">time</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">n_trials</span><span class="p">))</span>

<span class="n">ref_impl</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span>
<span class="n">time</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span><span class="n">ref_impl</span><span class="p">,</span> <span class="n">n_trials</span><span class="p">)(</span><span class="n">X_dense</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reference: </span><span class="si">%.3f</span><span class="s2"> ms (avg over </span><span class="si">%d</span><span class="s2"> trials)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">time</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">n_trials</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sparse Kernel: 8.136 ms (avg over 100 trials)
Reference: 46.953 ms (avg over 100 trials)
</pre></div>
</div>
</div>
</div>
</section>
<section id="sparse-access-patterns-on-dense-data">
<h2>Sparse Access Patterns on Dense Data<a class="headerlink" href="#sparse-access-patterns-on-dense-data" title="Link to this heading">#</a></h2>
<p>In our previous example we considered the case when the data itself is sparse. This manifested itself in the kernel structure as a dimension in the kernel grid that was dynamic and looped over the number of nonzero blocks (<code class="docutils literal notranslate"><span class="pre">num_blocks</span></code>).</p>
<p>A second useful programming pattern emerges when the underlying data is dense, but we wish to perform sparse computation over it. Our kernel grid in this case will be dense, but we wish to skip over some blocks in the grid as indicated by a block-sparse mask. This type of programming pattern commonly arises when using masks in many machine learning applications, such as causal or local masks in self-attention. In these cases, we can entirely skip over computation in blocks where the mask is zeroed-out. Examples of this programming pattern can be found in the Splash Attention and Grouped Matrix Multiplication kernels located in <code class="docutils literal notranslate"><span class="pre">jax/experimental/pallas/ops/tpu</span></code>, or in PyTorch‚Äôs <a class="reference external" href="https://pytorch.org/blog/flexattention/">FlexAttention</a>.</p>
<p>The main performance consideration with dealing with a sparse access pattern on dense data is the interaction with pipelining. On any given kernel iteration, the Pallas pipeline emitter will attempt to prefetch the next block of data by calling the <code class="docutils literal notranslate"><span class="pre">index_map</span></code> for each <code class="docutils literal notranslate"><span class="pre">BlockSpec</span></code> on the next iteration of the grid. However, if our computation is sparse we may be skipping the computation for the next block in the grid, so we need some method to tell the pipeline instead begin fetching the <em>next block that we are not skipping</em>. In order to do this, we need to construct <em>prefetch maps</em> which contains indices to the next non-skipped block of data for each kernel input. The following diagram illustrates how a prefetch map could be constructed for a block-sparse mask that is stored in a COO-like format.</p>
<p><img alt="prefetch_map" src="../../_images/prefetch_map.svg" /></p>
<p><em>Left: A sparse access pattern, where the color blue denotes blocks with non-zero masks that we need to compute. Right: The prefetch map, where each element of the array contains the index of the next non-zero block data.</em></p>
<p>Once the prefetch map has been constructed, we can pass the map as a scalar prefetch argument and query it in the <code class="docutils literal notranslate"><span class="pre">index_map</span></code> function of the BlockSpec.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">mask_index_map</span><span class="p">(</span><span class="n">prefetch_map</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="o">...</span><span class="p">):</span>
  <span class="n">next_nonzero_block</span> <span class="o">=</span> <span class="n">prefetch_map</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">next_nonzero_block</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>We can construct similar index maps for the other inputs to the kernel. For dense inputs you will most likely need to construct prefetch maps which point to the next non-zero block index in the grid. Our next example will provide an example of using these prefetch maps.</p>
</section>
<section id="example-dense-dense-matrix-multiplication-with-a-block-sparse-output-mask">
<h2>Example: Dense &#64; Dense Matrix Multiplication with a Block-Sparse Output Mask<a class="headerlink" href="#example-dense-dense-matrix-multiplication-with-a-block-sparse-output-mask" title="Link to this heading">#</a></h2>
<p>In our next example we will cover dense matrix multiplication fused with a sparse output mask using a prefetch map to improve pipelining performance. We will use the mask to selectively skip computing output blocks that are zeroed-out, therefore saving on computation costs.</p>
<p>As we will be working with a sparse mask, we will begin by implementing a function that converts an <code class="docutils literal notranslate"><span class="pre">N</span> <span class="pre">x</span> <span class="pre">M</span></code> mask stored in dense format into a block-sparse format. We additionally need to compute prefetch maps to help the pipeline emitter know which block to fetch next. In total, our <code class="docutils literal notranslate"><span class="pre">sparsify_mask</span></code> function computes:</p>
<ul class="simple">
<li><p>A <code class="docutils literal notranslate"><span class="pre">block_mask</span></code> of shape <code class="docutils literal notranslate"><span class="pre">(num_N_blocks,</span> <span class="pre">num_M_blocks)</span></code> indicating if a block is all-zeros (value <code class="docutils literal notranslate"><span class="pre">0</span></code>) or contains non-zero elements (value <code class="docutils literal notranslate"><span class="pre">1</span></code>). If the <code class="docutils literal notranslate"><span class="pre">block_mask</span></code> has a value of 0 we can skip computing the block in the kernel.</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">prefetch_mask</span></code> array of shape <code class="docutils literal notranslate"><span class="pre">(num_N_blocks,</span> <span class="pre">num_M_blocks)</span></code> consisting of indices into <code class="docutils literal notranslate"><span class="pre">mask_data</span></code> for the next non-zero block.</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">prefetch_i</span></code> array of shape <code class="docutils literal notranslate"><span class="pre">(num_N_blocks,</span> <span class="pre">num_M_blocks)</span></code> consisting of the next non-masked <code class="docutils literal notranslate"><span class="pre">i</span></code> index of the mask.</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">prefetch_j</span></code> array of shape <code class="docutils literal notranslate"><span class="pre">(num_N_blocks,</span> <span class="pre">num_M_blocks)</span></code> consisting of the next non-masked <code class="docutils literal notranslate"><span class="pre">j</span></code> index of the mask.</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">mask_data</span></code> array of shape <code class="docutils literal notranslate"><span class="pre">(num_blocks,</span> <span class="pre">blk_N,</span> <span class="pre">blk_M)</span></code> containing data for non-zero blocks of the mask.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sparsify_mask</span><span class="p">(</span><span class="n">mask</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span>
                  <span class="n">block_shape</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Preprocesses a mask into a sparse representation.</span>

<span class="sd">  Args:</span>
<span class="sd">    mask: A boolean array of shape [M, N]</span>
<span class="sd">    block_shape: The size of a single block.</span>

<span class="sd">  Returns:</span>
<span class="sd">    block_mask: A block_shape array of booleans indicating whether a block</span>
<span class="sd">      is all-zeros (0) or contains non-zero elements (1).</span>
<span class="sd">    prefetch_mask: A block_shape array of integers indicating the index of the</span>
<span class="sd">      next non-zero block.</span>
<span class="sd">    mask_data: A (num_blocks, block_shape) array containing</span>
<span class="sd">      the data for non-zero blocks of the mask.</span>
<span class="sd">  &quot;&quot;&quot;</span>
  <span class="n">M</span><span class="p">,</span> <span class="n">N</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">shape</span>
  <span class="n">bm</span><span class="p">,</span> <span class="n">bn</span> <span class="o">=</span> <span class="n">block_shape</span>

  <span class="n">block_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">M</span> <span class="o">//</span> <span class="n">bm</span><span class="p">,</span> <span class="n">N</span> <span class="o">//</span> <span class="n">bn</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">mask</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
  <span class="n">mask_types_finder</span> <span class="o">=</span> <span class="p">[]</span>
  <span class="n">mask_data</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="n">next_mask_type_idx</span> <span class="o">=</span> <span class="mi">0</span>
  <span class="n">prefetch_mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">block_mask</span><span class="p">)</span>
  <span class="n">next_i</span> <span class="o">=</span> <span class="p">(</span><span class="n">M</span> <span class="o">//</span> <span class="n">bm</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
  <span class="n">next_j</span> <span class="o">=</span> <span class="p">(</span><span class="n">N</span> <span class="o">//</span> <span class="n">bn</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
  <span class="n">prefetch_i</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">block_mask</span><span class="p">)</span>
  <span class="n">prefetch_j</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">block_mask</span><span class="p">)</span>
  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">M</span> <span class="o">//</span> <span class="n">bm</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span> <span class="o">//</span> <span class="n">bn</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
      <span class="n">mask_block</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[</span><span class="n">i</span> <span class="o">*</span> <span class="n">bm</span> <span class="p">:(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">bm</span><span class="p">,</span>
                        <span class="n">j</span> <span class="o">*</span> <span class="n">bn</span> <span class="p">:(</span><span class="n">j</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">bn</span><span class="p">]</span>
      <span class="n">is_nonzero</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">mask_block</span><span class="p">)</span>
      <span class="k">if</span> <span class="n">is_nonzero</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
          <span class="n">type_index</span> <span class="o">=</span> <span class="n">mask_types_finder</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">mask_block</span><span class="p">))</span>
        <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
          <span class="n">type_index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">mask_types_finder</span><span class="p">)</span>
          <span class="n">mask_types_finder</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">mask_block</span><span class="p">))</span>
          <span class="n">mask_data</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mask_block</span><span class="p">)</span>
        <span class="n">next_mask_type_idx</span> <span class="o">=</span> <span class="n">type_index</span>
        <span class="n">next_i</span> <span class="o">=</span> <span class="n">i</span>
        <span class="n">next_j</span> <span class="o">=</span> <span class="n">j</span>
      <span class="k">else</span><span class="p">:</span>
        <span class="n">type_index</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
      <span class="n">block_mask</span> <span class="o">=</span> <span class="n">block_mask</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">is_nonzero</span><span class="p">)</span>
      <span class="n">prefetch_mask</span> <span class="o">=</span> <span class="n">prefetch_mask</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">next_mask_type_idx</span><span class="p">)</span>
      <span class="n">prefetch_i</span> <span class="o">=</span> <span class="n">prefetch_i</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">next_i</span><span class="p">)</span>
      <span class="n">prefetch_j</span> <span class="o">=</span> <span class="n">prefetch_j</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">next_j</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">block_mask</span><span class="p">,</span> <span class="n">prefetch_mask</span><span class="p">,</span> <span class="n">prefetch_i</span><span class="p">,</span> <span class="n">prefetch_j</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">mask_data</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>In terms of the structure of the kernel, we use the same grid pattern as the standard matrix multiplication kernel we covered in previous tutorials with a 3 loops over the <code class="docutils literal notranslate"><span class="pre">N</span></code>, <code class="docutils literal notranslate"><span class="pre">M</span></code>, and <code class="docutils literal notranslate"><span class="pre">K</span></code> dimensions. Within the kernel itself, we first check the <code class="docutils literal notranslate"><span class="pre">block_mask</span></code> to see if the mask for the current output block was all zeros. If the mask is all zeros, we can skip computation and move onto the next block; otherwise we need to compute the matrix multiplication and then mask the result.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">M</span> <span class="o">=</span> <span class="n">N</span> <span class="o">=</span> <span class="n">K</span> <span class="o">=</span> <span class="mi">16384</span>
<span class="n">blk_M</span> <span class="o">=</span> <span class="n">blk_N</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">blk_K</span> <span class="o">=</span> <span class="mi">1024</span>

<span class="k">def</span> <span class="nf">sparse_mask_matmul</span><span class="p">(</span>
    <span class="n">block_mask_ref</span><span class="p">,</span> <span class="n">prefetch_mask</span><span class="p">,</span> <span class="n">prefetch_i</span><span class="p">,</span> <span class="n">prefetch_j</span><span class="p">,</span> <span class="c1"># Scalar prefetch inputs.</span>
    <span class="n">x_ref</span><span class="p">,</span> <span class="n">y_ref</span><span class="p">,</span> <span class="n">mask_ref</span><span class="p">,</span> <span class="n">o_ref</span><span class="p">,</span>  <span class="c1"># Kernel inputs.</span>
    <span class="n">accum_scratch</span>
    <span class="p">):</span>
  <span class="k">del</span> <span class="n">prefetch_mask</span><span class="p">,</span> <span class="n">prefetch_i</span><span class="p">,</span> <span class="n">prefetch_j</span>
  <span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">pl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">pl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
  <span class="n">should_compute</span> <span class="o">=</span> <span class="n">block_mask_ref</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span>
  <span class="nd">@pl</span><span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">k</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">_</span><span class="p">():</span>
    <span class="n">o_ref</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">o_ref</span><span class="p">)</span>
    <span class="n">accum_scratch</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">accum_scratch</span><span class="p">[</span><span class="o">...</span><span class="p">])</span>

  <span class="c1"># We only compute the output for blocks with non-zero masks.</span>
  <span class="c1"># Otherwise we skip the computation entirely.</span>
  <span class="nd">@pl</span><span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">should_compute</span><span class="p">)</span>
  <span class="k">def</span> <span class="nf">_</span><span class="p">():</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x_ref</span><span class="p">[</span><span class="o">...</span><span class="p">],</span> <span class="n">y_ref</span><span class="p">[</span><span class="o">...</span><span class="p">],</span> <span class="n">preferred_element_type</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">accum_scratch</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">+=</span> <span class="n">result</span>
    <span class="nd">@pl</span><span class="o">.</span><span class="n">when</span><span class="p">(</span><span class="n">k</span> <span class="o">==</span> <span class="n">pl</span><span class="o">.</span><span class="n">num_programs</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">_</span><span class="p">():</span>
      <span class="n">o_ref</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="n">mask_ref</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">*</span> <span class="n">accum_scratch</span><span class="p">[</span><span class="o">...</span><span class="p">])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">o_ref</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">M</span><span class="p">,</span> <span class="n">K</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">mask</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>
<span class="n">block_mask</span><span class="p">,</span> <span class="n">prefetch_mask</span><span class="p">,</span> <span class="n">prefetch_i</span><span class="p">,</span> <span class="n">prefetch_j</span><span class="p">,</span> <span class="n">sparse_mask_data</span> <span class="o">=</span> <span class="n">sparsify_mask</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="p">(</span><span class="n">blk_M</span><span class="p">,</span> <span class="n">blk_N</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">x_map</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">block_mask</span><span class="p">,</span> <span class="n">prefetch_mask</span><span class="p">,</span> <span class="n">prefetch_i</span><span class="p">,</span> <span class="n">prefetch_j</span><span class="p">):</span>
  <span class="k">del</span> <span class="n">prefetch_mask</span><span class="p">,</span> <span class="n">prefetch_j</span>
  <span class="c1"># Zero-out the k index if the mask is zero, to avoid constantly fetching</span>
  <span class="c1"># new blocks in the inner loop for blocks we are skipping.</span>
  <span class="n">k_fetch</span> <span class="o">=</span> <span class="p">(</span><span class="n">block_mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">k</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">prefetch_i</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="n">k_fetch</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">y_map</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">block_mask</span><span class="p">,</span> <span class="n">prefetch_mask</span><span class="p">,</span> <span class="n">prefetch_i</span><span class="p">,</span> <span class="n">prefetch_j</span><span class="p">):</span>
  <span class="k">del</span> <span class="n">prefetch_mask</span><span class="p">,</span> <span class="n">prefetch_i</span>
  <span class="n">k_fetch</span> <span class="o">=</span> <span class="p">(</span><span class="n">block_mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)</span> <span class="o">*</span> <span class="n">k</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">k_fetch</span><span class="p">,</span> <span class="n">prefetch_j</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">mask_map</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">block_mask</span><span class="p">,</span> <span class="n">prefetch_mask</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span><span class="p">):</span>
  <span class="k">del</span> <span class="n">k</span><span class="p">,</span> <span class="n">block_mask</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">prefetch_mask</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">],</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">o_map</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span><span class="p">):</span>
  <span class="k">del</span> <span class="n">k</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)</span>

<span class="n">grid_spec</span> <span class="o">=</span> <span class="n">pltpu</span><span class="o">.</span><span class="n">PrefetchScalarGridSpec</span><span class="p">(</span>
    <span class="n">num_scalar_prefetch</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
    <span class="n">grid</span><span class="o">=</span><span class="p">(</span><span class="n">M</span> <span class="o">//</span> <span class="n">blk_M</span><span class="p">,</span> <span class="n">N</span> <span class="o">//</span> <span class="n">blk_N</span><span class="p">,</span> <span class="n">K</span> <span class="o">//</span> <span class="n">blk_K</span><span class="p">),</span>
    <span class="n">in_specs</span><span class="o">=</span><span class="p">[</span><span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="n">blk_M</span><span class="p">,</span> <span class="n">blk_K</span><span class="p">),</span> <span class="n">x_map</span><span class="p">),</span>
              <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="n">blk_K</span><span class="p">,</span> <span class="n">blk_N</span><span class="p">),</span> <span class="n">y_map</span><span class="p">),</span>
              <span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">blk_M</span><span class="p">,</span> <span class="n">blk_N</span><span class="p">),</span> <span class="n">mask_map</span><span class="p">)],</span>
    <span class="n">out_specs</span><span class="o">=</span><span class="n">pl</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">((</span><span class="n">blk_M</span><span class="p">,</span> <span class="n">blk_N</span><span class="p">),</span> <span class="n">o_map</span><span class="p">),</span>
    <span class="n">scratch_shapes</span><span class="o">=</span><span class="p">[</span><span class="n">pltpu</span><span class="o">.</span><span class="n">VMEM</span><span class="p">((</span><span class="n">blk_M</span><span class="p">,</span> <span class="n">blk_N</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)]</span>
<span class="p">)</span>
<span class="n">kernel</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">pallas_call</span><span class="p">(</span>
  <span class="n">sparse_mask_matmul</span><span class="p">,</span>
  <span class="n">grid_spec</span><span class="o">=</span><span class="n">grid_spec</span><span class="p">,</span>
  <span class="n">out_shape</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">((</span><span class="n">M</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">),</span>
<span class="p">)</span>
<span class="n">args</span> <span class="o">=</span> <span class="p">(</span><span class="n">block_mask</span><span class="p">,</span> <span class="n">prefetch_mask</span><span class="p">,</span> <span class="n">prefetch_i</span><span class="p">,</span> <span class="n">prefetch_j</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">sparse_mask_data</span><span class="p">)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">kernel</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>

<span class="n">ref</span> <span class="o">=</span> <span class="n">mask</span> <span class="o">*</span> <span class="p">(</span><span class="n">X</span> <span class="o">@</span> <span class="n">Y</span><span class="p">)</span>
<span class="n">diff</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">ref</span> <span class="o">-</span> <span class="n">result</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;mean |result - ref|:&#39;</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">diff</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>mean |result - ref|: 1.0252e-05
</pre></div>
</div>
</div>
</div>
<p>Now let‚Äôs compare performance versus a naive dense implementation. On TPU v5e, we achieve around a ~1.8x speed increase with the sparse kernel, compared to a theoretical best-case of 2x from using a lower triangular mask and only visiting half of the possible outputs.</p>
<p>We would generally expect performance to get closer to the theoretical peak as our inputs get larger, since a few of the main reasons why we don‚Äôt exactly reach theoretical performance are:</p>
<ul class="simple">
<li><p>We skip slightly less than half of computation since the blocks along the diagonal are mixed 0s and 1s, and for mixed blocks we need to compute the entire block. With larger inputs, our overhead for mixed blocks becomes smaller relative to the overall computation.</p></li>
<li><p>The pipeline bubble also accounts for a less percentage of the overall runtime as inputs become larger.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">n_trials</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">pallas_impl</span> <span class="o">=</span> <span class="k">lambda</span> <span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">kernel</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
<span class="n">time</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span><span class="n">pallas_impl</span><span class="p">,</span> <span class="n">n_trials</span><span class="p">)(</span><span class="n">block_mask</span><span class="p">,</span> <span class="n">prefetch_mask</span><span class="p">,</span> <span class="n">prefetch_i</span><span class="p">,</span> <span class="n">prefetch_j</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">sparse_mask_data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sparse Kernel: </span><span class="si">%.3f</span><span class="s2"> ms (avg over </span><span class="si">%d</span><span class="s2"> trials)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">time</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">n_trials</span><span class="p">))</span>

<span class="n">ref_impl</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="k">lambda</span> <span class="n">mask</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">mask</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">y</span><span class="p">))</span>
<span class="n">time</span> <span class="o">=</span> <span class="n">benchmark</span><span class="p">(</span><span class="n">ref_impl</span><span class="p">,</span> <span class="n">n_trials</span><span class="p">)(</span><span class="n">mask</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reference: </span><span class="si">%.3f</span><span class="s2"> ms (avg over </span><span class="si">%d</span><span class="s2"> trials)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">time</span> <span class="o">*</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">n_trials</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sparse Kernel: 28.648 ms (avg over 100 trials)
Reference: 49.988 ms (avg over 100 trials)
</pre></div>
</div>
</div>
</div>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="matmul.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Matrix Multiplication</p>
      </div>
    </a>
    <a class="right-next"
       href="distributed.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Distributed Computing in Pallas for TPUs</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-block-indexing-with-scalar-prefetch">Dynamic Block Indexing with Scalar Prefetch</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-block-dynamic-slice-with-scalar-prefetch">Example: Block Dynamic Slice with Scalar Prefetch</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sparse-kernels-representing-sparse-data">Sparse Kernels: Representing Sparse Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-sparse-dense-matrix-multiplication">Example: Sparse @ Dense Matrix Multiplication</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sparse-access-patterns-on-dense-data">Sparse Access Patterns on Dense Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-dense-dense-matrix-multiplication-with-a-block-sparse-output-mask">Example: Dense @ Dense Matrix Multiplication with a Block-Sparse Output Mask</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The JAX authors
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2024, The JAX Authors.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>