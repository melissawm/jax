
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>GPU performance tips &#8212; JAX  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/style.css?v=7143c0a5" />
    <link rel="stylesheet" href="_static/style.css" type="text/css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=30646c52"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'gpu_performance_tips';</script>
    <link rel="icon" href="_static/favicon.png"/>
    <link rel="author" title="About these documents" href="about.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Profiling computation" href="profiling.html" />
    <link rel="prev" title="Persistent compilation cache" href="persistent_compilation_cache.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/jax_logo_250px.png" class="logo__image only-light" alt="JAX  documentation - Home"/>
    <script>document.write(`<img src="_static/jax_logo_250px.png" class="logo__image only-dark" alt="JAX  documentation - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="notebooks/thinking_in_jax.html">Quickstart: How to think in JAX</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="notebooks/Common_Gotchas_in_JAX.html">üî™ JAX - The Sharp Bits üî™</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="tutorials.html">Tutorials</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="jit-compilation.html">Just-in-time compilation</a></li>
<li class="toctree-l2"><a class="reference internal" href="automatic-vectorization.html">Automatic vectorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="automatic-differentiation.html">Automatic differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="random-numbers.html">Pseudorandom numbers</a></li>
<li class="toctree-l2"><a class="reference internal" href="stateful-computations.html">Stateful computations</a></li>
<li class="toctree-l2"><a class="reference internal" href="control-flow.html">Control flow and logical operators with JIT</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytrees.html">Pytrees</a></li>
<li class="toctree-l2"><a class="reference internal" href="working-with-pytrees.html">Working with pytrees</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Resources, guides, and references</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="key-concepts.html">Key concepts</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="advanced_guides.html">Resources and Advanced Guides</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="notebooks/autodiff_cookbook.html">The Autodiff Cookbook</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/Custom_derivative_rules_for_Python_code.html">Custom derivative rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/autodiff_remat.html">Control autodiff‚Äôs saved values with <code class="docutils literal notranslate"><span class="pre">jax.checkpoint</span></code> (aka <code class="docutils literal notranslate"><span class="pre">jax.remat</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="advanced-autodiff.html">Advanced automatic differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="errors.html">Errors</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="debugging.html">Introduction to debugging</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="debugging/print_breakpoint.html">Compiled prints and breakpoints</a></li>
<li class="toctree-l3"><a class="reference internal" href="debugging/checkify_guide.html">The <code class="docutils literal notranslate"><span class="pre">checkify</span></code> transformation</a></li>
<li class="toctree-l3"><a class="reference internal" href="debugging/flags.html">JAX debugging flags</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="debugging/flags.html">JAX debugging flags</a></li>
<li class="toctree-l2"><a class="reference internal" href="transfer_guard.html">Transfer guard</a></li>
<li class="toctree-l2"><a class="reference internal" href="persistent_compilation_cache.html">Persistent compilation cache</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">GPU performance tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="profiling.html">Profiling computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="device_memory_profiling.html">Profiling device memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/Distributed_arrays_and_automatic_parallelization.html">Distributed arrays and automatic parallelization</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/explicit-sharding.html">Explicit sharding (a.k.a. ‚Äúsharding in types‚Äù)</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/shard_map.html">Manual parallelism with <code class="docutils literal notranslate"><span class="pre">shard_map</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/layout.html">Device-local array layout control</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/host-offloading.html">JAX Memories and Host Offloading</a></li>

<li class="toctree-l2"><a class="reference internal" href="multi_process.html">Introduction to multi-controller JAX (aka multi-process/multi-host JAX)</a></li>
<li class="toctree-l2"><a class="reference internal" href="distributed_data_loading.html">Distributed data loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="external-callbacks.html">External callbacks</a></li>
<li class="toctree-l2"><a class="reference internal" href="ffi.html">Foreign function interface (FFI)</a></li>
<li class="toctree-l2"><a class="reference internal" href="gradient-checkpointing.html">Gradient checkpointing with <code class="docutils literal notranslate"><span class="pre">jax.checkpoint</span></code> (<code class="docutils literal notranslate"><span class="pre">jax.remat</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="aot.html">Ahead-of-time lowering and compilation</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="export/index.html">Exporting and serialization</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="export/export.html">Exporting and serializing staged-out computations</a></li>
<li class="toctree-l3"><a class="reference internal" href="export/shape_poly.html">Shape polymorphism</a></li>
<li class="toctree-l3"><a class="reference internal" href="export/jax2tf.html">Interoperation with TensorFlow</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="type_promotion.html">Type promotion semantics</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="pallas/index.html">Pallas: a JAX kernel language</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="pallas/quickstart.html">Pallas Quickstart</a></li>
<li class="toctree-l3"><a class="reference internal" href="pallas/pipelining.html">Software Pipelining</a></li>
<li class="toctree-l3"><a class="reference internal" href="pallas/grid_blockspec.html">Grids and BlockSpecs</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pallas/tpu/index.html">Pallas TPU</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="pallas/tpu/details.html">Writing TPU kernels with Pallas</a></li>
<li class="toctree-l4"><a class="reference internal" href="pallas/tpu/pipelining.html">TPU Pipelining</a></li>
<li class="toctree-l4"><a class="reference internal" href="pallas/tpu/matmul.html">Matrix Multiplication</a></li>
<li class="toctree-l4"><a class="reference internal" href="pallas/tpu/sparse.html">Scalar Prefetch and Block-Sparse Computation</a></li>
<li class="toctree-l4"><a class="reference internal" href="pallas/tpu/distributed.html">Distributed Computing in Pallas for TPUs</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pallas/gpu/index.html">Pallas:Mosaic GPU</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="pallas/gpu/reference.html">Writing Mosaic GPU kernels with Pallas</a></li>
<li class="toctree-l4"><a class="reference internal" href="pallas/gpu/pipelining.html">Mosaic GPU Pipelining</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="pallas/design/index.html">Pallas Design Notes</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="pallas/design/design.html">Pallas Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="pallas/design/async_note.html">Pallas Async Operations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="pallas/CHANGELOG.html">Pallas Changelog</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/neural_network_with_tfds_data.html">Training a simple neural network, with tensorflow/datasets data loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/Neural_Network_and_Data_Loading.html">Training a simple neural network, with PyTorch data loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/vmapped_log_probs.html">Autobatching for Bayesian inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="notebooks/convolutions.html">Generalized convolutions in JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="xla_flags.html">XLA compiler flags</a></li>
<li class="toctree-l2"><a class="reference internal" href="sharded-computation.html">Introduction to parallel programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax-primitives.html">JAX Internals: primitives</a></li>
<li class="toctree-l2"><a class="reference internal" href="jaxpr.html">JAX internals: The jaxpr language</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="contributor_guide.html">Developer notes</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="contributing.html">Contributing to JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="developer.html">Building from source</a></li>
<li class="toctree-l2"><a class="reference internal" href="investigating_a_regression.html">Investigating a regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="autodidax.html">Autodidax: JAX core from scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="autodidax2_part1.html">Autodidax2, part 1: JAX from scratch, again</a></li>

<li class="toctree-l2 has-children"><a class="reference internal" href="jep/index.html">JAX Enhancement Proposals (JEPs)</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="jep/263-prng.html">263: JAX PRNG Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/2026-custom-derivatives.html">2026: Custom JVP/VJP rules for JAX-transformable functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/4008-custom-vjp-update.html">4008: Custom VJP and `nondiff_argnums` update</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/4410-omnistaging.html">4410: Omnistaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/9263-typed-keys.html">9263: Typed keys &amp; pluggable RNGs</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/9407-type-promotion.html">9407: Design of Type Promotion Semantics for JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/9419-jax-versioning.html">9419: Jax and Jaxlib versioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/10657-sequencing-effects.html">10657: Sequencing side-effects in JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/11830-new-remat-checkpoint.html">11830: `jax.remat` / `jax.checkpoint` new implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/12049-type-annotations.html">12049: Type Annotation Roadmap for JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/14273-shard-map.html">14273: `shard_map` (`shmap`) for simple per-device code</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/15856-jex.html">15856: `jax.extend`, an extensions module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/17111-shmap-transpose.html">17111: Efficient transposition of `shard_map` (and other maps)</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/18137-numpy-scipy-scope.html">18137: Scope of JAX NumPy &amp; SciPy Wrappers</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/25516-effver.html">25516: Effort-based versioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="jep/28661-jax-array-protocol.html">28661: Supporting the `__jax_array__` protocol</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="extensions.html">Extension guides</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="notebooks/Writing_custom_interpreters_in_Jax.html">Writing custom Jaxpr interpreters in JAX</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="jax.extend.html"><code class="docutils literal notranslate"><span class="pre">jax.extend</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="jax.extend.core.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.core</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.extend.linear_util.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.linear_util</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.extend.mlir.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.mlir</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.extend.random.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.random</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="building_on_jax.html">Building on JAX</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="notes.html">Notes</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="api_compatibility.html">API compatibility</a></li>
<li class="toctree-l2"><a class="reference internal" href="deprecation.html">Python and NumPy version support policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="async_dispatch.html">Asynchronous dispatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="concurrency.html">Concurrency</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpu_memory_allocation.html">GPU memory allocation</a></li>
<li class="toctree-l2"><a class="reference internal" href="rank_promotion_warning.html">Rank promotion warning</a></li>
<li class="toctree-l2"><a class="reference internal" href="type_promotion.html">Type promotion semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="default_dtypes.html">Default dtypes and the X64 flag</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="jax.html">Public API: <code class="docutils literal notranslate"><span class="pre">jax</span></code> package</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="jax.numpy.html"><code class="docutils literal notranslate"><span class="pre">jax.numpy</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.fft.html">jax.numpy.fft.fft</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.fft2.html">jax.numpy.fft.fft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.fftfreq.html">jax.numpy.fft.fftfreq</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.fftn.html">jax.numpy.fft.fftn</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.fftshift.html">jax.numpy.fft.fftshift</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.hfft.html">jax.numpy.fft.hfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.ifft.html">jax.numpy.fft.ifft</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.ifft2.html">jax.numpy.fft.ifft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.ifftn.html">jax.numpy.fft.ifftn</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.ifftshift.html">jax.numpy.fft.ifftshift</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.ihfft.html">jax.numpy.fft.ihfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.irfft.html">jax.numpy.fft.irfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.irfft2.html">jax.numpy.fft.irfft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.irfftn.html">jax.numpy.fft.irfftn</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.rfft.html">jax.numpy.fft.rfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.rfft2.html">jax.numpy.fft.rfft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.rfftfreq.html">jax.numpy.fft.rfftfreq</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.numpy.fft.rfftn.html">jax.numpy.fft.rfftn</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="jax.scipy.html"><code class="docutils literal notranslate"><span class="pre">jax.scipy</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.scipy.stats.bernoulli.logpmf.html">jax.scipy.stats.bernoulli.logpmf</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.scipy.stats.bernoulli.pmf.html">jax.scipy.stats.bernoulli.pmf</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.scipy.stats.bernoulli.cdf.html">jax.scipy.stats.bernoulli.cdf</a></li>
<li class="toctree-l3"><a class="reference internal" href="_autosummary/jax.scipy.stats.bernoulli.ppf.html">jax.scipy.stats.bernoulli.ppf</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="jax.lax.html"><code class="docutils literal notranslate"><span class="pre">jax.lax</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.random.html"><code class="docutils literal notranslate"><span class="pre">jax.random</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.sharding.html"><code class="docutils literal notranslate"><span class="pre">jax.sharding</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.debug.html"><code class="docutils literal notranslate"><span class="pre">jax.debug</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.dlpack.html"><code class="docutils literal notranslate"><span class="pre">jax.dlpack</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.distributed.html"><code class="docutils literal notranslate"><span class="pre">jax.distributed</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.dtypes.html"><code class="docutils literal notranslate"><span class="pre">jax.dtypes</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.ffi.html"><code class="docutils literal notranslate"><span class="pre">jax.ffi</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.flatten_util.html"><code class="docutils literal notranslate"><span class="pre">jax.flatten_util</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.image.html"><code class="docutils literal notranslate"><span class="pre">jax.image</span></code> module</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="jax.nn.html"><code class="docutils literal notranslate"><span class="pre">jax.nn</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="jax.nn.initializers.html"><code class="docutils literal notranslate"><span class="pre">jax.nn.initializers</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="jax.ops.html"><code class="docutils literal notranslate"><span class="pre">jax.ops</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.profiler.html"><code class="docutils literal notranslate"><span class="pre">jax.profiler</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.stages.html"><code class="docutils literal notranslate"><span class="pre">jax.stages</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.test_util.html"><code class="docutils literal notranslate"><span class="pre">jax.test_util</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.tree.html"><code class="docutils literal notranslate"><span class="pre">jax.tree</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.tree_util.html"><code class="docutils literal notranslate"><span class="pre">jax.tree_util</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.typing.html"><code class="docutils literal notranslate"><span class="pre">jax.typing</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="jax.export.html"><code class="docutils literal notranslate"><span class="pre">jax.export</span></code> module</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="jax.extend.html"><code class="docutils literal notranslate"><span class="pre">jax.extend</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="jax.extend.core.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.core</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.extend.linear_util.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.linear_util</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.extend.mlir.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.mlir</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.extend.random.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.random</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="jax.example_libraries.html"><code class="docutils literal notranslate"><span class="pre">jax.example_libraries</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="jax.example_libraries.optimizers.html"><code class="docutils literal notranslate"><span class="pre">jax.example_libraries.optimizers</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.example_libraries.stax.html"><code class="docutils literal notranslate"><span class="pre">jax.example_libraries.stax</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="jax.experimental.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.checkify.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.checkify</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.compilation_cache.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.compilation_cache</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.custom_dce.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.custom_dce</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.custom_partitioning.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.custom_partitioning</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.jet.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.jet</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.key_reuse.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.key_reuse</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.mesh_utils.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.mesh_utils</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.multihost_utils.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.multihost_utils</span></code> module</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="jax.experimental.pallas.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pallas</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="jax.experimental.pallas.mosaic_gpu.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pallas.mosaic_gpu</span></code> module</a></li>
<li class="toctree-l4"><a class="reference internal" href="jax.experimental.pallas.triton.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pallas.triton</span></code> module</a></li>
<li class="toctree-l4"><a class="reference internal" href="jax.experimental.pallas.tpu.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pallas.tpu</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.pjit.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pjit</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.serialize_executable.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.serialize_executable</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="jax.experimental.shard_map.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.shard_map</span></code> module</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="jax.experimental.sparse.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.sparse</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-22"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.BCOO.html">jax.experimental.sparse.BCOO</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_broadcast_in_dim.html">jax.experimental.sparse.bcoo_broadcast_in_dim</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_concatenate.html">jax.experimental.sparse.bcoo_concatenate</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_dot_general.html">jax.experimental.sparse.bcoo_dot_general</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_dot_general_sampled.html">jax.experimental.sparse.bcoo_dot_general_sampled</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_dynamic_slice.html">jax.experimental.sparse.bcoo_dynamic_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_extract.html">jax.experimental.sparse.bcoo_extract</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_fromdense.html">jax.experimental.sparse.bcoo_fromdense</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_gather.html">jax.experimental.sparse.bcoo_gather</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_multiply_dense.html">jax.experimental.sparse.bcoo_multiply_dense</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_multiply_sparse.html">jax.experimental.sparse.bcoo_multiply_sparse</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_update_layout.html">jax.experimental.sparse.bcoo_update_layout</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_reduce_sum.html">jax.experimental.sparse.bcoo_reduce_sum</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_reshape.html">jax.experimental.sparse.bcoo_reshape</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_slice.html">jax.experimental.sparse.bcoo_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_sort_indices.html">jax.experimental.sparse.bcoo_sort_indices</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_squeeze.html">jax.experimental.sparse.bcoo_squeeze</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_sum_duplicates.html">jax.experimental.sparse.bcoo_sum_duplicates</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_todense.html">jax.experimental.sparse.bcoo_todense</a></li>
<li class="toctree-l4"><a class="reference internal" href="_autosummary/jax.experimental.sparse.bcoo_transpose.html">jax.experimental.sparse.bcoo_transpose</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="jax.lib.html"><code class="docutils literal notranslate"><span class="pre">jax.lib</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.addressable_shards.html">jax.Array.addressable_shards</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.all.html">jax.Array.all</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.any.html">jax.Array.any</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.argmax.html">jax.Array.argmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.argmin.html">jax.Array.argmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.argpartition.html">jax.Array.argpartition</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.argsort.html">jax.Array.argsort</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.astype.html">jax.Array.astype</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.at.html">jax.Array.at</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.choose.html">jax.Array.choose</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.clip.html">jax.Array.clip</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.compress.html">jax.Array.compress</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.committed.html">jax.Array.committed</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.conj.html">jax.Array.conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.conjugate.html">jax.Array.conjugate</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.copy.html">jax.Array.copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.copy_to_host_async.html">jax.Array.copy_to_host_async</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.cumprod.html">jax.Array.cumprod</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.cumsum.html">jax.Array.cumsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.device.html">jax.Array.device</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.diagonal.html">jax.Array.diagonal</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.dot.html">jax.Array.dot</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.dtype.html">jax.Array.dtype</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.flat.html">jax.Array.flat</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.flatten.html">jax.Array.flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.global_shards.html">jax.Array.global_shards</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.imag.html">jax.Array.imag</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.is_fully_addressable.html">jax.Array.is_fully_addressable</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.is_fully_replicated.html">jax.Array.is_fully_replicated</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.item.html">jax.Array.item</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.itemsize.html">jax.Array.itemsize</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.max.html">jax.Array.max</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.mean.html">jax.Array.mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.min.html">jax.Array.min</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.nbytes.html">jax.Array.nbytes</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.ndim.html">jax.Array.ndim</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.nonzero.html">jax.Array.nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.prod.html">jax.Array.prod</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.ptp.html">jax.Array.ptp</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.ravel.html">jax.Array.ravel</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.real.html">jax.Array.real</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.repeat.html">jax.Array.repeat</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.reshape.html">jax.Array.reshape</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.round.html">jax.Array.round</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.searchsorted.html">jax.Array.searchsorted</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.shape.html">jax.Array.shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.sharding.html">jax.Array.sharding</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.size.html">jax.Array.size</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.sort.html">jax.Array.sort</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.squeeze.html">jax.Array.squeeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.std.html">jax.Array.std</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.sum.html">jax.Array.sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.swapaxes.html">jax.Array.swapaxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.take.html">jax.Array.take</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.to_device.html">jax.Array.to_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.trace.html">jax.Array.trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.transpose.html">jax.Array.transpose</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.var.html">jax.Array.var</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.view.html">jax.Array.view</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.T.html">jax.Array.T</a></li>
<li class="toctree-l2"><a class="reference internal" href="_autosummary/jax.Array.mT.html">jax.Array.mT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="about.html">About the project</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently asked questions (FAQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="changelog.html">Change log</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary of terms</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="config_options.html">Configuration Options</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="advanced_guides.html" class="nav-link">Resources and Advanced Guides</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">GPU performance tips</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/jax-ml/jax" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/gpu_performance_tips.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>GPU performance tips</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matmul-precision">Matmul precision</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#xla-performance-flags">XLA performance flags</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-generation-flags">Code generation flags</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#communication-tips">Communication tips</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-and-manual-pgle">Auto and manual PGLE</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-pgle">Auto PGLE</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#collecting-nvidia-nsight-systems-profiles-when-using-autopgle">Collecting NVIDIA Nsight Systems profiles when using AutoPGLE</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#manual-pgle">Manual PGLE</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#flags">Flags</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pipeline-parallelism-on-gpu">Pipeline Parallelism on GPU</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-xla-flags">Using XLA Flags</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-psend-and-precv">Using <code class="docutils literal notranslate"><span class="pre">psend</span></code> and <code class="docutils literal notranslate"><span class="pre">precv</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nccl-flags">NCCL flags</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-process">Multi-Process</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="gpu-performance-tips">
<h1>GPU performance tips<a class="headerlink" href="#gpu-performance-tips" title="Link to this heading">#</a></h1>
<!--* freshness: { reviewed: '2025-03-20' } *-->
<p>This document focuses on performance tips for neural network workloads</p>
<section id="matmul-precision">
<h2>Matmul precision<a class="headerlink" href="#matmul-precision" title="Link to this heading">#</a></h2>
<p>On recent GPU generations, such as the Nvidia A100 generation or later, it can
be a good idea to perform most computations in <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> precision. For
example, if using <a class="reference external" href="https://github.com/google/flax">Flax</a>, instantiate <code class="docutils literal notranslate"><span class="pre">Dense</span></code>
layers using <code class="docutils literal notranslate"><span class="pre">flax.linen.Dense(...,</span> <span class="pre">dtype=jax.numpy.bfloat16)</span></code>. Here are some
code examples:</p>
<ul class="simple">
<li><p>In the <a class="reference external" href="https://github.com/google/flax/tree/main/examples/lm1b">Flax LM1B
example</a>, <code class="docutils literal notranslate"><span class="pre">Dense</span></code>
modules are <a class="reference external" href="https://github.com/google/flax/blob/fd8fd76a4af5307a61f85bac98feab9b26d60db8/examples/lm1b/models.py#L188">instantiated with a configurable
dtype</a>
which <a class="reference external" href="https://github.com/google/flax/blob/fd8fd76a4af5307a61f85bac98feab9b26d60db8/examples/lm1b/configs/default.py#L112">defaults</a> to
<a class="reference external" href="https://github.com/google/flax/blob/c0087535d7f2e5bfcbf2a7be6825b9f5055a54c6/examples/lm1b/train.py#L431">bfloat16</a>.</p></li>
<li><p>In <a class="reference external" href="https://github.com/google/maxtext">MaxText</a>, <code class="docutils literal notranslate"><span class="pre">DenseGeneral</span></code> modules are
also <a class="reference external" href="https://github.com/google/maxtext/blob/07dc6ce27ced1246407d0de311d4a0d6a9fd46d8/MaxText/layers.py#L592">instantiated with a configurable
dtype</a>
that <a class="reference external" href="https://github.com/google/maxtext/blob/07dc6ce27ced1246407d0de311d4a0d6a9fd46d8/MaxText/configs/base.yml#L41">defaults to
bfloat16</a>.</p></li>
</ul>
</section>
<section id="xla-performance-flags">
<h2>XLA performance flags<a class="headerlink" href="#xla-performance-flags" title="Link to this heading">#</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>JAX-Toolbox also has a page on <a class="reference external" href="https://github.com/NVIDIA/JAX-Toolbox/blob/main/rosetta/docs/GPU_performance.md">NVIDIA XLA performance FLAGS</a>.</p>
</div>
<p>The existence and exact behavior of XLA flags may be <code class="docutils literal notranslate"><span class="pre">jaxlib</span></code>-version dependent.</p>
<p>As of <code class="docutils literal notranslate"><span class="pre">jaxlib==0.4.18</span></code> (released <a class="reference external" href="https://pypi.org/project/jaxlib/#history">Oct 6
2023</a>), setting these XLA flags can
improve performance. Some are related to communication between GPUs, and so are
only relevant when running computations on multiple devices, while others are
related to code generation on each device.</p>
<p>Some of these may be set by default in future releases.</p>
<p>These flags can be set via the <code class="docutils literal notranslate"><span class="pre">XLA_FLAGS</span></code> shell environment variable. For
example, we can add this to the top of a Python file:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;XLA_FLAGS&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s1">&#39;--xla_gpu_triton_gemm_any=True &#39;</span>
    <span class="s1">&#39;--xla_gpu_enable_latency_hiding_scheduler=true &#39;</span>
<span class="p">)</span>
</pre></div>
</div>
<p>For more examples, see also <a class="reference external" href="https://github.com/NVIDIA/JAX-Toolbox/blob/main/rosetta/rosetta/projects/pax/README.md#xla-flags">XLA Flags recommended for Pax
training on Nvidia GPUs</a>.</p>
<section id="code-generation-flags">
<h3>Code generation flags<a class="headerlink" href="#code-generation-flags" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>‚Äìxla_gpu_triton_gemm_any</strong> Use the Triton-based GEMM (matmul) emitter for
any GEMM that it supports. The default value is False.</p></li>
</ul>
</section>
</section>
<section id="communication-tips">
<h2>Communication tips<a class="headerlink" href="#communication-tips" title="Link to this heading">#</a></h2>
<section id="auto-and-manual-pgle">
<h3>Auto and manual PGLE<a class="headerlink" href="#auto-and-manual-pgle" title="Link to this heading">#</a></h3>
<p>The Profile Guided Latency Estimator (PGLE) workflow measures the actual running time
of compute and collectives, the the profile information is fed back into XLA compiler
for a better scheduling decision.</p>
<p>The Profile Guided Latency Estimator can be used manually or automatically. In the auto mode
JAX will collect profile information and recompile a module in a single run. While
in manual mode you need to run a task twice, the first time to collect and save profiles
and the second to compile and run with provided data.</p>
<p><strong>Important</strong>: the JAX profiler, which is used by both of the PGLE workflows documented
below, cannot co-exist with the NVIDIA Nsight Systems profiler. This limitation can be
avoided by using the JAX compilation cache, as described below.</p>
</section>
<section id="auto-pgle">
<h3>Auto PGLE<a class="headerlink" href="#auto-pgle" title="Link to this heading">#</a></h3>
<p>The auto PGLE can be turned on by setting the following environment variables:</p>
<p>Mandatory:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">JAX_ENABLE_PGLE</span><span class="o">=</span><span class="nb">true</span>

<span class="c1"># For JAX version &lt;= 0.5.0 make sure to include:</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">XLA_FLAGS</span><span class="o">=</span><span class="s2">&quot;--xla_gpu_enable_latency_hiding_scheduler=true&quot;</span>
</pre></div>
</div>
<p>Optional:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">JAX_PGLE_PROFILING_RUNS</span><span class="o">=</span><span class="m">3</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">JAX_PGLE_AGGREGATION_PERCENTILE</span><span class="o">=</span><span class="m">85</span>

<span class="c1"># Right now the auto PGLE profile collection doesn&#39;t work with command buffer.</span>
<span class="c1"># If the command buffer is enabled, Auto PGLE will disable it during profile</span>
<span class="c1"># collection and enable it back after the recompilation. If you need to have a</span>
<span class="c1"># consistent command buffer logic with and with PGLE profile you can disable it</span>
<span class="c1"># manually:</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">XLA_FLAGS</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">${</span><span class="nv">XLA_FLAGS</span><span class="si">}</span><span class="s2"> --xla_gpu_enable_command_buffer=&#39;&#39;&quot;</span>
</pre></div>
</div>
<p>Or in the JAX this can be set as the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">config</span>

<span class="k">with</span> <span class="n">config</span><span class="o">.</span><span class="n">enable_pgle</span><span class="p">(</span><span class="kc">True</span><span class="p">),</span> <span class="n">config</span><span class="o">.</span><span class="n">pgle_profiling_runs</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
  <span class="c1"># Run with the profiler collecting performance information.</span>
  <span class="n">train_step</span><span class="p">()</span>
  <span class="c1"># Automatically re-compile with PGLE profile results</span>
  <span class="n">train_step</span><span class="p">()</span>
  <span class="o">...</span>
</pre></div>
</div>
<p>You can control amount of reruns used to collect profile data by changing <code class="docutils literal notranslate"><span class="pre">JAX_PGLE_PROFILING_RUNS</span></code>.
Increasing this parameter would lead to better profile information, but it will also increase the
amount of non-optimized training steps.</p>
<p>Decreasing the <code class="docutils literal notranslate"><span class="pre">JAX_PGLE_AGGREGATION_PERCENTILE</span></code> parameter might help in case when performance between steps is too noisy to filter out a non-relevant measures.</p>
<p><strong>Attention:</strong> Auto PGLE doesn‚Äôt work for pre-compiled modules. Since JAX need to recompile the module during execution the auto PGLE will not work neither for AoT nor for the following case:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">from</span> <span class="nn">jax._src</span> <span class="kn">import</span> <span class="n">config</span>

<span class="n">train_step_compiled</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">()</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span><span class="o">.</span><span class="n">compile</span><span class="p">()</span>

<span class="k">with</span> <span class="n">config</span><span class="o">.</span><span class="n">enable_pgle</span><span class="p">(</span><span class="kc">True</span><span class="p">),</span> <span class="n">config</span><span class="o">.</span><span class="n">pgle_profiling_runs</span><span class="p">(</span><span class="mi">1</span><span class="p">):</span>
  <span class="n">train_step_compiled</span><span class="p">()</span>
  <span class="c1"># No effect since module was pre-compiled.</span>
  <span class="n">train_step_compiled</span><span class="p">()</span>
</pre></div>
</div>
<section id="collecting-nvidia-nsight-systems-profiles-when-using-autopgle">
<h4>Collecting NVIDIA Nsight Systems profiles when using AutoPGLE<a class="headerlink" href="#collecting-nvidia-nsight-systems-profiles-when-using-autopgle" title="Link to this heading">#</a></h4>
<p><a class="reference external" href="https://github.com/jax-ml/jax/pull/24910">jax#24910</a> (JAX v0.5.1 and newer) added a
new JAX configuration option, <code class="docutils literal notranslate"><span class="pre">JAX_COMPILATION_CACHE_EXPECT_PGLE</span></code>, which tells JAX to
attempt to load PGLE-optimized compiled functions from the persistent compilation
cache.</p>
<p>This allows a two-step process, where the first step writes a PGLE-optimized function
to the cache:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">JAX_ENABLE_COMPILATION_CACHE</span><span class="o">=</span>yes<span class="w">          </span><span class="c1"># not strictly needed, on by default</span>
<span class="nb">export</span><span class="w"> </span><span class="nv">JAX_COMPILATION_CACHE_DIR</span><span class="o">=</span>/root/jax_cache
<span class="nv">JAX_ENABLE_PGLE</span><span class="o">=</span>yes<span class="w"> </span>python<span class="w"> </span>my-model.py
</pre></div>
</div>
<p>And the second step uses Nsight Systems and loads the PGLE-optimized function from the
cache:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nv">JAX_COMPILATION_CACHE_EXPECT_PGLE</span><span class="o">=</span>yes<span class="w"> </span>nsys<span class="w"> </span>profile<span class="w"> </span>python<span class="w"> </span>my-model.py
</pre></div>
</div>
<p>See also <a class="reference external" href="https://docs.jax.dev/en/latest/persistent_compilation_cache.html#pitfalls">this page</a> for more
information about the persistent compilation cache and possible pitfalls.</p>
</section>
</section>
<section id="manual-pgle">
<h3>Manual PGLE<a class="headerlink" href="#manual-pgle" title="Link to this heading">#</a></h3>
<p>If you still want to use a manual Profile Guided Latency Estimator the workflow in XLA/GPU is:</p>
<ul class="simple">
<li><ol class="arabic simple">
<li><p>Run your workload once, with async collectives and latency hiding scheduler enabled.</p></li>
</ol>
</li>
</ul>
<p>You could do so by setting:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">XLA_FLAGS</span><span class="o">=</span><span class="s2">&quot;--xla_gpu_enable_latency_hiding_scheduler=true&quot;</span>
</pre></div>
</div>
<ul class="simple">
<li><ol class="arabic simple" start="2">
<li><p>Collect and post process a profile by using JAX profiler, saving the extracted instruction latencies into a binary protobuf file.</p></li>
</ol>
</li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">etils</span> <span class="kn">import</span> <span class="n">epath</span>
<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">from</span> <span class="nn">jax.experimental</span> <span class="kn">import</span> <span class="n">profiler</span> <span class="k">as</span> <span class="n">exp_profiler</span>

<span class="c1"># Define your profile directory</span>
<span class="n">profile_dir</span> <span class="o">=</span> <span class="s1">&#39;gs://my_bucket/profile&#39;</span>
<span class="n">jax</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">start_trace</span><span class="p">(</span><span class="n">profile_dir</span><span class="p">)</span>

<span class="c1"># run your workflow</span>
<span class="c1"># for i in range(10):</span>
<span class="c1">#   train_step()</span>

<span class="c1"># Stop trace</span>
<span class="n">jax</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">stop_trace</span><span class="p">()</span>
<span class="n">profile_dir</span> <span class="o">=</span> <span class="n">epath</span><span class="o">.</span><span class="n">Path</span><span class="p">(</span><span class="n">profile_dir</span><span class="p">)</span>
<span class="n">directories</span> <span class="o">=</span> <span class="n">profile_dir</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="s1">&#39;plugins/profile/*/&#39;</span><span class="p">)</span>
<span class="n">directories</span> <span class="o">=</span> <span class="p">[</span><span class="n">d</span> <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">directories</span> <span class="k">if</span> <span class="n">d</span><span class="o">.</span><span class="n">is_dir</span><span class="p">()]</span>
<span class="n">rundir</span> <span class="o">=</span> <span class="n">directories</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="n">logging</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;rundir: </span><span class="si">%s</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">rundir</span><span class="p">)</span>

<span class="c1"># Post process the profile</span>
<span class="n">fdo_profile</span> <span class="o">=</span> <span class="n">exp_profiler</span><span class="o">.</span><span class="n">get_profiled_instructions_proto</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">fspath</span><span class="p">(</span><span class="n">rundir</span><span class="p">))</span>

<span class="c1"># Save the profile proto to a file.</span>
<span class="n">dump_dir</span> <span class="o">=</span> <span class="n">rundir</span> <span class="o">/</span> <span class="s1">&#39;profile.pb&#39;</span>
<span class="n">dump_dir</span><span class="o">.</span><span class="n">parent</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dump_dir</span><span class="o">.</span><span class="n">write_bytes</span><span class="p">(</span><span class="n">fdo_profile</span><span class="p">)</span>

</pre></div>
</div>
<p>After this step, you will get a <code class="docutils literal notranslate"><span class="pre">profile.pb</span></code> file under the <code class="docutils literal notranslate"><span class="pre">rundir</span></code> printed in the code.</p>
<ul class="simple">
<li><ol class="arabic simple" start="3">
<li><p>Run the workload again feeding that file into the compilation.</p></li>
</ol>
</li>
</ul>
<p>You need to pass the <code class="docutils literal notranslate"><span class="pre">profile.pb</span></code> file to the <code class="docutils literal notranslate"><span class="pre">--xla_gpu_pgle_profile_file_or_directory_path</span></code> flag.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="w"> </span><span class="nb">export</span><span class="w"> </span><span class="nv">XLA_FLAGS</span><span class="o">=</span><span class="s2">&quot;--xla_gpu_enable_latency_hiding_scheduler=true --xla_gpu_pgle_profile_file_or_directory_path=/path/to/profile/profile.pb&quot;</span>
</pre></div>
</div>
<p>To enable logging in the XLA and check if the profile is good, set the logging level to include <code class="docutils literal notranslate"><span class="pre">INFO</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">TF_CPP_MIN_LOG_LEVEL</span><span class="o">=</span><span class="m">0</span>
</pre></div>
</div>
<p>Run the real workflow, if you found these loggings in the running log, it means the profiler is used in the latency hiding scheduler:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mi">2023</span><span class="o">-</span><span class="mi">07</span><span class="o">-</span><span class="mi">21</span> <span class="mi">16</span><span class="p">:</span><span class="mi">09</span><span class="p">:</span><span class="mf">43.551600</span><span class="p">:</span> <span class="n">I</span> <span class="n">external</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">service</span><span class="o">/</span><span class="n">gpu</span><span class="o">/</span><span class="n">gpu_hlo_schedule</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">478</span><span class="p">]</span> <span class="n">Using</span> <span class="n">PGLE</span> <span class="n">profile</span> <span class="kn">from</span> <span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">profile</span><span class="o">/</span><span class="n">plugins</span><span class="o">/</span><span class="n">profile</span><span class="o">/</span><span class="mi">2023_07_20_18_29_30</span><span class="o">/</span><span class="n">profile</span><span class="o">.</span><span class="n">pb</span>
<span class="mi">2023</span><span class="o">-</span><span class="mi">07</span><span class="o">-</span><span class="mi">21</span> <span class="mi">16</span><span class="p">:</span><span class="mi">09</span><span class="p">:</span><span class="mf">43.551741</span><span class="p">:</span> <span class="n">I</span> <span class="n">external</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">service</span><span class="o">/</span><span class="n">gpu</span><span class="o">/</span><span class="n">gpu_hlo_schedule</span><span class="o">.</span><span class="n">cc</span><span class="p">:</span><span class="mi">573</span><span class="p">]</span> <span class="n">Found</span> <span class="n">profile</span><span class="p">,</span> <span class="n">using</span> <span class="n">profile</span> <span class="n">guided</span> <span class="n">latency</span> <span class="n">estimator</span>
</pre></div>
</div>
<section id="flags">
<h4>Flags<a class="headerlink" href="#flags" title="Link to this heading">#</a></h4>
<ul>
<li><p><strong>‚Äìxla_gpu_enable_latency_hiding_scheduler</strong> This flag enables latency hiding
schedulers to overlap asynchronous communication with computation efficiently.
The default value is False.</p></li>
<li><p><strong>‚Äìxla_gpu_memory_limit_slop_factor</strong>¬†This flag serves as a multiplier applied
to the total available memory, creating a threshold that guides the Latency Hiding
Scheduler (LHS) in balancing memory reduction and latency hiding optimizations.
The default value is 95.</p>
<p>This factor effectively establishes a memory limit for compiler passes, determining
when the scheduler should prioritize:</p>
<ol class="arabic simple">
<li><p>Memory reduction: When memory usage approaches or exceeds the calculated threshold.</p></li>
<li><p>Latency hiding: When memory usage is below the threshold, allowing for more
aggressive optimizations that may temporarily increase memory usage but improve
overall performance.</p></li>
</ol>
<p>By adjusting this factor, users can fine-tune the trade-off between memory efficiency
and performance optimizations.</p>
</li>
<li><p><strong>‚Äìxla_gpu_all_gather_combine_threshold_bytes</strong>
<strong>‚Äìxla_gpu_reduce_scatter_combine_threshold_bytes</strong>
<strong>‚Äìxla_gpu_all_reduce_combine_threshold_bytes</strong>
These flags tune when to combine multiple small
<code class="docutils literal notranslate"><span class="pre">AllGather</span></code>/<code class="docutils literal notranslate"><span class="pre">ReduceScatter</span></code>/<code class="docutils literal notranslate"><span class="pre">AllReduce</span></code> into one big
<code class="docutils literal notranslate"><span class="pre">AllGather</span></code>/<code class="docutils literal notranslate"><span class="pre">ReduceScatter</span></code>/<code class="docutils literal notranslate"><span class="pre">AllReduce</span></code> to reduce time spent on cross-device
communication. For example, for the <code class="docutils literal notranslate"><span class="pre">AllGather</span></code>/<code class="docutils literal notranslate"><span class="pre">ReduceScatter</span></code> thresholds
on a Transformer-based workload, consider tuning them high enough so as to
combine at least a Transformer Layer‚Äôs weight <code class="docutils literal notranslate"><span class="pre">AllGather</span></code>/<code class="docutils literal notranslate"><span class="pre">ReduceScatter</span></code>. By
default, the <code class="docutils literal notranslate"><span class="pre">combine_threshold_bytes</span></code> is set to 256.</p></li>
</ul>
</section>
</section>
<section id="pipeline-parallelism-on-gpu">
<h3>Pipeline Parallelism on GPU<a class="headerlink" href="#pipeline-parallelism-on-gpu" title="Link to this heading">#</a></h3>
<section id="using-xla-flags">
<h4>Using XLA Flags<a class="headerlink" href="#using-xla-flags" title="Link to this heading">#</a></h4>
<p>XLA implements SPMD-based pipeline parallelism optimizations. This is a scaling
technique where the forward and backward pass are split into multiple pipeline
stages. Each device (or device group) processes the result of the previous
pipeline stage (or the pipeline input) and sends its partial result to the next
stage until the end of the pipeline is reached. This optimization works best
when the latency of the computation is larger than communication. At compile
time, the operations will be rearranged to overlap communication with
computation.</p>
<p>For an optimized schedule, we recommend these XLA flags:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">--</span><span class="n">xla_gpu_enable_latency_hiding_scheduler</span><span class="o">=</span><span class="n">true</span>
<span class="o">--</span><span class="n">xla_gpu_enable_command_buffer</span><span class="o">=</span><span class="s1">&#39;&#39;</span>
<span class="o">--</span><span class="n">xla_disable_hlo_passes</span><span class="o">=</span><span class="n">collective</span><span class="o">-</span><span class="n">permute</span><span class="o">-</span><span class="n">motion</span>
<span class="o">--</span><span class="n">xla_gpu_experimental_pipeline_parallelism_opt_level</span><span class="o">=</span><span class="n">PIPELINE_PARALLELISM_OPT_LEVEL_ENABLE</span>
</pre></div>
</div>
<p>The following JAX example demonstrates a pattern where communication operations
are scheduled to overlap with computations. In this example we will illustrate
how to set up an optimized pipeline parallelism scheduling using 4 GPUs that
form a communication ring (device 0 -&gt; device 1 -&gt; device 2 -&gt; device 3 -&gt;
device 0). We refer to the pattern <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">-&gt;</span> <span class="pre">1</span> <span class="pre">-&gt;</span> <span class="pre">2</span> <span class="pre">-&gt;</span> <span class="pre">3</span></code> as the forward edge, and
<code class="docutils literal notranslate"><span class="pre">3</span> <span class="pre">-&gt;</span> <span class="pre">0</span></code> as the back edge.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Imports and setup</span>
<span class="kn">import</span> <span class="nn">functools</span>
<span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">sharding</span>
<span class="kn">from</span> <span class="nn">jax.experimental</span> <span class="kn">import</span> <span class="n">mesh_utils</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">import</span> <span class="nn">jax.random</span>

<span class="n">NUM_DEVICES</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">NUM_MICROBATCHES</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">NUM_CIRC_REPEATS</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">CONTRACTING_DIM_SIZE</span> <span class="o">=</span> <span class="mi">4096</span>
<span class="n">NON_CONTRACTING_DIM_SIZE</span> <span class="o">=</span> <span class="mi">8192</span>
<span class="n">COMPUTE_INTENSITY</span> <span class="o">=</span> <span class="mi">32</span>

<span class="c1"># Creates a collective permute for the &quot;forward edge&quot;.</span>
<span class="c1"># 0-&gt;1, 1-&gt;2, ... (N-2)-&gt;(N-1)</span>
<span class="k">def</span> <span class="nf">shift_right</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
  <span class="n">padding</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span> <span class="o">+</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span> <span class="o">*</span> <span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
  <span class="c1"># Use lax.slice to guarantee the gradient is a pad.</span>
  <span class="k">return</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">padding</span><span class="p">),</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">arr</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>


<span class="c1"># Creates a collective permute for the &quot;back edge&quot;.</span>
<span class="c1"># (N-1)-&gt;0</span>
<span class="k">def</span> <span class="nf">cycle_back</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
  <span class="n">padding</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="n">NUM_DEVICES</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]]</span> <span class="o">+</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span> <span class="o">*</span> <span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">slice</span><span class="p">(</span>
      <span class="n">jnp</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="n">padding</span><span class="p">),</span>
      <span class="p">[</span><span class="n">NUM_DEVICES</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">arr</span><span class="o">.</span><span class="n">ndim</span> <span class="o">-</span> <span class="mi">1</span><span class="p">),</span>
      <span class="p">(</span><span class="n">NUM_DEVICES</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],)</span> <span class="o">+</span> <span class="n">arr</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span>
  <span class="p">)</span>


<span class="k">def</span> <span class="nf">select_on_first_device</span><span class="p">(</span><span class="n">then_value</span><span class="p">,</span> <span class="n">else_value</span><span class="p">):</span>
  <span class="k">assert</span> <span class="n">then_value</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">else_value</span><span class="o">.</span><span class="n">shape</span>
  <span class="n">is_first_device</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">broadcasted_iota</span><span class="p">(</span><span class="s2">&quot;int32&quot;</span><span class="p">,</span> <span class="n">then_value</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">is_first_device</span><span class="p">,</span> <span class="n">then_value</span><span class="p">,</span> <span class="n">else_value</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">select_on_last_device</span><span class="p">(</span><span class="n">then_value</span><span class="p">,</span> <span class="n">else_value</span><span class="p">):</span>
  <span class="k">assert</span> <span class="n">then_value</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">else_value</span><span class="o">.</span><span class="n">shape</span>
  <span class="n">is_last_device</span> <span class="o">=</span> <span class="p">(</span>
      <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">broadcasted_iota</span><span class="p">(</span><span class="s2">&quot;int32&quot;</span><span class="p">,</span> <span class="n">then_value</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="n">NUM_DEVICES</span> <span class="o">-</span> <span class="mi">1</span>
  <span class="p">)</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">is_last_device</span><span class="p">,</span> <span class="n">then_value</span><span class="p">,</span> <span class="n">else_value</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">select_on_first_cycle</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">then_value</span><span class="p">,</span> <span class="n">else_value</span><span class="p">):</span>
  <span class="k">assert</span> <span class="n">then_value</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">else_value</span><span class="o">.</span><span class="n">shape</span>
  <span class="n">is_first_cycle</span> <span class="o">=</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">NUM_MICROBATCHES</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">is_first_cycle</span><span class="p">,</span> <span class="n">then_value</span><span class="p">,</span> <span class="n">else_value</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">while_body</span><span class="p">(</span><span class="n">carry</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
<span class="w">  </span><span class="sd">&quot;&quot;&quot;Body of the pipeline while loop.&quot;&quot;&quot;</span>
  <span class="n">weights</span><span class="p">,</span> <span class="n">input_buffer</span><span class="p">,</span> <span class="n">output_buffer</span><span class="p">,</span> <span class="n">fwd_edge_data</span><span class="p">,</span> <span class="n">bwd_edge_data</span> <span class="o">=</span> <span class="n">carry</span>

  <span class="c1"># Read input data from input buffer.</span>
  <span class="n">input_data</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">dynamic_slice</span><span class="p">(</span>
      <span class="n">input_buffer</span><span class="p">,</span>
      <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">0</span><span class="p">)</span> <span class="o">%</span> <span class="n">NUM_MICROBATCHES</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
      <span class="p">(</span><span class="n">NUM_DEVICES</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">CONTRACTING_DIM_SIZE</span><span class="p">,</span> <span class="n">NON_CONTRACTING_DIM_SIZE</span><span class="p">),</span>
  <span class="p">)</span>

  <span class="c1"># Collective permute on the &quot;forward edge&quot; shifts data to the next stage.</span>
  <span class="n">fwd_edge_data</span> <span class="o">=</span> <span class="n">shift_right</span><span class="p">(</span><span class="n">fwd_edge_data</span><span class="p">)</span>

  <span class="c1"># Select compute argument based on device and pipeline cycle.</span>
  <span class="n">compute_argument</span> <span class="o">=</span> <span class="n">select_on_first_device</span><span class="p">(</span>
      <span class="n">select_on_first_cycle</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">input_data</span><span class="p">,</span> <span class="n">bwd_edge_data</span><span class="p">),</span>
      <span class="n">fwd_edge_data</span><span class="p">,</span>
  <span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">NUM_DEVICES</span><span class="p">,</span> <span class="n">CONTRACTING_DIM_SIZE</span><span class="p">,</span> <span class="n">NON_CONTRACTING_DIM_SIZE</span><span class="p">))</span>

  <span class="c1"># A few matmuls to simulate compute.</span>
  <span class="n">tmp</span> <span class="o">=</span> <span class="n">compute_argument</span>
  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">COMPUTE_INTENSITY</span><span class="p">):</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">dot_general</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">tmp</span><span class="p">,</span> <span class="p">(((</span><span class="mi">2</span><span class="p">,),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)),</span> <span class="p">((</span><span class="mi">0</span><span class="p">,),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,))))</span>
  <span class="n">compute_result</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
      <span class="p">(</span><span class="n">NUM_DEVICES</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">CONTRACTING_DIM_SIZE</span><span class="p">,</span> <span class="n">NON_CONTRACTING_DIM_SIZE</span><span class="p">)</span>
  <span class="p">)</span>

  <span class="c1"># Read data from buffer to pass it to the first device of the pipeline on the</span>
  <span class="c1"># &quot;back edge&quot;.</span>
  <span class="n">bwd_edge_data</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">dynamic_slice</span><span class="p">(</span>
      <span class="n">output_buffer</span><span class="p">,</span>
      <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span> <span class="o">%</span> <span class="n">NUM_MICROBATCHES</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
      <span class="p">(</span><span class="n">NUM_DEVICES</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">CONTRACTING_DIM_SIZE</span><span class="p">,</span> <span class="n">NON_CONTRACTING_DIM_SIZE</span><span class="p">),</span>
  <span class="p">)</span>

  <span class="c1"># Collective permute on the &quot;back edge&quot; passes data to the first device.</span>
  <span class="n">bwd_edge_data</span> <span class="o">=</span> <span class="n">cycle_back</span><span class="p">(</span><span class="n">bwd_edge_data</span><span class="p">)</span>

  <span class="c1"># Update output buffer. We do this after reading from it to avoid the data</span>
  <span class="c1"># dependency.</span>
  <span class="n">output_buffer</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">dynamic_update_slice</span><span class="p">(</span>
      <span class="n">output_buffer</span><span class="p">,</span>
      <span class="n">compute_result</span><span class="p">,</span>
      <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span> <span class="o">+</span> <span class="n">i</span><span class="p">)</span> <span class="o">%</span> <span class="n">NUM_MICROBATCHES</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
  <span class="p">)</span>

  <span class="n">fwd_edge_data</span> <span class="o">=</span> <span class="n">compute_result</span>
  <span class="n">carry</span> <span class="o">=</span> <span class="p">(</span>
      <span class="n">weights</span><span class="p">,</span>
      <span class="n">input_buffer</span><span class="p">,</span>
      <span class="n">output_buffer</span><span class="p">,</span>
      <span class="n">fwd_edge_data</span><span class="p">,</span>
      <span class="n">bwd_edge_data</span><span class="p">,</span>
  <span class="p">)</span>
  <span class="k">return</span> <span class="n">carry</span><span class="p">,</span> <span class="n">i</span>


<span class="nd">@functools</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">,</span> <span class="n">static_argnames</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;mesh&quot;</span><span class="p">])</span>
<span class="k">def</span> <span class="nf">entry_computation</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">input_buffer</span><span class="p">,</span> <span class="n">mesh</span><span class="p">):</span>

  <span class="c1"># Init output buffer.</span>
  <span class="n">output_buffer</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">input_buffer</span><span class="p">)</span>

  <span class="c1"># Init dummy data for forward and backward edge passed through the while loop.</span>
  <span class="n">dummy_data</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
      <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">NUM_DEVICES</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">CONTRACTING_DIM_SIZE</span><span class="p">,</span> <span class="n">NON_CONTRACTING_DIM_SIZE</span><span class="p">)</span>
  <span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">dummy_data</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span>
      <span class="n">dummy_data</span><span class="p">,</span>
      <span class="n">sharding</span><span class="o">.</span><span class="n">NamedSharding</span><span class="p">(</span>
          <span class="n">mesh</span><span class="p">,</span> <span class="n">sharding</span><span class="o">.</span><span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
      <span class="p">),</span>
  <span class="p">)</span>

  <span class="c1"># Start pipeline.</span>
  <span class="n">carry</span> <span class="o">=</span> <span class="n">weights</span><span class="p">,</span> <span class="n">input_buffer</span><span class="p">,</span> <span class="n">output_buffer</span><span class="p">,</span> <span class="n">dummy_data</span><span class="p">,</span> <span class="n">dummy_data</span>
  <span class="n">num_iterations</span> <span class="o">=</span> <span class="n">NUM_CIRC_REPEATS</span> <span class="o">*</span> <span class="n">NUM_MICROBATCHES</span> <span class="o">+</span> <span class="n">NUM_DEVICES</span> <span class="o">-</span> <span class="mi">1</span>
  <span class="n">carry</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span><span class="n">while_body</span><span class="p">,</span> <span class="n">carry</span><span class="p">,</span> <span class="n">xs</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">))</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">output_buffer</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">carry</span>

  <span class="k">return</span> <span class="n">output_buffer</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">_</span><span class="p">):</span>

  <span class="c1"># Expect constant number of devices.</span>
  <span class="k">assert</span> <span class="n">NUM_DEVICES</span> <span class="o">==</span> <span class="n">jax</span><span class="o">.</span><span class="n">local_device_count</span><span class="p">()</span>

  <span class="c1"># Create mesh.</span>
  <span class="n">mesh</span> <span class="o">=</span> <span class="n">sharding</span><span class="o">.</span><span class="n">Mesh</span><span class="p">(</span>
      <span class="n">mesh_utils</span><span class="o">.</span><span class="n">create_device_mesh</span><span class="p">([</span><span class="n">NUM_DEVICES</span><span class="p">]),</span>
      <span class="n">axis_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span>
  <span class="p">)</span>

  <span class="c1"># Init weights.</span>
  <span class="n">weights</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">CONTRACTING_DIM_SIZE</span>
  <span class="n">weights</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">broadcast_in_dim</span><span class="p">(</span>
      <span class="n">weights</span><span class="p">,</span>
      <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">NUM_DEVICES</span><span class="p">,</span> <span class="n">CONTRACTING_DIM_SIZE</span><span class="p">,</span> <span class="n">CONTRACTING_DIM_SIZE</span><span class="p">),</span>
      <span class="n">broadcast_dimensions</span><span class="o">=</span><span class="p">(),</span>
  <span class="p">)</span>
  <span class="n">weights</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span>
      <span class="n">weights</span><span class="p">,</span>
      <span class="n">sharding</span><span class="o">.</span><span class="n">NamedSharding</span><span class="p">(</span>
          <span class="n">mesh</span><span class="p">,</span> <span class="n">sharding</span><span class="o">.</span><span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
      <span class="p">),</span>
  <span class="p">)</span>

  <span class="c1"># Init random input and replicate it across all devices.</span>
  <span class="n">random_key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">input_buffer</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span>
      <span class="n">random_key</span><span class="p">,</span>
      <span class="n">shape</span><span class="o">=</span><span class="p">(</span>
          <span class="n">NUM_MICROBATCHES</span><span class="p">,</span>
          <span class="n">CONTRACTING_DIM_SIZE</span><span class="p">,</span>
          <span class="n">NON_CONTRACTING_DIM_SIZE</span><span class="p">,</span>
      <span class="p">),</span>
  <span class="p">)</span>
  <span class="n">input_buffer</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">broadcast_in_dim</span><span class="p">(</span>
      <span class="n">input_buffer</span><span class="p">,</span>
      <span class="n">shape</span><span class="o">=</span><span class="p">(</span>
          <span class="n">NUM_DEVICES</span><span class="p">,</span>
          <span class="n">NUM_MICROBATCHES</span><span class="p">,</span>
          <span class="n">CONTRACTING_DIM_SIZE</span><span class="p">,</span>
          <span class="n">NON_CONTRACTING_DIM_SIZE</span><span class="p">,</span>
      <span class="p">),</span>
      <span class="n">broadcast_dimensions</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
  <span class="p">)</span>
  <span class="n">input_buffer</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span>
      <span class="n">input_buffer</span><span class="p">,</span>
      <span class="n">sharding</span><span class="o">.</span><span class="n">NamedSharding</span><span class="p">(</span>
          <span class="n">mesh</span><span class="p">,</span> <span class="n">sharding</span><span class="o">.</span><span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
      <span class="p">),</span>
  <span class="p">)</span>

  <span class="c1"># Run computation.</span>
  <span class="n">output_buffer</span> <span class="o">=</span> <span class="n">entry_computation</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">input_buffer</span><span class="p">,</span> <span class="n">mesh</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output_buffer = </span><span class="se">\n</span><span class="si">{</span><span class="n">output_buffer</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="using-psend-and-precv">
<h4>Using <code class="docutils literal notranslate"><span class="pre">psend</span></code> and <code class="docutils literal notranslate"><span class="pre">precv</span></code><a class="headerlink" href="#using-psend-and-precv" title="Link to this heading">#</a></h4>
<p>The JAX example above lowers to <code class="docutils literal notranslate"><span class="pre">collective-permute</span></code> HLO instructions, which are
are implemented through <code class="docutils literal notranslate"><span class="pre">ncclSend</span></code> and <code class="docutils literal notranslate"><span class="pre">ncclRecv</span></code> on GPU. For users who want
more granular control over the ordering of collectives, they can use
<code class="docutils literal notranslate"><span class="pre">jax.lax.psend</span></code> and <code class="docutils literal notranslate"><span class="pre">jax.lax.precv</span></code> directly. Syntactically, these two functions
are analogous to their HLO counterparts. Users should keep in mind that their
program will deadlock when the source-target pairs in a <em>single</em> <code class="docutils literal notranslate"><span class="pre">psend</span></code> or
<code class="docutils literal notranslate"><span class="pre">precv</span></code> form a cycle, and when <code class="docutils literal notranslate"><span class="pre">psend</span></code> is not matched by <code class="docutils literal notranslate"><span class="pre">precv</span></code> and
vice-versa.</p>
<p>If cycles are required in the device communication pattern, deadlocks can be
avoided by making sure that (1) no single <code class="docutils literal notranslate"><span class="pre">psend</span></code> or <code class="docutils literal notranslate"><span class="pre">precv</span></code> function‚Äôs
source-target pairs contain a cycle, and that (2) a fake data dependency
is inserted to sequentialize the send/recv pairs. No collective can be scheduled
between <code class="docutils literal notranslate"><span class="pre">psend</span></code>/<code class="docutils literal notranslate"><span class="pre">precv</span></code> paris, which can only be controlled through
<code class="docutils literal notranslate"><span class="pre">jax.lax.optimization_barrier</span></code> at the JAX level. The test case
<code class="docutils literal notranslate"><span class="pre">test_psend_precv_basic_with_no_deadlock_cycle</span></code> in the file
<a class="reference external" href="https://github.com/jax-ml/jax/blob/main/tests/shard_map_test.py"><code class="docutils literal notranslate"><span class="pre">shard_map_test.py</span></code></a> is one such example.</p>
<p>The pipeline parallelism example in the previous section uses the
<code class="docutils literal notranslate"><span class="pre">--xla_gpu_experimental_pipeline_parallelism_opt_level</span></code> XLA flag. The same
program can be rewritten using <code class="docutils literal notranslate"><span class="pre">psend</span></code> and <code class="docutils literal notranslate"><span class="pre">precv</span></code> without the flag, if manually
pipelined.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1">## same setup and imports</span>
<span class="k">def</span> <span class="nf">while_body</span><span class="p">(</span><span class="n">carry</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
  <span class="p">(</span>
      <span class="n">weights</span><span class="p">,</span>
      <span class="n">input_buffer</span><span class="p">,</span>
      <span class="n">output_buffer</span><span class="p">,</span>
      <span class="n">prev_compute_res</span><span class="p">,</span>
      <span class="n">prev_stage_slice_fwd</span><span class="p">,</span>
      <span class="n">prev_stage_slice_bwd</span><span class="p">,</span>
  <span class="p">)</span> <span class="o">=</span> <span class="n">carry</span>

  <span class="c1"># Read input data from input buffer.</span>
  <span class="n">input_slice</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">dynamic_slice</span><span class="p">(</span>
      <span class="n">input_buffer</span><span class="p">,</span>
      <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">0</span><span class="p">)</span> <span class="o">%</span> <span class="n">NUM_MICROBATCHES</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
      <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">CONTRACTING_DIM_SIZE</span><span class="p">,</span> <span class="n">NON_CONTRACTING_DIM_SIZE</span><span class="p">),</span>
  <span class="p">)</span>

  <span class="c1"># send_fwd</span>
  <span class="n">fwd_send_token</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">psend</span><span class="p">(</span>
      <span class="n">prev_compute_res</span><span class="p">,</span>
      <span class="n">axis_name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span>
      <span class="n">perm</span><span class="o">=</span><span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)],</span>
  <span class="p">)</span>

  <span class="c1"># Select compute argument based on device and pipeline cycle</span>
  <span class="n">compute_argument</span> <span class="o">=</span> <span class="n">select_on_first_device</span><span class="p">(</span>
      <span class="n">select_on_first_cycle</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">input_slice</span><span class="p">,</span> <span class="n">prev_stage_slice_bwd</span><span class="p">),</span>
      <span class="n">prev_stage_slice_fwd</span><span class="p">,</span>
  <span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">CONTRACTING_DIM_SIZE</span><span class="p">,</span> <span class="n">NON_CONTRACTING_DIM_SIZE</span><span class="p">))</span>

  <span class="n">tmp</span> <span class="o">=</span> <span class="n">compute_argument</span>
  <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">COMPUTE_INTENSITY</span><span class="p">):</span>
    <span class="n">tmp</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">dot_general</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">tmp</span><span class="p">,</span> <span class="p">(((</span><span class="mi">2</span><span class="p">,),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,)),</span> <span class="p">((</span><span class="mi">0</span><span class="p">,),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,))))</span>
  <span class="n">compute_result</span> <span class="o">=</span> <span class="n">tmp</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
      <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">CONTRACTING_DIM_SIZE</span><span class="p">,</span> <span class="n">NON_CONTRACTING_DIM_SIZE</span><span class="p">)</span>
  <span class="p">)</span>

  <span class="n">buffer_slice_for_bwd_ppermute</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">dynamic_slice</span><span class="p">(</span>
      <span class="n">output_buffer</span><span class="p">,</span>
      <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">NUM_MICROBATCHES</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
      <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">CONTRACTING_DIM_SIZE</span><span class="p">,</span> <span class="n">NON_CONTRACTING_DIM_SIZE</span><span class="p">),</span>
  <span class="p">)</span>

  <span class="c1"># make sure ppermute is scheduled after send_fwd</span>
  <span class="n">buffer_slice_for_bwd_ppermute_after_send_fwd</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="p">(</span>
      <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">optimization_barrier</span><span class="p">(</span>
          <span class="p">(</span><span class="n">buffer_slice_for_bwd_ppermute</span><span class="p">,</span> <span class="n">fwd_send_token</span><span class="p">)</span>
      <span class="p">)</span>
  <span class="p">)</span>
  <span class="c1"># ppermute_bwd</span>
  <span class="n">ppermute_bwd_data</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">ppermute</span><span class="p">(</span>
      <span class="n">buffer_slice_for_bwd_ppermute_after_send_fwd</span><span class="p">,</span>
      <span class="n">axis_name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span>
      <span class="n">perm</span><span class="o">=</span><span class="p">[(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">0</span><span class="p">)],</span>
  <span class="p">)</span>

  <span class="c1"># make sure recv is scheduled after ppermute</span>
  <span class="n">precv_token</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">optimization_barrier</span><span class="p">(</span>
      <span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">create_token</span><span class="p">(),</span> <span class="n">ppermute_bwd_data</span><span class="p">)</span>
  <span class="p">)</span>

  <span class="c1"># recv_fwd, matches the send_fwd in the next iteration</span>
  <span class="n">fwd_recv_data</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">precv</span><span class="p">(</span>
      <span class="n">precv_token</span><span class="p">,</span>
      <span class="n">out_shape</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">(</span>
          <span class="n">input_slice</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">input_slice</span><span class="o">.</span><span class="n">dtype</span>
      <span class="p">),</span>
      <span class="n">axis_name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span>
      <span class="n">perm</span><span class="o">=</span><span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)],</span>
  <span class="p">)</span>
  <span class="n">update_output_buffer</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">dynamic_update_slice</span><span class="p">(</span>
      <span class="n">output_buffer</span><span class="p">,</span>
      <span class="n">compute_result</span><span class="p">,</span>
      <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span> <span class="o">%</span> <span class="n">NUM_MICROBATCHES</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
  <span class="p">)</span>
  <span class="n">carry</span> <span class="o">=</span> <span class="p">(</span>
      <span class="n">weights</span><span class="p">,</span>
      <span class="n">input_buffer</span><span class="p">,</span>
      <span class="n">update_output_buffer</span><span class="p">,</span>
      <span class="n">compute_result</span><span class="p">,</span>
      <span class="n">fwd_recv_data</span><span class="p">,</span>
      <span class="n">ppermute_bwd_data</span><span class="p">,</span>
  <span class="p">)</span>
  <span class="k">return</span> <span class="n">carry</span><span class="p">,</span> <span class="n">i</span>


<span class="k">def</span> <span class="nf">entry_computation</span><span class="p">(</span>
    <span class="n">weights</span><span class="p">,</span> <span class="n">input_buffer</span><span class="p">,</span> <span class="n">dummy_data</span><span class="p">,</span> <span class="n">mesh</span>
<span class="p">):</span>

  <span class="c1"># Init output buffer.</span>
  <span class="n">output_buffer</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">input_buffer</span><span class="p">)</span>

  <span class="c1"># Start pipeline.</span>
  <span class="n">dummy_slice_fwd</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">precv</span><span class="p">(</span>
      <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">create_token</span><span class="p">(),</span>
      <span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">(</span><span class="n">dummy_data</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dummy_data</span><span class="o">.</span><span class="n">dtype</span><span class="p">),</span>
      <span class="n">axis_name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span>
      <span class="n">perm</span><span class="o">=</span><span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)],</span>
  <span class="p">)</span>

  <span class="n">carry</span> <span class="o">=</span> <span class="p">(</span>
      <span class="n">weights</span><span class="p">,</span>
      <span class="n">input_buffer</span><span class="p">,</span>
      <span class="n">output_buffer</span><span class="p">,</span>
      <span class="n">dummy_slice_fwd</span><span class="p">,</span>
      <span class="n">dummy_data</span><span class="p">,</span>
      <span class="n">dummy_data</span><span class="p">,</span>
  <span class="p">)</span>

  <span class="n">num_iterations</span> <span class="o">=</span> <span class="n">NUM_CIRC_REPEATS</span> <span class="o">*</span> <span class="n">NUM_MICROBATCHES</span> <span class="o">+</span> <span class="n">NUM_DEVICES</span> <span class="o">-</span> <span class="mi">1</span>
  <span class="n">carry</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span><span class="n">while_body</span><span class="p">,</span> <span class="n">carry</span><span class="p">,</span> <span class="n">xs</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">num_iterations</span><span class="p">))</span>

  <span class="n">_</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">psend</span><span class="p">(</span>
      <span class="n">carry</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
      <span class="n">axis_name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span>
      <span class="n">perm</span><span class="o">=</span><span class="p">[(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)],</span>
  <span class="p">)</span>

  <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">output_buffer</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">carry</span>

  <span class="k">return</span> <span class="n">output_buffer</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">_</span><span class="p">):</span>

  <span class="c1"># Expect constant number of devices.</span>
  <span class="k">assert</span> <span class="n">NUM_DEVICES</span> <span class="o">==</span> <span class="n">jax</span><span class="o">.</span><span class="n">local_device_count</span><span class="p">()</span>

  <span class="c1"># Create mesh.</span>
  <span class="n">mesh</span> <span class="o">=</span> <span class="n">Mesh</span><span class="p">(</span>
      <span class="n">mesh_utils</span><span class="o">.</span><span class="n">create_device_mesh</span><span class="p">([</span><span class="n">NUM_DEVICES</span><span class="p">]),</span>
      <span class="n">axis_names</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">],</span>
  <span class="p">)</span>
  <span class="c1"># Init weights.</span>
  <span class="n">weights</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">CONTRACTING_DIM_SIZE</span>
  <span class="n">weights</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">broadcast_in_dim</span><span class="p">(</span>
      <span class="n">weights</span><span class="p">,</span>
      <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">NUM_DEVICES</span><span class="p">,</span> <span class="n">CONTRACTING_DIM_SIZE</span><span class="p">,</span> <span class="n">CONTRACTING_DIM_SIZE</span><span class="p">),</span>
      <span class="n">broadcast_dimensions</span><span class="o">=</span><span class="p">(),</span>
  <span class="p">)</span>
  <span class="n">weights</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span>
      <span class="n">weights</span><span class="p">,</span> <span class="n">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">P</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">))</span>
  <span class="p">)</span>
  <span class="c1"># Init input.</span>
  <span class="n">random_key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">input_buffer</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span>
      <span class="n">random_key</span><span class="p">,</span>
      <span class="n">shape</span><span class="o">=</span><span class="p">(</span>
          <span class="n">NUM_MICROBATCHES</span><span class="p">,</span>
          <span class="n">CONTRACTING_DIM_SIZE</span><span class="p">,</span>
          <span class="n">NON_CONTRACTING_DIM_SIZE</span><span class="p">,</span>
      <span class="p">),</span>
  <span class="p">)</span>
  <span class="n">input_buffer</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">broadcast_in_dim</span><span class="p">(</span>
      <span class="n">input_buffer</span><span class="p">,</span>
      <span class="n">shape</span><span class="o">=</span><span class="p">(</span>
          <span class="n">NUM_DEVICES</span><span class="p">,</span>
          <span class="n">NUM_MICROBATCHES</span><span class="p">,</span>
          <span class="n">CONTRACTING_DIM_SIZE</span><span class="p">,</span>
          <span class="n">NON_CONTRACTING_DIM_SIZE</span><span class="p">,</span>
      <span class="p">),</span>
      <span class="n">broadcast_dimensions</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
  <span class="p">)</span>

  <span class="n">input_buffer</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span>
      <span class="n">input_buffer</span><span class="p">,</span>
      <span class="n">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">P</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)),</span>
  <span class="p">)</span>
  <span class="c1"># Init dummy data for forward and backward edge passed through the while</span>
  <span class="c1"># loop.</span>
  <span class="n">dummy_slice</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
      <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">NUM_DEVICES</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">CONTRACTING_DIM_SIZE</span><span class="p">,</span> <span class="n">NON_CONTRACTING_DIM_SIZE</span><span class="p">)</span>
  <span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
  <span class="n">dummy_data</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">device_put</span><span class="p">(</span>
      <span class="n">dummy_slice</span><span class="p">,</span>
      <span class="n">NamedSharding</span><span class="p">(</span><span class="n">mesh</span><span class="p">,</span> <span class="n">P</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)),</span>
  <span class="p">)</span>

  <span class="n">entry</span> <span class="o">=</span> <span class="n">partial</span><span class="p">(</span><span class="n">entry_computation</span><span class="p">,</span> <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">)</span>

  <span class="n">output_buffer</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span>
      <span class="n">jax</span><span class="o">.</span><span class="n">shard_map</span><span class="p">(</span>
          <span class="n">entry</span><span class="p">,</span>
          <span class="n">mesh</span><span class="o">=</span><span class="n">mesh</span><span class="p">,</span>
          <span class="n">in_specs</span><span class="o">=</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">),</span>
          <span class="n">out_specs</span><span class="o">=</span><span class="n">P</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">),</span>
          <span class="n">check_vma</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
      <span class="p">)</span>
  <span class="p">)(</span><span class="n">weights</span><span class="p">,</span> <span class="n">input_buffer</span><span class="p">,</span> <span class="n">dummy_data</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;output_buffer = </span><span class="se">\n</span><span class="si">{</span><span class="n">output_buffer</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="nccl-flags">
<h2>NCCL flags<a class="headerlink" href="#nccl-flags" title="Link to this heading">#</a></h2>
<p>These Nvidia NCCL flag values may be useful for single-host multi-device
computations on Nvidia GPUs:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
  <span class="s2">&quot;NCCL_LL128_BUFFSIZE&quot;</span><span class="p">:</span> <span class="s2">&quot;-2&quot;</span><span class="p">,</span>
  <span class="s2">&quot;NCCL_LL_BUFFSIZE&quot;</span><span class="p">:</span> <span class="s2">&quot;-2&quot;</span><span class="p">,</span>
   <span class="s2">&quot;NCCL_PROTO&quot;</span><span class="p">:</span> <span class="s2">&quot;SIMPLE,LL,LL128&quot;</span><span class="p">,</span>
 <span class="p">})</span>
</pre></div>
</div>
<p>These NCCL flags could improve single-host communication speed. These flags
don‚Äôt seem useful for multi-host communication yet.</p>
</section>
<section id="multi-process">
<h2>Multi-Process<a class="headerlink" href="#multi-process" title="Link to this heading">#</a></h2>
<p>We recommend using one process per GPU and not one per node.  In some
cases, this can speed up jitted computation. The
<a class="reference internal" href="_autosummary/jax.distributed.initialize.html#jax.distributed.initialize" title="jax.distributed.initialize"><code class="xref py py-func docutils literal notranslate"><span class="pre">jax.distributed.initialize()</span></code></a> API will automatically understand
that configuration when run under SLURM. However, this only a rule of
thumb and it may be useful to test both one process per GPU and one
process per node on your use case.</p>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="persistent_compilation_cache.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Persistent compilation cache</p>
      </div>
    </a>
    <a class="right-next"
       href="profiling.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Profiling computation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#matmul-precision">Matmul precision</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#xla-performance-flags">XLA performance flags</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-generation-flags">Code generation flags</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#communication-tips">Communication tips</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-and-manual-pgle">Auto and manual PGLE</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#auto-pgle">Auto PGLE</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#collecting-nvidia-nsight-systems-profiles-when-using-autopgle">Collecting NVIDIA Nsight Systems profiles when using AutoPGLE</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#manual-pgle">Manual PGLE</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#flags">Flags</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pipeline-parallelism-on-gpu">Pipeline Parallelism on GPU</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-xla-flags">Using XLA Flags</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-psend-and-precv">Using <code class="docutils literal notranslate"><span class="pre">psend</span></code> and <code class="docutils literal notranslate"><span class="pre">precv</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#nccl-flags">NCCL flags</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-process">Multi-Process</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The JAX authors
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2024, The JAX Authors.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>