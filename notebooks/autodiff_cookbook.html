
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>The Autodiff Cookbook &#8212; JAX  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/style.css?v=7143c0a5" />
    <link rel="stylesheet" href="../_static/style.css" type="text/css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=30646c52"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/autodiff_cookbook';</script>
    <link rel="icon" href="../_static/favicon.png"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Custom derivative rules" href="Custom_derivative_rules_for_Python_code.html" />
    <link rel="prev" title="Resources and Advanced Guides" href="../advanced_guides.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/jax_logo_250px.png" class="logo__image only-light" alt="JAX  documentation - Home"/>
    <script>document.write(`<img src="../_static/jax_logo_250px.png" class="logo__image only-dark" alt="JAX  documentation - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="thinking_in_jax.html">Quickstart: How to think in JAX</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Common_Gotchas_in_JAX.html">🔪 JAX - The Sharp Bits 🔪</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../tutorials.html">Tutorials</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../jit-compilation.html">Just-in-time compilation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../automatic-vectorization.html">Automatic vectorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../automatic-differentiation.html">Automatic differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../random-numbers.html">Pseudorandom numbers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../stateful-computations.html">Stateful computations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../control-flow.html">Control flow and logical operators with JIT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../pytrees.html">Pytrees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../working-with-pytrees.html">Working with pytrees</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Resources, guides, and references</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../key-concepts.html">Key concepts</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../advanced_guides.html">Resources and Advanced Guides</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2 current active"><a class="current reference internal" href="#">The Autodiff Cookbook</a></li>
<li class="toctree-l2"><a class="reference internal" href="Custom_derivative_rules_for_Python_code.html">Custom derivative rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="autodiff_remat.html">Control autodiff’s saved values with <code class="docutils literal notranslate"><span class="pre">jax.checkpoint</span></code> (aka <code class="docutils literal notranslate"><span class="pre">jax.remat</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../advanced-autodiff.html">Advanced automatic differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../errors.html">Errors</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../debugging.html">Introduction to debugging</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../debugging/print_breakpoint.html">Compiled prints and breakpoints</a></li>
<li class="toctree-l3"><a class="reference internal" href="../debugging/checkify_guide.html">The <code class="docutils literal notranslate"><span class="pre">checkify</span></code> transformation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../debugging/flags.html">JAX debugging flags</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../debugging/flags.html">JAX debugging flags</a></li>
<li class="toctree-l2"><a class="reference internal" href="../transfer_guard.html">Transfer guard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../persistent_compilation_cache.html">Persistent compilation cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu_performance_tips.html">GPU performance tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="../profiling.html">Profiling computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../device_memory_profiling.html">Profiling device memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="Distributed_arrays_and_automatic_parallelization.html">Distributed arrays and automatic parallelization</a></li>
<li class="toctree-l2"><a class="reference internal" href="explicit-sharding.html">Explicit sharding (a.k.a. “sharding in types”)</a></li>
<li class="toctree-l2"><a class="reference internal" href="shard_map.html">Manual parallelism with <code class="docutils literal notranslate"><span class="pre">shard_map</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="layout.html">Device-local array layout control</a></li>
<li class="toctree-l2"><a class="reference internal" href="host-offloading.html">JAX Memories and Host Offloading</a></li>

<li class="toctree-l2"><a class="reference internal" href="../multi_process.html">Introduction to multi-controller JAX (aka multi-process/multi-host JAX)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../distributed_data_loading.html">Distributed data loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../external-callbacks.html">External callbacks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../ffi.html">Foreign function interface (FFI)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gradient-checkpointing.html">Gradient checkpointing with <code class="docutils literal notranslate"><span class="pre">jax.checkpoint</span></code> (<code class="docutils literal notranslate"><span class="pre">jax.remat</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../aot.html">Ahead-of-time lowering and compilation</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../export/index.html">Exporting and serialization</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../export/export.html">Exporting and serializing staged-out computations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../export/shape_poly.html">Shape polymorphism</a></li>
<li class="toctree-l3"><a class="reference internal" href="../export/jax2tf.html">Interoperation with TensorFlow</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../type_promotion.html">Type promotion semantics</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../pallas/index.html">Pallas: a JAX kernel language</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../pallas/quickstart.html">Pallas Quickstart</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pallas/pipelining.html">Software Pipelining</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pallas/grid_blockspec.html">Grids and BlockSpecs</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../pallas/tpu/index.html">Pallas TPU</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../pallas/tpu/details.html">Writing TPU kernels with Pallas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../pallas/tpu/pipelining.html">TPU Pipelining</a></li>
<li class="toctree-l4"><a class="reference internal" href="../pallas/tpu/matmul.html">Matrix Multiplication</a></li>
<li class="toctree-l4"><a class="reference internal" href="../pallas/tpu/sparse.html">Scalar Prefetch and Block-Sparse Computation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../pallas/tpu/distributed.html">Distributed Computing in Pallas for TPUs</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../pallas/gpu/index.html">Pallas:Mosaic GPU</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../pallas/gpu/reference.html">Writing Mosaic GPU kernels with Pallas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../pallas/gpu/pipelining.html">Mosaic GPU Pipelining</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../pallas/design/index.html">Pallas Design Notes</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../pallas/design/design.html">Pallas Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="../pallas/design/async_note.html">Pallas Async Operations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../pallas/CHANGELOG.html">Pallas Changelog</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="neural_network_with_tfds_data.html">Training a simple neural network, with tensorflow/datasets data loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="Neural_Network_and_Data_Loading.html">Training a simple neural network, with PyTorch data loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="vmapped_log_probs.html">Autobatching for Bayesian inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="convolutions.html">Generalized convolutions in JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../xla_flags.html">XLA compiler flags</a></li>
<li class="toctree-l2"><a class="reference internal" href="../sharded-computation.html">Introduction to parallel programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax-primitives.html">JAX Internals: primitives</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jaxpr.html">JAX internals: The jaxpr language</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../contributor_guide.html">Developer notes</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../contributing.html">Contributing to JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../developer.html">Building from source</a></li>
<li class="toctree-l2"><a class="reference internal" href="../investigating_a_regression.html">Investigating a regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autodidax.html">Autodidax: JAX core from scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../autodidax2_part1.html">Autodidax2, part 1: JAX from scratch, again</a></li>

<li class="toctree-l2 has-children"><a class="reference internal" href="../jep/index.html">JAX Enhancement Proposals (JEPs)</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../jep/263-prng.html">263: JAX PRNG Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/2026-custom-derivatives.html">2026: Custom JVP/VJP rules for JAX-transformable functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/4008-custom-vjp-update.html">4008: Custom VJP and `nondiff_argnums` update</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/4410-omnistaging.html">4410: Omnistaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/9263-typed-keys.html">9263: Typed keys &amp; pluggable RNGs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/9407-type-promotion.html">9407: Design of Type Promotion Semantics for JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/9419-jax-versioning.html">9419: Jax and Jaxlib versioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/10657-sequencing-effects.html">10657: Sequencing side-effects in JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/11830-new-remat-checkpoint.html">11830: `jax.remat` / `jax.checkpoint` new implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/12049-type-annotations.html">12049: Type Annotation Roadmap for JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/14273-shard-map.html">14273: `shard_map` (`shmap`) for simple per-device code</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/15856-jex.html">15856: `jax.extend`, an extensions module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/17111-shmap-transpose.html">17111: Efficient transposition of `shard_map` (and other maps)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/18137-numpy-scipy-scope.html">18137: Scope of JAX NumPy &amp; SciPy Wrappers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/25516-effver.html">25516: Effort-based versioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jep/28661-jax-array-protocol.html">28661: Supporting the `__jax_array__` protocol</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../extensions.html">Extension guides</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="Writing_custom_interpreters_in_Jax.html">Writing custom Jaxpr interpreters in JAX</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../jax.extend.html"><code class="docutils literal notranslate"><span class="pre">jax.extend</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../jax.extend.core.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.core</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.extend.linear_util.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.linear_util</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.extend.mlir.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.mlir</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.extend.random.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.random</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../building_on_jax.html">Building on JAX</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../notes.html">Notes</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../api_compatibility.html">API compatibility</a></li>
<li class="toctree-l2"><a class="reference internal" href="../deprecation.html">Python and NumPy version support policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../async_dispatch.html">Asynchronous dispatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../concurrency.html">Concurrency</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gpu_memory_allocation.html">GPU memory allocation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../rank_promotion_warning.html">Rank promotion warning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../type_promotion.html">Type promotion semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../default_dtypes.html">Default dtypes and the X64 flag</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../jax.html">Public API: <code class="docutils literal notranslate"><span class="pre">jax</span></code> package</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../jax.numpy.html"><code class="docutils literal notranslate"><span class="pre">jax.numpy</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.fft.html">jax.numpy.fft.fft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.fft2.html">jax.numpy.fft.fft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.fftfreq.html">jax.numpy.fft.fftfreq</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.fftn.html">jax.numpy.fft.fftn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.fftshift.html">jax.numpy.fft.fftshift</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.hfft.html">jax.numpy.fft.hfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.ifft.html">jax.numpy.fft.ifft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.ifft2.html">jax.numpy.fft.ifft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.ifftn.html">jax.numpy.fft.ifftn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.ifftshift.html">jax.numpy.fft.ifftshift</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.ihfft.html">jax.numpy.fft.ihfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.irfft.html">jax.numpy.fft.irfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.irfft2.html">jax.numpy.fft.irfft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.irfftn.html">jax.numpy.fft.irfftn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.rfft.html">jax.numpy.fft.rfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.rfft2.html">jax.numpy.fft.rfft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.rfftfreq.html">jax.numpy.fft.rfftfreq</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.numpy.fft.rfftn.html">jax.numpy.fft.rfftn</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../jax.scipy.html"><code class="docutils literal notranslate"><span class="pre">jax.scipy</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.scipy.stats.bernoulli.logpmf.html">jax.scipy.stats.bernoulli.logpmf</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.scipy.stats.bernoulli.pmf.html">jax.scipy.stats.bernoulli.pmf</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.scipy.stats.bernoulli.cdf.html">jax.scipy.stats.bernoulli.cdf</a></li>
<li class="toctree-l3"><a class="reference internal" href="../_autosummary/jax.scipy.stats.bernoulli.ppf.html">jax.scipy.stats.bernoulli.ppf</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../jax.lax.html"><code class="docutils literal notranslate"><span class="pre">jax.lax</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.random.html"><code class="docutils literal notranslate"><span class="pre">jax.random</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.sharding.html"><code class="docutils literal notranslate"><span class="pre">jax.sharding</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.debug.html"><code class="docutils literal notranslate"><span class="pre">jax.debug</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.dlpack.html"><code class="docutils literal notranslate"><span class="pre">jax.dlpack</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.distributed.html"><code class="docutils literal notranslate"><span class="pre">jax.distributed</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.dtypes.html"><code class="docutils literal notranslate"><span class="pre">jax.dtypes</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.ffi.html"><code class="docutils literal notranslate"><span class="pre">jax.ffi</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.flatten_util.html"><code class="docutils literal notranslate"><span class="pre">jax.flatten_util</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.image.html"><code class="docutils literal notranslate"><span class="pre">jax.image</span></code> module</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../jax.nn.html"><code class="docutils literal notranslate"><span class="pre">jax.nn</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../jax.nn.initializers.html"><code class="docutils literal notranslate"><span class="pre">jax.nn.initializers</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../jax.ops.html"><code class="docutils literal notranslate"><span class="pre">jax.ops</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.profiler.html"><code class="docutils literal notranslate"><span class="pre">jax.profiler</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.stages.html"><code class="docutils literal notranslate"><span class="pre">jax.stages</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.test_util.html"><code class="docutils literal notranslate"><span class="pre">jax.test_util</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.tree.html"><code class="docutils literal notranslate"><span class="pre">jax.tree</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.tree_util.html"><code class="docutils literal notranslate"><span class="pre">jax.tree_util</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.typing.html"><code class="docutils literal notranslate"><span class="pre">jax.typing</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../jax.export.html"><code class="docutils literal notranslate"><span class="pre">jax.export</span></code> module</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../jax.extend.html"><code class="docutils literal notranslate"><span class="pre">jax.extend</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../jax.extend.core.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.core</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.extend.linear_util.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.linear_util</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.extend.mlir.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.mlir</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.extend.random.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.random</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../jax.example_libraries.html"><code class="docutils literal notranslate"><span class="pre">jax.example_libraries</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../jax.example_libraries.optimizers.html"><code class="docutils literal notranslate"><span class="pre">jax.example_libraries.optimizers</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.example_libraries.stax.html"><code class="docutils literal notranslate"><span class="pre">jax.example_libraries.stax</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../jax.experimental.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../jax.experimental.checkify.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.checkify</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.experimental.compilation_cache.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.compilation_cache</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.experimental.custom_dce.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.custom_dce</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.experimental.custom_partitioning.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.custom_partitioning</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.experimental.jet.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.jet</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.experimental.key_reuse.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.key_reuse</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.experimental.mesh_utils.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.mesh_utils</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.experimental.multihost_utils.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.multihost_utils</span></code> module</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../jax.experimental.pallas.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pallas</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../jax.experimental.pallas.mosaic_gpu.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pallas.mosaic_gpu</span></code> module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../jax.experimental.pallas.triton.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pallas.triton</span></code> module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../jax.experimental.pallas.tpu.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pallas.tpu</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../jax.experimental.pjit.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pjit</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.experimental.serialize_executable.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.serialize_executable</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../jax.experimental.shard_map.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.shard_map</span></code> module</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../jax.experimental.sparse.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.sparse</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-22"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.BCOO.html">jax.experimental.sparse.BCOO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_broadcast_in_dim.html">jax.experimental.sparse.bcoo_broadcast_in_dim</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_concatenate.html">jax.experimental.sparse.bcoo_concatenate</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_dot_general.html">jax.experimental.sparse.bcoo_dot_general</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_dot_general_sampled.html">jax.experimental.sparse.bcoo_dot_general_sampled</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_dynamic_slice.html">jax.experimental.sparse.bcoo_dynamic_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_extract.html">jax.experimental.sparse.bcoo_extract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_fromdense.html">jax.experimental.sparse.bcoo_fromdense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_gather.html">jax.experimental.sparse.bcoo_gather</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_multiply_dense.html">jax.experimental.sparse.bcoo_multiply_dense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_multiply_sparse.html">jax.experimental.sparse.bcoo_multiply_sparse</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_update_layout.html">jax.experimental.sparse.bcoo_update_layout</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_reduce_sum.html">jax.experimental.sparse.bcoo_reduce_sum</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_reshape.html">jax.experimental.sparse.bcoo_reshape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_slice.html">jax.experimental.sparse.bcoo_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_sort_indices.html">jax.experimental.sparse.bcoo_sort_indices</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_squeeze.html">jax.experimental.sparse.bcoo_squeeze</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_sum_duplicates.html">jax.experimental.sparse.bcoo_sum_duplicates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_todense.html">jax.experimental.sparse.bcoo_todense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../_autosummary/jax.experimental.sparse.bcoo_transpose.html">jax.experimental.sparse.bcoo_transpose</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../jax.lib.html"><code class="docutils literal notranslate"><span class="pre">jax.lib</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.addressable_shards.html">jax.Array.addressable_shards</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.all.html">jax.Array.all</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.any.html">jax.Array.any</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.argmax.html">jax.Array.argmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.argmin.html">jax.Array.argmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.argpartition.html">jax.Array.argpartition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.argsort.html">jax.Array.argsort</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.astype.html">jax.Array.astype</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.at.html">jax.Array.at</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.choose.html">jax.Array.choose</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.clip.html">jax.Array.clip</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.compress.html">jax.Array.compress</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.committed.html">jax.Array.committed</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.conj.html">jax.Array.conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.conjugate.html">jax.Array.conjugate</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.copy.html">jax.Array.copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.copy_to_host_async.html">jax.Array.copy_to_host_async</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.cumprod.html">jax.Array.cumprod</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.cumsum.html">jax.Array.cumsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.device.html">jax.Array.device</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.diagonal.html">jax.Array.diagonal</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.dot.html">jax.Array.dot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.dtype.html">jax.Array.dtype</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.flat.html">jax.Array.flat</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.flatten.html">jax.Array.flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.global_shards.html">jax.Array.global_shards</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.imag.html">jax.Array.imag</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.is_fully_addressable.html">jax.Array.is_fully_addressable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.is_fully_replicated.html">jax.Array.is_fully_replicated</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.item.html">jax.Array.item</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.itemsize.html">jax.Array.itemsize</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.max.html">jax.Array.max</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.mean.html">jax.Array.mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.min.html">jax.Array.min</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.nbytes.html">jax.Array.nbytes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.ndim.html">jax.Array.ndim</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.nonzero.html">jax.Array.nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.prod.html">jax.Array.prod</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.ptp.html">jax.Array.ptp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.ravel.html">jax.Array.ravel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.real.html">jax.Array.real</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.repeat.html">jax.Array.repeat</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.reshape.html">jax.Array.reshape</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.round.html">jax.Array.round</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.searchsorted.html">jax.Array.searchsorted</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.shape.html">jax.Array.shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.sharding.html">jax.Array.sharding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.size.html">jax.Array.size</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.sort.html">jax.Array.sort</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.squeeze.html">jax.Array.squeeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.std.html">jax.Array.std</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.sum.html">jax.Array.sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.swapaxes.html">jax.Array.swapaxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.take.html">jax.Array.take</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.to_device.html">jax.Array.to_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.trace.html">jax.Array.trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.transpose.html">jax.Array.transpose</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.var.html">jax.Array.var</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.view.html">jax.Array.view</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.T.html">jax.Array.T</a></li>
<li class="toctree-l2"><a class="reference internal" href="../_autosummary/jax.Array.mT.html">jax.Array.mT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../about.html">About the project</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../faq.html">Frequently asked questions (FAQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../changelog.html">Change log</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary of terms</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../config_options.html">Configuration Options</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
        <div class="header-article-item">



<nav aria-label="Breadcrumb">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../advanced_guides.html" class="nav-link">Resources and Advanced Guides</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">The...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/jax-ml/jax" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notebooks/autodiff_cookbook.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>The Autodiff Cookbook</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradients">Gradients</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#starting-with-grad">Starting with <code class="docutils literal notranslate"><span class="pre">grad</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#differentiating-with-respect-to-nested-lists-tuples-and-dicts">Differentiating with respect to nested lists, tuples, and dicts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluate-a-function-and-its-gradient-using-value-and-grad">Evaluate a function and its gradient using <code class="docutils literal notranslate"><span class="pre">value_and_grad</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checking-against-numerical-differences">Checking against numerical differences</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hessian-vector-products-with-grad-of-grad">Hessian-vector products with <code class="docutils literal notranslate"><span class="pre">grad</span></code>-of-<code class="docutils literal notranslate"><span class="pre">grad</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jacobians-and-hessians-using-jacfwd-and-jacrev">Jacobians and Hessians using <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code> and <code class="docutils literal notranslate"><span class="pre">jacrev</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-it-s-made-two-foundational-autodiff-functions">How it’s made: two foundational autodiff functions</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jacobian-vector-products-jvps-aka-forward-mode-autodiff">Jacobian-Vector products (JVPs, aka forward-mode autodiff)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#jvps-in-math">JVPs in math</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#jvps-in-jax-code">JVPs in JAX code</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-jacobian-products-vjps-aka-reverse-mode-autodiff">Vector-Jacobian products (VJPs, aka reverse-mode autodiff)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#vjps-in-math">VJPs in math</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#vjps-in-jax-code">VJPs in JAX code</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-valued-gradients-with-vjps">Vector-valued gradients with VJPs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hessian-vector-products-using-both-forward-and-reverse-mode">Hessian-vector products using both forward- and reverse-mode</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#composing-vjps-jvps-and-vmap">Composing VJPs, JVPs, and <code class="docutils literal notranslate"><span class="pre">vmap</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jacobian-matrix-and-matrix-jacobian-products">Jacobian-Matrix and Matrix-Jacobian products</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-implementation-of-jacfwd-and-jacrev">The implementation of <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code> and <code class="docutils literal notranslate"><span class="pre">jacrev</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#complex-numbers-and-differentiation">Complex numbers and differentiation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-advanced-autodiff">More advanced autodiff</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="the-autodiff-cookbook">
<h1>The Autodiff Cookbook<a class="headerlink" href="#the-autodiff-cookbook" title="Link to this heading">#</a></h1>
<!--* freshness: { reviewed: '2024-04-08' } *-->
<p><a class="reference external" href="https://colab.research.google.com/github/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.ipynb"><img alt="Open in Colab" src="https://colab.research.google.com/assets/colab-badge.svg" /></a> <a class="reference external" href="https://kaggle.com/kernels/welcome?src=https://github.com/jax-ml/jax/blob/main/docs/notebooks/autodiff_cookbook.ipynb"><img alt="Open in Kaggle" src="https://kaggle.com/static/images/open-in-kaggle.svg" /></a></p>
<p>JAX has a pretty general automatic differentiation system. In this notebook, we’ll go through a whole bunch of neat autodiff ideas that you can cherry pick for your own work, starting with the basics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">grad</span><span class="p">,</span> <span class="n">jit</span><span class="p">,</span> <span class="n">vmap</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">random</span>

<span class="n">key</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="gradients">
<h2>Gradients<a class="headerlink" href="#gradients" title="Link to this heading">#</a></h2>
<section id="starting-with-grad">
<h3>Starting with <code class="docutils literal notranslate"><span class="pre">grad</span></code><a class="headerlink" href="#starting-with-grad" title="Link to this heading">#</a></h3>
<p>You can differentiate a function with <code class="docutils literal notranslate"><span class="pre">grad</span></code>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">grad_tanh</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grad_tanh</span><span class="p">(</span><span class="mf">2.0</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0.070650816
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">grad</span></code> takes a function and returns a function. If you have a Python function <code class="docutils literal notranslate"><span class="pre">f</span></code> that evaluates the mathematical function <span class="math notranslate nohighlight">\(f\)</span>, then <code class="docutils literal notranslate"><span class="pre">grad(f)</span></code> is a Python function that evaluates the mathematical function <span class="math notranslate nohighlight">\(\nabla f\)</span>. That means <code class="docutils literal notranslate"><span class="pre">grad(f)(x)</span></code> represents the value <span class="math notranslate nohighlight">\(\nabla f(x)\)</span>.</p>
<p>Since <code class="docutils literal notranslate"><span class="pre">grad</span></code> operates on functions, you can apply it to its own output to differentiate as many times as you like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">))(</span><span class="mf">2.0</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">)))(</span><span class="mf">2.0</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>-0.13621868
0.25265405
</pre></div>
</div>
</div>
</div>
<p>Let’s look at computing gradients with <code class="docutils literal notranslate"><span class="pre">grad</span></code> in a linear logistic regression model. First, the setup:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Outputs probability of a label being true.</span>
<span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>

<span class="c1"># Build a toy dataset.</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.52</span><span class="p">,</span> <span class="mf">1.12</span><span class="p">,</span>  <span class="mf">0.77</span><span class="p">],</span>
                   <span class="p">[</span><span class="mf">0.88</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.08</span><span class="p">,</span> <span class="mf">0.15</span><span class="p">],</span>
                   <span class="p">[</span><span class="mf">0.52</span><span class="p">,</span> <span class="mf">0.06</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.30</span><span class="p">],</span>
                   <span class="p">[</span><span class="mf">0.74</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.49</span><span class="p">,</span> <span class="mf">1.39</span><span class="p">]])</span>
<span class="n">targets</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="kc">True</span><span class="p">,</span> <span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">,</span> <span class="kc">True</span><span class="p">])</span>

<span class="c1"># Training loss is the negative log-likelihood of the training examples.</span>
<span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="n">label_probs</span> <span class="o">=</span> <span class="n">preds</span> <span class="o">*</span> <span class="n">targets</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">preds</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">targets</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">label_probs</span><span class="p">))</span>

<span class="c1"># Initialize random model coefficients</span>
<span class="n">key</span><span class="p">,</span> <span class="n">W_key</span><span class="p">,</span> <span class="n">b_key</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">W</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">W_key</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">b_key</span><span class="p">,</span> <span class="p">())</span>
</pre></div>
</div>
</div>
</div>
<p>Use the <code class="docutils literal notranslate"><span class="pre">grad</span></code> function with its <code class="docutils literal notranslate"><span class="pre">argnums</span></code> argument to differentiate a function with respect to positional arguments.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Differentiate `loss` with respect to the first positional argument:</span>
<span class="n">W_grad</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">argnums</span><span class="o">=</span><span class="mi">0</span><span class="p">)(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;W_grad&#39;</span><span class="p">,</span> <span class="n">W_grad</span><span class="p">)</span>

<span class="c1"># Since argnums=0 is the default, this does the same thing:</span>
<span class="n">W_grad</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">)(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;W_grad&#39;</span><span class="p">,</span> <span class="n">W_grad</span><span class="p">)</span>

<span class="c1"># But we can choose different values too, and drop the keyword:</span>
<span class="n">b_grad</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="mi">1</span><span class="p">)(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;b_grad&#39;</span><span class="p">,</span> <span class="n">b_grad</span><span class="p">)</span>

<span class="c1"># Including tuple values</span>
<span class="n">W_grad</span><span class="p">,</span> <span class="n">b_grad</span> <span class="o">=</span> <span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;W_grad&#39;</span><span class="p">,</span> <span class="n">W_grad</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;b_grad&#39;</span><span class="p">,</span> <span class="n">b_grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>W_grad [-0.433146  -0.7354605 -1.2598922]
W_grad [-0.433146  -0.7354605 -1.2598922]
b_grad -0.6900178
W_grad [-0.433146  -0.7354605 -1.2598922]
b_grad -0.6900178
</pre></div>
</div>
</div>
</div>
<p>This <code class="docutils literal notranslate"><span class="pre">grad</span></code> API has a direct correspondence to the excellent notation in Spivak’s classic <em>Calculus on Manifolds</em> (1965), also used in Sussman and Wisdom’s <a class="reference external" href="https://mitpress.mit.edu/9780262028967/structure-and-interpretation-of-classical-mechanics"><em>Structure and Interpretation of Classical Mechanics</em></a> (2015) and their <a class="reference external" href="https://mitpress.mit.edu/9780262019347/functional-differential-geometry"><em>Functional Differential Geometry</em></a> (2013). Both books are open-access. See in particular the “Prologue” section of <em>Functional Differential Geometry</em> for a defense of this notation.</p>
<p>Essentially, when using the <code class="docutils literal notranslate"><span class="pre">argnums</span></code> argument, if <code class="docutils literal notranslate"><span class="pre">f</span></code> is a Python function for evaluating the mathematical function <span class="math notranslate nohighlight">\(f\)</span>, then the Python expression <code class="docutils literal notranslate"><span class="pre">grad(f,</span> <span class="pre">i)</span></code> evaluates to a Python function for evaluating <span class="math notranslate nohighlight">\(\partial_i f\)</span>.</p>
</section>
<section id="differentiating-with-respect-to-nested-lists-tuples-and-dicts">
<h3>Differentiating with respect to nested lists, tuples, and dicts<a class="headerlink" href="#differentiating-with-respect-to-nested-lists-tuples-and-dicts" title="Link to this heading">#</a></h3>
<p>Differentiating with respect to standard Python containers just works, so use tuples, lists, and dicts (and arbitrary nesting) however you like.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">loss2</span><span class="p">(</span><span class="n">params_dict</span><span class="p">):</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">predict</span><span class="p">(</span><span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;W&#39;</span><span class="p">],</span> <span class="n">params_dict</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">],</span> <span class="n">inputs</span><span class="p">)</span>
    <span class="n">label_probs</span> <span class="o">=</span> <span class="n">preds</span> <span class="o">*</span> <span class="n">targets</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">preds</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">targets</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">label_probs</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">loss2</span><span class="p">)({</span><span class="s1">&#39;W&#39;</span><span class="p">:</span> <span class="n">W</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">:</span> <span class="n">b</span><span class="p">}))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;W&#39;: Array([-0.433146 , -0.7354605, -1.2598922], dtype=float32), &#39;b&#39;: Array(-0.6900178, dtype=float32)}
</pre></div>
</div>
</div>
</div>
<p>You can <a class="reference external" href="https://github.com/jax-ml/jax/issues/446#issuecomment-467105048">register your own container types</a> to work with not just <code class="docutils literal notranslate"><span class="pre">grad</span></code> but all the JAX transformations (<code class="docutils literal notranslate"><span class="pre">jit</span></code>, <code class="docutils literal notranslate"><span class="pre">vmap</span></code>, etc.).</p>
</section>
<section id="evaluate-a-function-and-its-gradient-using-value-and-grad">
<h3>Evaluate a function and its gradient using <code class="docutils literal notranslate"><span class="pre">value_and_grad</span></code><a class="headerlink" href="#evaluate-a-function-and-its-gradient-using-value-and-grad" title="Link to this heading">#</a></h3>
<p>Another convenient function is <code class="docutils literal notranslate"><span class="pre">value_and_grad</span></code> for efficiently computing both a function’s value as well as its gradient’s value:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">value_and_grad</span>
<span class="n">loss_value</span><span class="p">,</span> <span class="n">Wb_grad</span> <span class="o">=</span> <span class="n">value_and_grad</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">))(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;loss value&#39;</span><span class="p">,</span> <span class="n">loss_value</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;loss value&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>loss value 2.9729187
loss value 2.9729187
</pre></div>
</div>
</div>
</div>
</section>
<section id="checking-against-numerical-differences">
<h3>Checking against numerical differences<a class="headerlink" href="#checking-against-numerical-differences" title="Link to this heading">#</a></h3>
<p>A great thing about derivatives is that they’re straightforward to check with finite differences:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set a step size for finite differences calculations</span>
<span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-4</span>

<span class="c1"># Check b_grad with scalar finite differences</span>
<span class="n">b_grad_numerical</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span> <span class="o">+</span> <span class="n">eps</span> <span class="o">/</span> <span class="mf">2.</span><span class="p">)</span> <span class="o">-</span> <span class="n">loss</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span> <span class="o">-</span> <span class="n">eps</span> <span class="o">/</span> <span class="mf">2.</span><span class="p">))</span> <span class="o">/</span> <span class="n">eps</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;b_grad_numerical&#39;</span><span class="p">,</span> <span class="n">b_grad_numerical</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;b_grad_autodiff&#39;</span><span class="p">,</span> <span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="mi">1</span><span class="p">)(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>

<span class="c1"># Check W_grad with finite differences in a random direction</span>
<span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
<span class="n">vec</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">subkey</span><span class="p">,</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">unitvec</span> <span class="o">=</span> <span class="n">vec</span> <span class="o">/</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">vdot</span><span class="p">(</span><span class="n">vec</span><span class="p">,</span> <span class="n">vec</span><span class="p">))</span>
<span class="n">W_grad_numerical</span> <span class="o">=</span> <span class="p">(</span><span class="n">loss</span><span class="p">(</span><span class="n">W</span> <span class="o">+</span> <span class="n">eps</span> <span class="o">/</span> <span class="mf">2.</span> <span class="o">*</span> <span class="n">unitvec</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">-</span> <span class="n">loss</span><span class="p">(</span><span class="n">W</span> <span class="o">-</span> <span class="n">eps</span> <span class="o">/</span> <span class="mf">2.</span> <span class="o">*</span> <span class="n">unitvec</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span> <span class="o">/</span> <span class="n">eps</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;W_dirderiv_numerical&#39;</span><span class="p">,</span> <span class="n">W_grad_numerical</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;W_dirderiv_autodiff&#39;</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">vdot</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">loss</span><span class="p">)(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">unitvec</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>b_grad_numerical -0.6890297
b_grad_autodiff -0.6900178
W_dirderiv_numerical 1.3017654
W_dirderiv_autodiff 1.3006744
</pre></div>
</div>
</div>
</div>
<p>JAX provides a simple convenience function that does essentially the same thing, but checks up to any order of differentiation that you like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax.test_util</span> <span class="kn">import</span> <span class="n">check_grads</span>
<span class="n">check_grads</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">),</span> <span class="n">order</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># check up to 2nd order derivatives</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="hessian-vector-products-with-grad-of-grad">
<h3>Hessian-vector products with <code class="docutils literal notranslate"><span class="pre">grad</span></code>-of-<code class="docutils literal notranslate"><span class="pre">grad</span></code><a class="headerlink" href="#hessian-vector-products-with-grad-of-grad" title="Link to this heading">#</a></h3>
<p>One thing we can do with higher-order <code class="docutils literal notranslate"><span class="pre">grad</span></code> is build a Hessian-vector product function. (Later on we’ll write an even more efficient implementation that mixes both forward- and reverse-mode, but this one will use pure reverse-mode.)</p>
<p>A Hessian-vector product function can be useful in a <a class="reference external" href="https://en.wikipedia.org/wiki/Truncated_Newton_method">truncated Newton Conjugate-Gradient algorithm</a> for minimizing smooth convex functions, or for studying the curvature of neural network training objectives (e.g. <a class="reference external" href="https://arxiv.org/abs/1406.2572">1</a>, <a class="reference external" href="https://arxiv.org/abs/1811.07062">2</a>, <a class="reference external" href="https://arxiv.org/abs/1706.04454">3</a>, <a class="reference external" href="https://arxiv.org/abs/1802.03451">4</a>).</p>
<p>For a scalar-valued function <span class="math notranslate nohighlight">\(f : \mathbb{R}^n \to \mathbb{R}\)</span> with continuous second derivatives (so that the Hessian matrix is symmetric), the Hessian at a point <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n\)</span> is written as <span class="math notranslate nohighlight">\(\partial^2 f(x)\)</span>. A Hessian-vector product function is then able to evaluate</p>
<p><span class="math notranslate nohighlight">\(\qquad v \mapsto \partial^2 f(x) \cdot v\)</span></p>
<p>for any <span class="math notranslate nohighlight">\(v \in \mathbb{R}^n\)</span>.</p>
<p>The trick is not to instantiate the full Hessian matrix: if <span class="math notranslate nohighlight">\(n\)</span> is large, perhaps in the millions or billions in the context of neural networks, then that might be impossible to store.</p>
<p>Luckily, <code class="docutils literal notranslate"><span class="pre">grad</span></code> already gives us a way to write an efficient Hessian-vector product function. We just have to use the identity</p>
<p><span class="math notranslate nohighlight">\(\qquad \partial^2 f (x) v = \partial [x \mapsto \partial f(x) \cdot v] = \partial g(x)\)</span>,</p>
<p>where <span class="math notranslate nohighlight">\(g(x) = \partial f(x) \cdot v\)</span> is a new scalar-valued function that dots the gradient of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x\)</span> with the vector <span class="math notranslate nohighlight">\(v\)</span>. Notice that we’re only ever differentiating scalar-valued functions of vector-valued arguments, which is exactly where we know <code class="docutils literal notranslate"><span class="pre">grad</span></code> is efficient.</p>
<p>In JAX code, we can just write this:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">hvp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">grad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">vdot</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">x</span><span class="p">),</span> <span class="n">v</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>This example shows that you can freely use lexical closure, and JAX will never get perturbed or confused.</p>
<p>We’ll check this implementation a few cells down, once we see how to compute dense Hessian matrices. We’ll also write an even better version that uses both forward-mode and reverse-mode.</p>
</section>
<section id="jacobians-and-hessians-using-jacfwd-and-jacrev">
<h3>Jacobians and Hessians using <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code> and <code class="docutils literal notranslate"><span class="pre">jacrev</span></code><a class="headerlink" href="#jacobians-and-hessians-using-jacfwd-and-jacrev" title="Link to this heading">#</a></h3>
<p>You can compute full Jacobian matrices using the <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code> and <code class="docutils literal notranslate"><span class="pre">jacrev</span></code> functions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">jacfwd</span><span class="p">,</span> <span class="n">jacrev</span>

<span class="c1"># Isolate the function from the weight matrix to the predictions</span>
<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">W</span><span class="p">:</span> <span class="n">predict</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

<span class="n">J</span> <span class="o">=</span> <span class="n">jacfwd</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">W</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;jacfwd result, with shape&quot;</span><span class="p">,</span> <span class="n">J</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">J</span><span class="p">)</span>

<span class="n">J</span> <span class="o">=</span> <span class="n">jacrev</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">W</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;jacrev result, with shape&quot;</span><span class="p">,</span> <span class="n">J</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">J</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>jacfwd result, with shape (4, 3)
[[ 0.05069415  0.1091874   0.07506633]
 [ 0.14170025 -0.17390487  0.02415345]
 [ 0.12579198  0.01451446 -0.31447992]
 [ 0.00574409 -0.0193281   0.01078958]]
jacrev result, with shape (4, 3)
[[ 0.05069415  0.10918741  0.07506634]
 [ 0.14170025 -0.17390487  0.02415345]
 [ 0.12579198  0.01451446 -0.31447995]
 [ 0.00574409 -0.0193281   0.01078958]]
</pre></div>
</div>
</div>
</div>
<p>These two functions compute the same values (up to machine numerics), but differ in their implementation: <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code> uses forward-mode automatic differentiation, which is more efficient for “tall” Jacobian matrices (more outputs than inputs), while <code class="docutils literal notranslate"><span class="pre">jacrev</span></code> uses reverse-mode, which is more efficient for “wide” Jacobian matrices (more inputs than outputs). For matrices that are near-square, <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code> probably has an edge over <code class="docutils literal notranslate"><span class="pre">jacrev</span></code>.</p>
<p>You can also use <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code> and <code class="docutils literal notranslate"><span class="pre">jacrev</span></code> with container types:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">predict_dict</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">predict</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="s1">&#39;W&#39;</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">],</span> <span class="n">inputs</span><span class="p">)</span>

<span class="n">J_dict</span> <span class="o">=</span> <span class="n">jacrev</span><span class="p">(</span><span class="n">predict_dict</span><span class="p">)({</span><span class="s1">&#39;W&#39;</span><span class="p">:</span> <span class="n">W</span><span class="p">,</span> <span class="s1">&#39;b&#39;</span><span class="p">:</span> <span class="n">b</span><span class="p">},</span> <span class="n">inputs</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">J_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Jacobian from </span><span class="si">{}</span><span class="s2"> to logits is&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Jacobian from W to logits is
[[ 0.05069415  0.10918741  0.07506634]
 [ 0.14170025 -0.17390487  0.02415345]
 [ 0.12579198  0.01451446 -0.31447995]
 [ 0.00574409 -0.0193281   0.01078958]]
Jacobian from b to logits is
[0.09748876 0.16102302 0.24190766 0.00776229]
</pre></div>
</div>
</div>
</div>
<p>For more details on forward- and reverse-mode, as well as how to implement <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code> and <code class="docutils literal notranslate"><span class="pre">jacrev</span></code> as efficiently as possible, read on!</p>
<p>Using a composition of two of these functions gives us a way to compute dense Hessian matrices:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">hessian</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jacfwd</span><span class="p">(</span><span class="n">jacrev</span><span class="p">(</span><span class="n">f</span><span class="p">))</span>

<span class="n">H</span> <span class="o">=</span> <span class="n">hessian</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">W</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;hessian, with shape&quot;</span><span class="p">,</span> <span class="n">H</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>hessian, with shape (4, 3, 3)
[[[ 0.02058932  0.04434624  0.03048803]
  [ 0.04434623  0.09551499  0.06566654]
  [ 0.03048803  0.06566655  0.04514575]]

 [[-0.0743913   0.09129842 -0.01268033]
  [ 0.09129842 -0.11204806  0.01556223]
  [-0.01268034  0.01556223 -0.00216142]]

 [[ 0.01176856  0.00135791 -0.02942139]
  [ 0.00135791  0.00015668 -0.00339478]
  [-0.0294214  -0.00339478  0.07355348]]

 [[-0.00418412  0.014079   -0.00785936]
  [ 0.014079   -0.04737393  0.02644569]
  [-0.00785936  0.02644569 -0.01476286]]]
</pre></div>
</div>
</div>
</div>
<p>This shape makes sense: if we start with a function <span class="math notranslate nohighlight">\(f : \mathbb{R}^n \to \mathbb{R}^m\)</span>, then at a point <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n\)</span> we expect to get the shapes</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f(x) \in \mathbb{R}^m\)</span>, the value of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\partial f(x) \in \mathbb{R}^{m \times n}\)</span>, the Jacobian matrix at <span class="math notranslate nohighlight">\(x\)</span>,</p></li>
<li><p><span class="math notranslate nohighlight">\(\partial^2 f(x) \in \mathbb{R}^{m \times n \times n}\)</span>, the Hessian at <span class="math notranslate nohighlight">\(x\)</span>,</p></li>
</ul>
<p>and so on.</p>
<p>To implement <code class="docutils literal notranslate"><span class="pre">hessian</span></code>, we could have used <code class="docutils literal notranslate"><span class="pre">jacfwd(jacrev(f))</span></code> or <code class="docutils literal notranslate"><span class="pre">jacrev(jacfwd(f))</span></code> or any other composition of the two. But forward-over-reverse is typically the most efficient. That’s because in the inner Jacobian computation we’re often differentiating a function wide Jacobian (maybe like a loss function <span class="math notranslate nohighlight">\(f : \mathbb{R}^n \to \mathbb{R}\)</span>), while in the outer Jacobian computation we’re differentiating a function with a square Jacobian (since <span class="math notranslate nohighlight">\(\nabla f : \mathbb{R}^n \to \mathbb{R}^n\)</span>), which is where forward-mode wins out.</p>
</section>
</section>
<section id="how-it-s-made-two-foundational-autodiff-functions">
<h2>How it’s made: two foundational autodiff functions<a class="headerlink" href="#how-it-s-made-two-foundational-autodiff-functions" title="Link to this heading">#</a></h2>
<section id="jacobian-vector-products-jvps-aka-forward-mode-autodiff">
<span id="jacobian-vector-product"></span><h3>Jacobian-Vector products (JVPs, aka forward-mode autodiff)<a class="headerlink" href="#jacobian-vector-products-jvps-aka-forward-mode-autodiff" title="Link to this heading">#</a></h3>
<p>JAX includes efficient and general implementations of both forward- and reverse-mode automatic differentiation. The familiar <code class="docutils literal notranslate"><span class="pre">grad</span></code> function is built on reverse-mode, but to explain the difference in the two modes, and when each can be useful, we need a bit of math background.</p>
<section id="jvps-in-math">
<h4>JVPs in math<a class="headerlink" href="#jvps-in-math" title="Link to this heading">#</a></h4>
<p>Mathematically, given a function <span class="math notranslate nohighlight">\(f : \mathbb{R}^n \to \mathbb{R}^m\)</span>, the Jacobian of <span class="math notranslate nohighlight">\(f\)</span> evaluated at an input point <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n\)</span>, denoted <span class="math notranslate nohighlight">\(\partial f(x)\)</span>, is often thought of as a matrix in <span class="math notranslate nohighlight">\(\mathbb{R}^m \times \mathbb{R}^n\)</span>:</p>
<p><span class="math notranslate nohighlight">\(\qquad \partial f(x) \in \mathbb{R}^{m \times n}\)</span>.</p>
<p>But we can also think of <span class="math notranslate nohighlight">\(\partial f(x)\)</span> as a linear map, which maps the tangent space of the domain of <span class="math notranslate nohighlight">\(f\)</span> at the point <span class="math notranslate nohighlight">\(x\)</span> (which is just another copy of <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span>) to the tangent space of the codomain of <span class="math notranslate nohighlight">\(f\)</span> at the point <span class="math notranslate nohighlight">\(f(x)\)</span> (a copy of <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span>):</p>
<p><span class="math notranslate nohighlight">\(\qquad \partial f(x) : \mathbb{R}^n \to \mathbb{R}^m\)</span>.</p>
<p>This map is called the <a class="reference external" href="https://en.wikipedia.org/wiki/Pushforward_(differential)">pushforward map</a> of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x\)</span>. The Jacobian matrix is just the matrix for this linear map in a standard basis.</p>
<p>If we don’t commit to one specific input point <span class="math notranslate nohighlight">\(x\)</span>, then we can think of the function <span class="math notranslate nohighlight">\(\partial f\)</span> as first taking an input point and returning the Jacobian linear map at that input point:</p>
<p><span class="math notranslate nohighlight">\(\qquad \partial f : \mathbb{R}^n \to \mathbb{R}^n \to \mathbb{R}^m\)</span>.</p>
<p>In particular, we can uncurry things so that given input point <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n\)</span> and a tangent vector <span class="math notranslate nohighlight">\(v \in \mathbb{R}^n\)</span>, we get back an output tangent vector in <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span>. We call that mapping, from <span class="math notranslate nohighlight">\((x, v)\)</span> pairs to output tangent vectors, the <em>Jacobian-vector product</em>, and write it as</p>
<p><span class="math notranslate nohighlight">\(\qquad (x, v) \mapsto \partial f(x) v\)</span></p>
</section>
<section id="jvps-in-jax-code">
<h4>JVPs in JAX code<a class="headerlink" href="#jvps-in-jax-code" title="Link to this heading">#</a></h4>
<p>Back in Python code, JAX’s <code class="docutils literal notranslate"><span class="pre">jvp</span></code> function models this transformation. Given a Python function that evaluates <span class="math notranslate nohighlight">\(f\)</span>, JAX’s <code class="docutils literal notranslate"><span class="pre">jvp</span></code> is a way to get a Python function for evaluating <span class="math notranslate nohighlight">\((x, v) \mapsto (f(x), \partial f(x) v)\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">jvp</span>

<span class="c1"># Isolate the function from the weight matrix to the predictions</span>
<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">W</span><span class="p">:</span> <span class="n">predict</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

<span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">subkey</span><span class="p">,</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Push forward the vector `v` along `f` evaluated at `W`</span>
<span class="n">y</span><span class="p">,</span> <span class="n">u</span> <span class="o">=</span> <span class="n">jvp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">W</span><span class="p">,),</span> <span class="p">(</span><span class="n">v</span><span class="p">,))</span>
</pre></div>
</div>
</div>
</div>
<p>In terms of <a class="reference external" href="https://wiki.haskell.org/Type_signature">Haskell-like type signatures</a>,
we could write</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">jvp</span><span class="w"> </span><span class="ow">::</span><span class="w"> </span><span class="p">(</span><span class="n">a</span><span class="w"> </span><span class="ow">-&gt;</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="ow">-&gt;</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="ow">-&gt;</span><span class="w"> </span><span class="kt">T</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="ow">-&gt;</span><span class="w"> </span><span class="p">(</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="kt">T</span><span class="w"> </span><span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
<p>where we use <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">a</span></code> to denote the type of the tangent space for <code class="docutils literal notranslate"><span class="pre">a</span></code>. In words, <code class="docutils literal notranslate"><span class="pre">jvp</span></code> takes as arguments a function of type <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">-&gt;</span> <span class="pre">b</span></code>, a value of type <code class="docutils literal notranslate"><span class="pre">a</span></code>, and a tangent vector value of type <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">a</span></code>. It gives back a pair consisting of a value of type <code class="docutils literal notranslate"><span class="pre">b</span></code> and an output tangent vector of type <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">b</span></code>.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">jvp</span></code>-transformed function is evaluated much like the original function, but paired up with each primal value of type <code class="docutils literal notranslate"><span class="pre">a</span></code> it pushes along tangent values of type <code class="docutils literal notranslate"><span class="pre">T</span> <span class="pre">a</span></code>. For each primitive numerical operation that the original function would have applied, the <code class="docutils literal notranslate"><span class="pre">jvp</span></code>-transformed function executes a “JVP rule” for that primitive that both evaluates the primitive on the primals and applies the primitive’s JVP at those primal values.</p>
<p>That evaluation strategy has some immediate implications about computational complexity: since we evaluate JVPs as we go, we don’t need to store anything for later, and so the memory cost is independent of the depth of the computation. In addition, the FLOP cost of the <code class="docutils literal notranslate"><span class="pre">jvp</span></code>-transformed function is about 3x the cost of just evaluating the function (one unit of work for evaluating the original function, for example <code class="docutils literal notranslate"><span class="pre">sin(x)</span></code>; one unit for linearizing, like <code class="docutils literal notranslate"><span class="pre">cos(x)</span></code>; and one unit for applying the linearized function to a vector, like <code class="docutils literal notranslate"><span class="pre">cos_x</span> <span class="pre">*</span> <span class="pre">v</span></code>). Put another way, for a fixed primal point <span class="math notranslate nohighlight">\(x\)</span>, we can evaluate <span class="math notranslate nohighlight">\(v \mapsto \partial f(x) \cdot v\)</span> for about the same marginal cost as evaluating <span class="math notranslate nohighlight">\(f\)</span>.</p>
<p>That memory complexity sounds pretty compelling! So why don’t we see forward-mode very often in machine learning?</p>
<p>To answer that, first think about how you could use a JVP to build a full Jacobian matrix. If we apply a JVP to a one-hot tangent vector, it reveals one column of the Jacobian matrix, corresponding to the nonzero entry we fed in. So we can build a full Jacobian one column at a time, and to get each column costs about the same as one function evaluation. That will be efficient for functions with “tall” Jacobians, but inefficient for “wide” Jacobians.</p>
<p>If you’re doing gradient-based optimization in machine learning, you probably want to minimize a loss function from parameters in <span class="math notranslate nohighlight">\(\mathbb{R}^n\)</span> to a scalar loss value in <span class="math notranslate nohighlight">\(\mathbb{R}\)</span>. That means the Jacobian of this function is a very wide matrix: <span class="math notranslate nohighlight">\(\partial f(x) \in \mathbb{R}^{1 \times n}\)</span>, which we often identify with the Gradient vector <span class="math notranslate nohighlight">\(\nabla f(x) \in \mathbb{R}^n\)</span>. Building that matrix one column at a time, with each call taking a similar number of FLOPs to evaluate the original function, sure seems inefficient! In particular, for training neural networks, where <span class="math notranslate nohighlight">\(f\)</span> is a training loss function and <span class="math notranslate nohighlight">\(n\)</span> can be in the millions or billions, this approach just won’t scale.</p>
<p>To do better for functions like this, we just need to use reverse-mode.</p>
</section>
</section>
<section id="vector-jacobian-products-vjps-aka-reverse-mode-autodiff">
<span id="vector-jacobian-product"></span><h3>Vector-Jacobian products (VJPs, aka reverse-mode autodiff)<a class="headerlink" href="#vector-jacobian-products-vjps-aka-reverse-mode-autodiff" title="Link to this heading">#</a></h3>
<p>Where forward-mode gives us back a function for evaluating Jacobian-vector products, which we can then use to build Jacobian matrices one column at a time, reverse-mode is a way to get back a function for evaluating vector-Jacobian products (equivalently Jacobian-transpose-vector products), which we can use to build Jacobian matrices one row at a time.</p>
<section id="vjps-in-math">
<h4>VJPs in math<a class="headerlink" href="#vjps-in-math" title="Link to this heading">#</a></h4>
<p>Let’s again consider a function <span class="math notranslate nohighlight">\(f : \mathbb{R}^n \to \mathbb{R}^m\)</span>.
Starting from our notation for JVPs, the notation for VJPs is pretty simple:</p>
<p><span class="math notranslate nohighlight">\(\qquad (x, v) \mapsto v \partial f(x)\)</span>,</p>
<p>where <span class="math notranslate nohighlight">\(v\)</span> is an element of the cotangent space of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x\)</span> (isomorphic to another copy of <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span>). When being rigorous, we should think of <span class="math notranslate nohighlight">\(v\)</span> as a linear map <span class="math notranslate nohighlight">\(v : \mathbb{R}^m \to \mathbb{R}\)</span>, and when we write <span class="math notranslate nohighlight">\(v \partial f(x)\)</span> we mean function composition <span class="math notranslate nohighlight">\(v \circ \partial f(x)\)</span>, where the types work out because <span class="math notranslate nohighlight">\(\partial f(x) : \mathbb{R}^n \to \mathbb{R}^m\)</span>. But in the common case we can identify <span class="math notranslate nohighlight">\(v\)</span> with a vector in <span class="math notranslate nohighlight">\(\mathbb{R}^m\)</span> and use the two almost interchangeably, just like we might sometimes flip between “column vectors” and “row vectors” without much comment.</p>
<p>With that identification, we can alternatively think of the linear part of a VJP as the transpose (or adjoint conjugate) of the linear part of a JVP:</p>
<p><span class="math notranslate nohighlight">\(\qquad (x, v) \mapsto \partial f(x)^\mathsf{T} v\)</span>.</p>
<p>For a given point <span class="math notranslate nohighlight">\(x\)</span>, we can write the signature as</p>
<p><span class="math notranslate nohighlight">\(\qquad \partial f(x)^\mathsf{T} : \mathbb{R}^m \to \mathbb{R}^n\)</span>.</p>
<p>The corresponding map on cotangent spaces is often called the <a class="reference external" href="https://en.wikipedia.org/wiki/Pullback_(differential_geometry)">pullback</a>
of <span class="math notranslate nohighlight">\(f\)</span> at <span class="math notranslate nohighlight">\(x\)</span>. The key for our purposes is that it goes from something that looks like the output of <span class="math notranslate nohighlight">\(f\)</span> to something that looks like the input of <span class="math notranslate nohighlight">\(f\)</span>, just like we might expect from a transposed linear function.</p>
</section>
<section id="vjps-in-jax-code">
<h4>VJPs in JAX code<a class="headerlink" href="#vjps-in-jax-code" title="Link to this heading">#</a></h4>
<p>Switching from math back to Python, the JAX function <code class="docutils literal notranslate"><span class="pre">vjp</span></code> can take a Python function for evaluating <span class="math notranslate nohighlight">\(f\)</span> and give us back a Python function for evaluating the VJP <span class="math notranslate nohighlight">\((x, v) \mapsto (f(x), v^\mathsf{T} \partial f(x))\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">vjp</span>

<span class="c1"># Isolate the function from the weight matrix to the predictions</span>
<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">W</span><span class="p">:</span> <span class="n">predict</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

<span class="n">y</span><span class="p">,</span> <span class="n">vjp_fun</span> <span class="o">=</span> <span class="n">vjp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>

<span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">subkey</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Pull back the covector `u` along `f` evaluated at `W`</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">vjp_fun</span><span class="p">(</span><span class="n">u</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>In terms of <a class="reference external" href="https://wiki.haskell.org/Type_signature">Haskell-like type signatures</a>,
we could write</p>
<div class="highlight-haskell notranslate"><div class="highlight"><pre><span></span><span class="nf">vjp</span><span class="w"> </span><span class="ow">::</span><span class="w"> </span><span class="p">(</span><span class="n">a</span><span class="w"> </span><span class="ow">-&gt;</span><span class="w"> </span><span class="n">b</span><span class="p">)</span><span class="w"> </span><span class="ow">-&gt;</span><span class="w"> </span><span class="n">a</span><span class="w"> </span><span class="ow">-&gt;</span><span class="w"> </span><span class="p">(</span><span class="n">b</span><span class="p">,</span><span class="w"> </span><span class="kt">CT</span><span class="w"> </span><span class="n">b</span><span class="w"> </span><span class="ow">-&gt;</span><span class="w"> </span><span class="kt">CT</span><span class="w"> </span><span class="n">a</span><span class="p">)</span>
</pre></div>
</div>
<p>where we use <code class="docutils literal notranslate"><span class="pre">CT</span> <span class="pre">a</span></code> to denote the type for the cotangent space for <code class="docutils literal notranslate"><span class="pre">a</span></code>. In words, <code class="docutils literal notranslate"><span class="pre">vjp</span></code> takes as arguments a function of type <code class="docutils literal notranslate"><span class="pre">a</span> <span class="pre">-&gt;</span> <span class="pre">b</span></code> and a point of type <code class="docutils literal notranslate"><span class="pre">a</span></code>, and gives back a pair consisting of a value of type <code class="docutils literal notranslate"><span class="pre">b</span></code> and a linear map of type <code class="docutils literal notranslate"><span class="pre">CT</span> <span class="pre">b</span> <span class="pre">-&gt;</span> <span class="pre">CT</span> <span class="pre">a</span></code>.</p>
<p>This is great because it lets us build Jacobian matrices one row at a time, and the FLOP cost for evaluating <span class="math notranslate nohighlight">\((x, v) \mapsto (f(x), v^\mathsf{T} \partial f(x))\)</span> is only about three times the cost of evaluating <span class="math notranslate nohighlight">\(f\)</span>. In particular, if we want the gradient of a function <span class="math notranslate nohighlight">\(f : \mathbb{R}^n \to \mathbb{R}\)</span>, we can do it in just one call. That’s how <code class="docutils literal notranslate"><span class="pre">grad</span></code> is efficient for gradient-based optimization, even for objectives like neural network training loss functions on millions or billions of parameters.</p>
<p>There’s a cost, though: though the FLOPs are friendly, memory scales with the depth of the computation. Also, the implementation is traditionally more complex than that of forward-mode, though JAX has some tricks up its sleeve (that’s a story for a future notebook!).</p>
<p>For more on how reverse-mode works, see <a class="reference external" href="http://videolectures.net/deeplearning2017_johnson_automatic_differentiation/">this tutorial video from the Deep Learning Summer School in 2017</a>.</p>
</section>
</section>
<section id="vector-valued-gradients-with-vjps">
<h3>Vector-valued gradients with VJPs<a class="headerlink" href="#vector-valued-gradients-with-vjps" title="Link to this heading">#</a></h3>
<p>If you’re interested in taking vector-valued gradients (like <code class="docutils literal notranslate"><span class="pre">tf.gradients</span></code>):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">vjp</span>

<span class="k">def</span> <span class="nf">vgrad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
  <span class="n">y</span><span class="p">,</span> <span class="n">vjp_fn</span> <span class="o">=</span> <span class="n">vjp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">vjp_fn</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">))[</span><span class="mi">0</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">vgrad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mi">3</span><span class="o">*</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[[6. 6.]
 [6. 6.]]
</pre></div>
</div>
</div>
</div>
</section>
<section id="hessian-vector-products-using-both-forward-and-reverse-mode">
<h3>Hessian-vector products using both forward- and reverse-mode<a class="headerlink" href="#hessian-vector-products-using-both-forward-and-reverse-mode" title="Link to this heading">#</a></h3>
<p>In a previous section, we implemented a Hessian-vector product function just using reverse-mode (assuming continuous second derivatives):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">hvp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">grad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">vdot</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">x</span><span class="p">),</span> <span class="n">v</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>That’s efficient, but we can do even better and save some memory by using forward-mode together with reverse-mode.</p>
<p>Mathematically, given a function <span class="math notranslate nohighlight">\(f : \mathbb{R}^n \to \mathbb{R}\)</span> to differentiate, a point <span class="math notranslate nohighlight">\(x \in \mathbb{R}^n\)</span> at which to linearize the function, and a vector <span class="math notranslate nohighlight">\(v \in \mathbb{R}^n\)</span>, the Hessian-vector product function we want is</p>
<p><span class="math notranslate nohighlight">\((x, v) \mapsto \partial^2 f(x) v\)</span></p>
<p>Consider the helper function <span class="math notranslate nohighlight">\(g : \mathbb{R}^n \to \mathbb{R}^n\)</span> defined to be the derivative (or gradient) of <span class="math notranslate nohighlight">\(f\)</span>, namely <span class="math notranslate nohighlight">\(g(x) = \partial f(x)\)</span>. All we need is its JVP, since that will give us</p>
<p><span class="math notranslate nohighlight">\((x, v) \mapsto \partial g(x) v = \partial^2 f(x) v\)</span>.</p>
<p>We can translate that almost directly into code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">jvp</span><span class="p">,</span> <span class="n">grad</span>

<span class="c1"># forward-over-reverse</span>
<span class="k">def</span> <span class="nf">hvp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">jvp</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">),</span> <span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Even better, since we didn’t have to call <code class="docutils literal notranslate"><span class="pre">jnp.dot</span></code> directly, this <code class="docutils literal notranslate"><span class="pre">hvp</span></code> function works with arrays of any shape and with arbitrary container types (like vectors stored as nested lists/dicts/tuples), and doesn’t even have a dependence on <code class="docutils literal notranslate"><span class="pre">jax.numpy</span></code>.</p>
<p>Here’s an example of how to use it:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">key</span><span class="p">,</span> <span class="n">subkey1</span><span class="p">,</span> <span class="n">subkey2</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">subkey1</span><span class="p">,</span> <span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">))</span>
<span class="n">V</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">subkey2</span><span class="p">,</span> <span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">40</span><span class="p">))</span>

<span class="n">ans1</span> <span class="o">=</span> <span class="n">hvp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">X</span><span class="p">,),</span> <span class="p">(</span><span class="n">V</span><span class="p">,))</span>
<span class="n">ans2</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">hessian</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">X</span><span class="p">),</span> <span class="n">V</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">ans1</span><span class="p">,</span> <span class="n">ans2</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
</pre></div>
</div>
</div>
</div>
<p>Another way you might consider writing this is using reverse-over-forward:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># reverse-over-forward</span>
<span class="k">def</span> <span class="nf">hvp_revfwd</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">):</span>
  <span class="n">g</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">primals</span><span class="p">:</span> <span class="n">jvp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">grad</span><span class="p">(</span><span class="n">g</span><span class="p">)(</span><span class="n">primals</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>That’s not quite as good, though, because forward-mode has less overhead than reverse-mode, and since the outer differentiation operator here has to differentiate a larger computation than the inner one, keeping forward-mode on the outside works best:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># reverse-over-reverse, only works for single arguments</span>
<span class="k">def</span> <span class="nf">hvp_revrev</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">primals</span><span class="p">,</span> <span class="n">tangents</span><span class="p">):</span>
  <span class="n">x</span><span class="p">,</span> <span class="o">=</span> <span class="n">primals</span>
  <span class="n">v</span><span class="p">,</span> <span class="o">=</span> <span class="n">tangents</span>
  <span class="k">return</span> <span class="n">grad</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">vdot</span><span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">x</span><span class="p">),</span> <span class="n">v</span><span class="p">))(</span><span class="n">x</span><span class="p">)</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Forward over reverse&quot;</span><span class="p">)</span>
<span class="o">%</span><span class="k">timeit</span> -n10 -r3 hvp(f, (X,), (V,))
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reverse over forward&quot;</span><span class="p">)</span>
<span class="o">%</span><span class="k">timeit</span> -n10 -r3 hvp_revfwd(f, (X,), (V,))
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reverse over reverse&quot;</span><span class="p">)</span>
<span class="o">%</span><span class="k">timeit</span> -n10 -r3 hvp_revrev(f, (X,), (V,))

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Naive full Hessian materialization&quot;</span><span class="p">)</span>
<span class="o">%</span><span class="k">timeit</span> -n10 -r3 jnp.tensordot(hessian(f)(X), V, 2)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Forward over reverse
3.07 ms ± 99.9 μs per loop (mean ± std. dev. of 3 runs, 10 loops each)
Reverse over forward
The slowest run took 4.09 times longer than the fastest. This could mean that an intermediate result is being cached.
9.89 ms ± 7.08 ms per loop (mean ± std. dev. of 3 runs, 10 loops each)
Reverse over reverse
13 ms ± 5.44 ms per loop (mean ± std. dev. of 3 runs, 10 loops each)
Naive full Hessian materialization
31 ms ± 2.01 ms per loop (mean ± std. dev. of 3 runs, 10 loops each)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="composing-vjps-jvps-and-vmap">
<h2>Composing VJPs, JVPs, and <code class="docutils literal notranslate"><span class="pre">vmap</span></code><a class="headerlink" href="#composing-vjps-jvps-and-vmap" title="Link to this heading">#</a></h2>
<section id="jacobian-matrix-and-matrix-jacobian-products">
<h3>Jacobian-Matrix and Matrix-Jacobian products<a class="headerlink" href="#jacobian-matrix-and-matrix-jacobian-products" title="Link to this heading">#</a></h3>
<p>Now that we have <code class="docutils literal notranslate"><span class="pre">jvp</span></code> and <code class="docutils literal notranslate"><span class="pre">vjp</span></code> transformations that give us functions to push-forward or pull-back single vectors at a time, we can use JAX’s <code class="docutils literal notranslate"><span class="pre">vmap</span></code> <a class="reference external" href="https://github.com/jax-ml/jax#auto-vectorization-with-vmap">transformation</a> to push and pull entire bases at once. In particular, we can use that to write fast matrix-Jacobian and Jacobian-matrix products.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Isolate the function from the weight matrix to the predictions</span>
<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">W</span><span class="p">:</span> <span class="n">predict</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>

<span class="c1"># Pull back the covectors `m_i` along `f`, evaluated at `W`, for all `i`.</span>
<span class="c1"># First, use a list comprehension to loop over rows in the matrix M.</span>
<span class="k">def</span> <span class="nf">loop_mjp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">M</span><span class="p">):</span>
    <span class="n">y</span><span class="p">,</span> <span class="n">vjp_fun</span> <span class="o">=</span> <span class="n">vjp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">vjp_fun</span><span class="p">(</span><span class="n">mi</span><span class="p">)</span> <span class="k">for</span> <span class="n">mi</span> <span class="ow">in</span> <span class="n">M</span><span class="p">])</span>

<span class="c1"># Now, use vmap to build a computation that does a single fast matrix-matrix</span>
<span class="c1"># multiply, rather than an outer loop over vector-matrix multiplies.</span>
<span class="k">def</span> <span class="nf">vmap_mjp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">M</span><span class="p">):</span>
    <span class="n">y</span><span class="p">,</span> <span class="n">vjp_fun</span> <span class="o">=</span> <span class="n">vjp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">outs</span><span class="p">,</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">vjp_fun</span><span class="p">)(</span><span class="n">M</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">outs</span>

<span class="n">key</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">num_covecs</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">U</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">num_covecs</span><span class="p">,)</span> <span class="o">+</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">loop_vs</span> <span class="o">=</span> <span class="n">loop_mjp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">U</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Non-vmapped Matrix-Jacobian product&#39;</span><span class="p">)</span>
<span class="o">%</span><span class="k">timeit</span> -n10 -r3 loop_mjp(f, W, M=U)

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Vmapped Matrix-Jacobian product&#39;</span><span class="p">)</span>
<span class="n">vmap_vs</span> <span class="o">=</span> <span class="n">vmap_mjp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">U</span><span class="p">)</span>
<span class="o">%</span><span class="k">timeit</span> -n10 -r3 vmap_mjp(f, W, M=U)

<span class="k">assert</span> <span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">loop_vs</span><span class="p">,</span> <span class="n">vmap_vs</span><span class="p">),</span> <span class="s1">&#39;Vmap and non-vmapped Matrix-Jacobian Products should be identical&#39;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Non-vmapped Matrix-Jacobian product
43.6 ms ± 1.99 ms per loop (mean ± std. dev. of 3 runs, 10 loops each)

Vmapped Matrix-Jacobian product
3.34 ms ± 61.5 μs per loop (mean ± std. dev. of 3 runs, 10 loops each)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_145138/3769736790.py:8: DeprecationWarning: vstack requires ndarray or scalar arguments, got &lt;class &#39;tuple&#39;&gt; at position 0. In a future JAX release this will be an error.
  return jnp.vstack([vjp_fun(mi) for mi in M])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">loop_jmp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">M</span><span class="p">):</span>
    <span class="c1"># jvp immediately returns the primal and tangent values as a tuple,</span>
    <span class="c1"># so we&#39;ll compute and select the tangents in a list comprehension</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">jvp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">W</span><span class="p">,),</span> <span class="p">(</span><span class="n">mi</span><span class="p">,))[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">mi</span> <span class="ow">in</span> <span class="n">M</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">vmap_jmp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">M</span><span class="p">):</span>
    <span class="n">_jvp</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">jvp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">W</span><span class="p">,),</span> <span class="p">(</span><span class="n">s</span><span class="p">,))[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">vmap</span><span class="p">(</span><span class="n">_jvp</span><span class="p">)(</span><span class="n">M</span><span class="p">)</span>

<span class="n">num_vecs</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">S</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="p">(</span><span class="n">num_vecs</span><span class="p">,)</span> <span class="o">+</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">loop_vs</span> <span class="o">=</span> <span class="n">loop_jmp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">S</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Non-vmapped Jacobian-Matrix product&#39;</span><span class="p">)</span>
<span class="o">%</span><span class="k">timeit</span> -n10 -r3 loop_jmp(f, W, M=S)
<span class="n">vmap_vs</span> <span class="o">=</span> <span class="n">vmap_jmp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">M</span><span class="o">=</span><span class="n">S</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">Vmapped Jacobian-Matrix product&#39;</span><span class="p">)</span>
<span class="o">%</span><span class="k">timeit</span> -n10 -r3 vmap_jmp(f, W, M=S)

<span class="k">assert</span> <span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">loop_vs</span><span class="p">,</span> <span class="n">vmap_vs</span><span class="p">),</span> <span class="s1">&#39;Vmap and non-vmapped Jacobian-Matrix products should be identical&#39;</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Non-vmapped Jacobian-Matrix product
118 ms ± 1.7 ms per loop (mean ± std. dev. of 3 runs, 10 loops each)

Vmapped Jacobian-Matrix product
1.56 ms ± 46.4 μs per loop (mean ± std. dev. of 3 runs, 10 loops each)
</pre></div>
</div>
</div>
</div>
</section>
<section id="the-implementation-of-jacfwd-and-jacrev">
<h3>The implementation of <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code> and <code class="docutils literal notranslate"><span class="pre">jacrev</span></code><a class="headerlink" href="#the-implementation-of-jacfwd-and-jacrev" title="Link to this heading">#</a></h3>
<p>Now that we’ve seen fast Jacobian-matrix and matrix-Jacobian products, it’s not hard to guess how to write <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code> and <code class="docutils literal notranslate"><span class="pre">jacrev</span></code>. We just use the same technique to push-forward or pull-back an entire standard basis (isomorphic to an identity matrix) at once.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">jacrev</span> <span class="k">as</span> <span class="n">builtin_jacrev</span>

<span class="k">def</span> <span class="nf">our_jacrev</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">jacfun</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">y</span><span class="p">,</span> <span class="n">vjp_fun</span> <span class="o">=</span> <span class="n">vjp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
        <span class="c1"># Use vmap to do a matrix-Jacobian product.</span>
        <span class="c1"># Here, the matrix is the Euclidean basis, so we get all</span>
        <span class="c1"># entries in the Jacobian at once.</span>
        <span class="n">J</span><span class="p">,</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">vjp_fun</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="mi">0</span><span class="p">)(</span><span class="n">jnp</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">J</span>
    <span class="k">return</span> <span class="n">jacfun</span>

<span class="k">assert</span> <span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">builtin_jacrev</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">W</span><span class="p">),</span> <span class="n">our_jacrev</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">W</span><span class="p">)),</span> <span class="s1">&#39;Incorrect reverse-mode Jacobian results!&#39;</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">jacfwd</span> <span class="k">as</span> <span class="n">builtin_jacfwd</span>

<span class="k">def</span> <span class="nf">our_jacfwd</span><span class="p">(</span><span class="n">f</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">jacfun</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="n">_jvp</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="n">jvp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">(</span><span class="n">x</span><span class="p">,),</span> <span class="p">(</span><span class="n">s</span><span class="p">,))[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">Jt</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">_jvp</span><span class="p">,</span> <span class="n">in_axes</span><span class="o">=</span><span class="mi">1</span><span class="p">)(</span><span class="n">jnp</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">Jt</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jacfun</span>

<span class="k">assert</span> <span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">builtin_jacfwd</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">W</span><span class="p">),</span> <span class="n">our_jacfwd</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">W</span><span class="p">)),</span> <span class="s1">&#39;Incorrect forward-mode Jacobian results!&#39;</span>
</pre></div>
</div>
</div>
</div>
<p>Interestingly, <a class="reference external" href="https://github.com/hips/autograd">Autograd</a> couldn’t do this. Our <a class="reference external" href="https://github.com/HIPS/autograd/blob/96a03f44da43cd7044c61ac945c483955deba957/autograd/differential_operators.py#L60">implementation</a> of reverse-mode <code class="docutils literal notranslate"><span class="pre">jacobian</span></code> in Autograd had to pull back one vector at a time with an outer-loop <code class="docutils literal notranslate"><span class="pre">map</span></code>. Pushing one vector at a time through the computation is much less efficient than batching it all together with <code class="docutils literal notranslate"><span class="pre">vmap</span></code>.</p>
<p>Another thing that Autograd couldn’t do is <code class="docutils literal notranslate"><span class="pre">jit</span></code>. Interestingly, no matter how much Python dynamism you use in your function to be differentiated, we could always use <code class="docutils literal notranslate"><span class="pre">jit</span></code> on the linear part of the computation. For example:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">x</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">return</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">x</span> <span class="o">**</span> <span class="mi">3</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span>
    <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">x</span>

<span class="n">y</span><span class="p">,</span> <span class="n">f_vjp</span> <span class="o">=</span> <span class="n">vjp</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="mf">4.</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">jit</span><span class="p">(</span><span class="n">f_vjp</span><span class="p">)(</span><span class="mf">1.</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(Array(3.1415927, dtype=float32, weak_type=True),)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="complex-numbers-and-differentiation">
<h2>Complex numbers and differentiation<a class="headerlink" href="#complex-numbers-and-differentiation" title="Link to this heading">#</a></h2>
<p>JAX is great at complex numbers and differentiation. To support both <a class="reference external" href="https://en.wikipedia.org/wiki/Holomorphic_function">holomorphic and non-holomorphic differentiation</a>, it helps to think in terms of JVPs and VJPs.</p>
<p>Consider a complex-to-complex function <span class="math notranslate nohighlight">\(f: \mathbb{C} \to \mathbb{C}\)</span> and identify it with a corresponding function <span class="math notranslate nohighlight">\(g: \mathbb{R}^2 \to \mathbb{R}^2\)</span>,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
  <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">imag</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">u</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">v</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span>

<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
  <span class="k">return</span> <span class="p">(</span><span class="n">u</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">),</span> <span class="n">v</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>That is, we’ve decomposed <span class="math notranslate nohighlight">\(f(z) = u(x, y) + v(x, y) i\)</span> where <span class="math notranslate nohighlight">\(z = x + y i\)</span>, and identified <span class="math notranslate nohighlight">\(\mathbb{C}\)</span> with <span class="math notranslate nohighlight">\(\mathbb{R}^2\)</span> to get <span class="math notranslate nohighlight">\(g\)</span>.</p>
<p>Since <span class="math notranslate nohighlight">\(g\)</span> only involves real inputs and outputs, we already know how to write a Jacobian-vector product for it, say given a tangent vector <span class="math notranslate nohighlight">\((c, d) \in \mathbb{R}^2\)</span>, namely</p>
<p><span class="math notranslate nohighlight">\(\begin{bmatrix} \partial_0 u(x, y) &amp; \partial_1 u(x, y) \\ \partial_0 v(x, y) &amp; \partial_1 v(x, y) \end{bmatrix}
\begin{bmatrix} c \\ d \end{bmatrix}\)</span>.</p>
<p>To get a JVP for the original function <span class="math notranslate nohighlight">\(f\)</span> applied to a tangent vector <span class="math notranslate nohighlight">\(c + di \in \mathbb{C}\)</span>, we just use the same definition and identify the result as another complex number,</p>
<p><span class="math notranslate nohighlight">\(\partial f(x + y i)(c + d i) =
\begin{matrix} \begin{bmatrix} 1 &amp; i \end{bmatrix} \\ ~ \end{matrix}
\begin{bmatrix} \partial_0 u(x, y) &amp; \partial_1 u(x, y) \\ \partial_0 v(x, y) &amp; \partial_1 v(x, y) \end{bmatrix}
\begin{bmatrix} c \\ d \end{bmatrix}\)</span>.</p>
<p>That’s our definition of the JVP of a <span class="math notranslate nohighlight">\(\mathbb{C} \to \mathbb{C}\)</span> function! Notice it doesn’t matter whether or not <span class="math notranslate nohighlight">\(f\)</span> is holomorphic: the JVP is unambiguous.</p>
<p>Here’s a check:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">check</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
  <span class="n">key</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

  <span class="c1"># random coeffs for u and v</span>
  <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
  <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">subkey</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,))</span>

  <span class="k">def</span> <span class="nf">fun</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">imag</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">u</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">v</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span>

  <span class="k">def</span> <span class="nf">u</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">y</span>

  <span class="k">def</span> <span class="nf">v</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">c</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="n">y</span>

  <span class="c1"># primal point</span>
  <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
  <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">subkey</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
  <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span>

  <span class="c1"># tangent vector</span>
  <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
  <span class="n">c</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">subkey</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
  <span class="n">z_dot</span> <span class="o">=</span> <span class="n">c</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span>

  <span class="c1"># check jvp</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">ans</span> <span class="o">=</span> <span class="n">jvp</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="p">(</span><span class="n">z</span><span class="p">,),</span> <span class="p">(</span><span class="n">z_dot</span><span class="p">,))</span>
  <span class="n">expected</span> <span class="o">=</span> <span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="mi">0</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">c</span> <span class="o">+</span>
              <span class="n">grad</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">d</span> <span class="o">+</span>
              <span class="n">grad</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="mi">0</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">c</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span><span class="o">+</span>
              <span class="n">grad</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">d</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span><span class="p">)</span>
  <span class="nb">print</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">ans</span><span class="p">,</span> <span class="n">expected</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">check</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">check</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">check</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>True
True
True
</pre></div>
</div>
</div>
</div>
<p>What about VJPs? We do something pretty similar: for a cotangent vector <span class="math notranslate nohighlight">\(c + di \in \mathbb{C}\)</span> we define the VJP of <span class="math notranslate nohighlight">\(f\)</span> as</p>
<p><span class="math notranslate nohighlight">\((c + di)^* \; \partial f(x + y i) =
\begin{matrix} \begin{bmatrix} c &amp; -d \end{bmatrix} \\ ~ \end{matrix}
\begin{bmatrix} \partial_0 u(x, y) &amp; \partial_1 u(x, y) \\ \partial_0 v(x, y) &amp; \partial_1 v(x, y) \end{bmatrix}
\begin{bmatrix} 1 \\ -i \end{bmatrix}\)</span>.</p>
<p>What’s with the negatives? They’re just to take care of complex conjugation, and the fact that we’re working with covectors.</p>
<p>Here’s a check of the VJP rules:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">check</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
  <span class="n">key</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

  <span class="c1"># random coeffs for u and v</span>
  <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
  <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">subkey</span><span class="p">,</span> <span class="p">(</span><span class="mi">4</span><span class="p">,))</span>

  <span class="k">def</span> <span class="nf">fun</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">imag</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">u</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">+</span> <span class="n">v</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span>

  <span class="k">def</span> <span class="nf">u</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">b</span> <span class="o">*</span> <span class="n">y</span>

  <span class="k">def</span> <span class="nf">v</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">c</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="n">y</span>

  <span class="c1"># primal point</span>
  <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
  <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">subkey</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
  <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span>

  <span class="c1"># cotangent vector</span>
  <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
  <span class="n">c</span><span class="p">,</span> <span class="n">d</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">subkey</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
  <span class="n">z_bar</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">c</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="mi">1</span><span class="n">j</span><span class="p">)</span>  <span class="c1"># for dtype control</span>

  <span class="c1"># check vjp</span>
  <span class="n">_</span><span class="p">,</span> <span class="n">fun_vjp</span> <span class="o">=</span> <span class="n">vjp</span><span class="p">(</span><span class="n">fun</span><span class="p">,</span> <span class="n">z</span><span class="p">)</span>
  <span class="n">ans</span><span class="p">,</span> <span class="o">=</span> <span class="n">fun_vjp</span><span class="p">(</span><span class="n">z_bar</span><span class="p">)</span>
  <span class="n">expected</span> <span class="o">=</span> <span class="p">(</span><span class="n">grad</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="mi">0</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">c</span> <span class="o">+</span>
              <span class="n">grad</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="mi">0</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">d</span><span class="p">)</span> <span class="o">+</span>
              <span class="n">grad</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="n">c</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="n">j</span><span class="p">)</span> <span class="o">+</span>
              <span class="n">grad</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">d</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="n">j</span><span class="p">))</span>
  <span class="k">assert</span> <span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">ans</span><span class="p">,</span> <span class="n">expected</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">check</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">check</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">check</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>What about convenience wrappers like <code class="docutils literal notranslate"><span class="pre">grad</span></code>, <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code>, and <code class="docutils literal notranslate"><span class="pre">jacrev</span></code>?</p>
<p>For <span class="math notranslate nohighlight">\(\mathbb{R} \to \mathbb{R}\)</span> functions, recall we defined <code class="docutils literal notranslate"><span class="pre">grad(f)(x)</span></code> as being <code class="docutils literal notranslate"><span class="pre">vjp(f,</span> <span class="pre">x)[1](1.0)</span></code>, which works because applying a VJP to a <code class="docutils literal notranslate"><span class="pre">1.0</span></code> value reveals the gradient (i.e. Jacobian, or derivative). We can do the same thing for <span class="math notranslate nohighlight">\(\mathbb{C} \to \mathbb{R}\)</span> functions: we can still use <code class="docutils literal notranslate"><span class="pre">1.0</span></code> as the cotangent vector, and we just get out a complex number result summarizing the full Jacobian:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
  <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">imag</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span>

<span class="n">z</span> <span class="o">=</span> <span class="mf">3.</span> <span class="o">+</span> <span class="mi">4</span><span class="n">j</span>
<span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">)(</span><span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Array(6.-8.j, dtype=complex64)
</pre></div>
</div>
</div>
</div>
<p>For general <span class="math notranslate nohighlight">\(\mathbb{C} \to \mathbb{C}\)</span> functions, the Jacobian has 4 real-valued degrees of freedom (as in the 2x2 Jacobian matrices above), so we can’t hope to represent all of them within a complex number. But we can for holomorphic functions! A holomorphic function is precisely a <span class="math notranslate nohighlight">\(\mathbb{C} \to \mathbb{C}\)</span> function with the special property that its derivative can be represented as a single complex number. (The <a class="reference external" href="https://en.wikipedia.org/wiki/Cauchy%E2%80%93Riemann_equations">Cauchy-Riemann equations</a> ensure that the above 2x2 Jacobians have the special form of a scale-and-rotate matrix in the complex plane, i.e. the action of a single complex number under multiplication.) And we can reveal that one complex number using a single call to <code class="docutils literal notranslate"><span class="pre">vjp</span></code> with a covector of <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p>
<p>Because this only works for holomorphic functions, to use this trick we need to promise JAX that our function is holomorphic; otherwise, JAX will raise an error when <code class="docutils literal notranslate"><span class="pre">grad</span></code> is used for a complex-output function:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="n">z</span> <span class="o">=</span> <span class="mf">3.</span> <span class="o">+</span> <span class="mi">4</span><span class="n">j</span>
<span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">holomorphic</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Array(-27.034946-3.8511534j, dtype=complex64, weak_type=True)
</pre></div>
</div>
</div>
</div>
<p>All the <code class="docutils literal notranslate"><span class="pre">holomorphic=True</span></code> promise does is disable the error when the output is complex-valued. We can still write <code class="docutils literal notranslate"><span class="pre">holomorphic=True</span></code> when the function isn’t holomorphic, but the answer we get out won’t represent the full Jacobian. Instead, it’ll be the Jacobian of the function where we just discard the imaginary part of the output:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">z</span><span class="p">):</span>
  <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">conjugate</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>

<span class="n">z</span> <span class="o">=</span> <span class="mf">3.</span> <span class="o">+</span> <span class="mi">4</span><span class="n">j</span>
<span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">holomorphic</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">z</span><span class="p">)</span>  <span class="c1"># f is not actually holomorphic!</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Array(1.-0.j, dtype=complex64, weak_type=True)
</pre></div>
</div>
</div>
</div>
<p>There are some useful upshots for how <code class="docutils literal notranslate"><span class="pre">grad</span></code> works here:</p>
<ol class="arabic simple">
<li><p>We can use <code class="docutils literal notranslate"><span class="pre">grad</span></code> on holomorphic <span class="math notranslate nohighlight">\(\mathbb{C} \to \mathbb{C}\)</span> functions.</p></li>
<li><p>We can use <code class="docutils literal notranslate"><span class="pre">grad</span></code> to optimize <span class="math notranslate nohighlight">\(f : \mathbb{C} \to \mathbb{R}\)</span> functions, like real-valued loss functions of complex parameters <code class="docutils literal notranslate"><span class="pre">x</span></code>, by taking steps in the direction of the conjugate of <code class="docutils literal notranslate"><span class="pre">grad(f)(x)</span></code>.</p></li>
<li><p>If we have an <span class="math notranslate nohighlight">\(\mathbb{R} \to \mathbb{R}\)</span> function that just happens to use some complex-valued operations internally (some of which must be non-holomorphic, e.g. FFTs used in convolutions) then <code class="docutils literal notranslate"><span class="pre">grad</span></code> still works and we get the same result that an implementation using only real values would have given.</p></li>
</ol>
<p>In any case, JVPs and VJPs are always unambiguous. And if we wanted to compute the full Jacobian matrix of a non-holomorphic <span class="math notranslate nohighlight">\(\mathbb{C} \to \mathbb{C}\)</span> function, we can do it with JVPs or VJPs!</p>
<p>You should expect complex numbers to work everywhere in JAX. Here’s differentiating through a Cholesky decomposition of a complex matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">A</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">5.</span><span class="p">,</span>    <span class="mf">2.</span><span class="o">+</span><span class="mi">3</span><span class="n">j</span><span class="p">,</span>    <span class="mi">5</span><span class="n">j</span><span class="p">],</span>
              <span class="p">[</span><span class="mf">2.</span><span class="o">-</span><span class="mi">3</span><span class="n">j</span><span class="p">,</span>   <span class="mf">7.</span><span class="p">,</span>  <span class="mf">1.</span><span class="o">+</span><span class="mi">7</span><span class="n">j</span><span class="p">],</span>
              <span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="n">j</span><span class="p">,</span>  <span class="mf">1.</span><span class="o">-</span><span class="mi">7</span><span class="n">j</span><span class="p">,</span>    <span class="mf">12.</span><span class="p">]])</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">L</span> <span class="o">-</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">L</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">grad</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">holomorphic</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">A</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Array([[-0.75341904 +0.j       , -3.0509028 -10.940545j ,
         5.9896846  +3.5423026j],
       [-3.0509028 +10.940545j , -8.904491   +0.j       ,
        -5.1351523  -6.559373j ],
       [ 5.9896846  -3.5423026j, -5.1351523  +6.559373j ,
         0.01320427 +0.j       ]], dtype=complex64)
</pre></div>
</div>
</div>
</div>
</section>
<section id="more-advanced-autodiff">
<h2>More advanced autodiff<a class="headerlink" href="#more-advanced-autodiff" title="Link to this heading">#</a></h2>
<p>In this notebook, we worked through some easy, and then progressively more complicated, applications of automatic differentiation in JAX. We hope you now feel that taking derivatives in JAX is easy and powerful.</p>
<p>There’s a whole world of other autodiff tricks and functionality out there. Topics we didn’t cover, but hope to in an “Advanced Autodiff Cookbook” include:</p>
<ul class="simple">
<li><p>Gauss-Newton Vector Products, linearizing once</p></li>
<li><p>Custom VJPs and JVPs</p></li>
<li><p>Efficient derivatives at fixed-points</p></li>
<li><p>Estimating the trace of a Hessian using random Hessian-vector products.</p></li>
<li><p>Forward-mode autodiff using only reverse-mode autodiff.</p></li>
<li><p>Taking derivatives with respect to custom data types.</p></li>
<li><p>Checkpointing (binomial checkpointing for efficient reverse-mode, not model snapshotting).</p></li>
<li><p>Optimizing VJPs with Jacobian pre-accumulation.</p></li>
</ul>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../advanced_guides.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Resources and Advanced Guides</p>
      </div>
    </a>
    <a class="right-next"
       href="Custom_derivative_rules_for_Python_code.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Custom derivative rules</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradients">Gradients</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#starting-with-grad">Starting with <code class="docutils literal notranslate"><span class="pre">grad</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#differentiating-with-respect-to-nested-lists-tuples-and-dicts">Differentiating with respect to nested lists, tuples, and dicts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluate-a-function-and-its-gradient-using-value-and-grad">Evaluate a function and its gradient using <code class="docutils literal notranslate"><span class="pre">value_and_grad</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checking-against-numerical-differences">Checking against numerical differences</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hessian-vector-products-with-grad-of-grad">Hessian-vector products with <code class="docutils literal notranslate"><span class="pre">grad</span></code>-of-<code class="docutils literal notranslate"><span class="pre">grad</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jacobians-and-hessians-using-jacfwd-and-jacrev">Jacobians and Hessians using <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code> and <code class="docutils literal notranslate"><span class="pre">jacrev</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-it-s-made-two-foundational-autodiff-functions">How it’s made: two foundational autodiff functions</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jacobian-vector-products-jvps-aka-forward-mode-autodiff">Jacobian-Vector products (JVPs, aka forward-mode autodiff)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#jvps-in-math">JVPs in math</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#jvps-in-jax-code">JVPs in JAX code</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-jacobian-products-vjps-aka-reverse-mode-autodiff">Vector-Jacobian products (VJPs, aka reverse-mode autodiff)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#vjps-in-math">VJPs in math</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#vjps-in-jax-code">VJPs in JAX code</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-valued-gradients-with-vjps">Vector-valued gradients with VJPs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hessian-vector-products-using-both-forward-and-reverse-mode">Hessian-vector products using both forward- and reverse-mode</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#composing-vjps-jvps-and-vmap">Composing VJPs, JVPs, and <code class="docutils literal notranslate"><span class="pre">vmap</span></code></a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jacobian-matrix-and-matrix-jacobian-products">Jacobian-Matrix and Matrix-Jacobian products</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-implementation-of-jacfwd-and-jacrev">The implementation of <code class="docutils literal notranslate"><span class="pre">jacfwd</span></code> and <code class="docutils literal notranslate"><span class="pre">jacrev</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#complex-numbers-and-differentiation">Complex numbers and differentiation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#more-advanced-autodiff">More advanced autodiff</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The JAX authors
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024, The JAX Authors.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>