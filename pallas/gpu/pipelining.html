
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Mosaic GPU Pipelining &#8212; JAX  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=5b4479735964841361fd" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=5b4479735964841361fd" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=5b4479735964841361fd" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/style.css?v=7143c0a5" />
    <link rel="stylesheet" href="../../_static/style.css" type="text/css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd" />
  <script src="../../_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=5b4479735964841361fd"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=30646c52"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'pallas/gpu/pipelining';</script>
    <link rel="icon" href="../../_static/favicon.png"/>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Pallas Design Notes" href="../design/index.html" />
    <link rel="prev" title="Writing Mosaic GPU kernels with Pallas" href="reference.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/jax_logo_250px.png" class="logo__image only-light" alt="JAX  documentation - Home"/>
    <script>document.write(`<img src="../../_static/jax_logo_250px.png" class="logo__image only-dark" alt="JAX  documentation - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/thinking_in_jax.html">Quickstart: How to think in JAX</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../notebooks/Common_Gotchas_in_JAX.html">üî™ JAX - The Sharp Bits üî™</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../tutorials.html">Tutorials</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../jit-compilation.html">Just-in-time compilation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../automatic-vectorization.html">Automatic vectorization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../automatic-differentiation.html">Automatic differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../random-numbers.html">Pseudorandom numbers</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../stateful-computations.html">Stateful computations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../control-flow.html">Control flow and logical operators with JIT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../pytrees.html">Pytrees</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../working-with-pytrees.html">Working with pytrees</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Resources, guides, and references</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../key-concepts.html">Key concepts</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../../advanced_guides.html">Resources and Advanced Guides</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/autodiff_cookbook.html">The Autodiff Cookbook</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Custom_derivative_rules_for_Python_code.html">Custom derivative rules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/autodiff_remat.html">Control autodiff‚Äôs saved values with <code class="docutils literal notranslate"><span class="pre">jax.checkpoint</span></code> (aka <code class="docutils literal notranslate"><span class="pre">jax.remat</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../advanced-autodiff.html">Advanced automatic differentiation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../errors.html">Errors</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../debugging.html">Introduction to debugging</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../debugging/print_breakpoint.html">Compiled prints and breakpoints</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../debugging/checkify_guide.html">The <code class="docutils literal notranslate"><span class="pre">checkify</span></code> transformation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../debugging/flags.html">JAX debugging flags</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../debugging/flags.html">JAX debugging flags</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../transfer_guard.html">Transfer guard</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../persistent_compilation_cache.html">Persistent compilation cache</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../gpu_performance_tips.html">GPU performance tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../profiling.html">Profiling computation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../device_memory_profiling.html">Profiling device memory</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Distributed_arrays_and_automatic_parallelization.html">Distributed arrays and automatic parallelization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/explicit-sharding.html">Explicit sharding (a.k.a. ‚Äúsharding in types‚Äù)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/shard_map.html">Manual parallelism with <code class="docutils literal notranslate"><span class="pre">shard_map</span></code></a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/layout.html">Device-local array layout control</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/host-offloading.html">JAX Memories and Host Offloading</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../multi_process.html">Introduction to multi-controller JAX (aka multi-process/multi-host JAX)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../distributed_data_loading.html">Distributed data loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../external-callbacks.html">External callbacks</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../ffi.html">Foreign function interface (FFI)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../gradient-checkpointing.html">Gradient checkpointing with <code class="docutils literal notranslate"><span class="pre">jax.checkpoint</span></code> (<code class="docutils literal notranslate"><span class="pre">jax.remat</span></code>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../aot.html">Ahead-of-time lowering and compilation</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../export/index.html">Exporting and serialization</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../export/export.html">Exporting and serializing staged-out computations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../export/shape_poly.html">Shape polymorphism</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../export/jax2tf.html">Interoperation with TensorFlow</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../type_promotion.html">Type promotion semantics</a></li>
<li class="toctree-l2 current active has-children"><a class="reference internal" href="../index.html">Pallas: a JAX kernel language</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../quickstart.html">Pallas Quickstart</a></li>
<li class="toctree-l3"><a class="reference internal" href="../pipelining.html">Software Pipelining</a></li>
<li class="toctree-l3"><a class="reference internal" href="../grid_blockspec.html">Grids and BlockSpecs</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../tpu/index.html">Pallas TPU</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../tpu/details.html">Writing TPU kernels with Pallas</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tpu/pipelining.html">TPU Pipelining</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tpu/matmul.html">Matrix Multiplication</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tpu/sparse.html">Scalar Prefetch and Block-Sparse Computation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tpu/distributed.html">Distributed Computing in Pallas for TPUs</a></li>
</ul>
</li>
<li class="toctree-l3 current active has-children"><a class="reference internal" href="index.html">Pallas:Mosaic GPU</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l4"><a class="reference internal" href="reference.html">Writing Mosaic GPU kernels with Pallas</a></li>
<li class="toctree-l4 current active"><a class="current reference internal" href="#">Mosaic GPU Pipelining</a></li>
</ul>
</li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../design/index.html">Pallas Design Notes</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../design/design.html">Pallas Design</a></li>
<li class="toctree-l4"><a class="reference internal" href="../design/async_note.html">Pallas Async Operations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../CHANGELOG.html">Pallas Changelog</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/neural_network_with_tfds_data.html">Training a simple neural network, with tensorflow/datasets data loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Neural_Network_and_Data_Loading.html">Training a simple neural network, with PyTorch data loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/vmapped_log_probs.html">Autobatching for Bayesian inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/convolutions.html">Generalized convolutions in JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../xla_flags.html">XLA compiler flags</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../sharded-computation.html">Introduction to parallel programming</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax-primitives.html">JAX Internals: primitives</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jaxpr.html">JAX internals: The jaxpr language</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../contributor_guide.html">Developer notes</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../contributing.html">Contributing to JAX</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../developer.html">Building from source</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../investigating_a_regression.html">Investigating a regression</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../autodidax.html">Autodidax: JAX core from scratch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../autodidax2_part1.html">Autodidax2, part 1: JAX from scratch, again</a></li>

<li class="toctree-l2 has-children"><a class="reference internal" href="../../jep/index.html">JAX Enhancement Proposals (JEPs)</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../jep/263-prng.html">263: JAX PRNG Design</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/2026-custom-derivatives.html">2026: Custom JVP/VJP rules for JAX-transformable functions</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/4008-custom-vjp-update.html">4008: Custom VJP and `nondiff_argnums` update</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/4410-omnistaging.html">4410: Omnistaging</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/9263-typed-keys.html">9263: Typed keys &amp; pluggable RNGs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/9407-type-promotion.html">9407: Design of Type Promotion Semantics for JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/9419-jax-versioning.html">9419: Jax and Jaxlib versioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/10657-sequencing-effects.html">10657: Sequencing side-effects in JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/11830-new-remat-checkpoint.html">11830: `jax.remat` / `jax.checkpoint` new implementation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/12049-type-annotations.html">12049: Type Annotation Roadmap for JAX</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/14273-shard-map.html">14273: `shard_map` (`shmap`) for simple per-device code</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/15856-jex.html">15856: `jax.extend`, an extensions module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/17111-shmap-transpose.html">17111: Efficient transposition of `shard_map` (and other maps)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/18137-numpy-scipy-scope.html">18137: Scope of JAX NumPy &amp; SciPy Wrappers</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/25516-effver.html">25516: Effort-based versioning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jep/28661-jax-array-protocol.html">28661: Supporting the `__jax_array__` protocol</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../extensions.html">Extension guides</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../notebooks/Writing_custom_interpreters_in_Jax.html">Writing custom Jaxpr interpreters in JAX</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../jax.extend.html"><code class="docutils literal notranslate"><span class="pre">jax.extend</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../jax.extend.core.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.core</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.extend.linear_util.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.linear_util</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.extend.mlir.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.mlir</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.extend.random.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.random</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../building_on_jax.html">Building on JAX</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../notes.html">Notes</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../../api_compatibility.html">API compatibility</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../deprecation.html">Python and NumPy version support policy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../async_dispatch.html">Asynchronous dispatch</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../concurrency.html">Concurrency</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../gpu_memory_allocation.html">GPU memory allocation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../rank_promotion_warning.html">Rank promotion warning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../type_promotion.html">Type promotion semantics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../default_dtypes.html">Default dtypes and the X64 flag</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../jax.html">Public API: <code class="docutils literal notranslate"><span class="pre">jax</span></code> package</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../jax.numpy.html"><code class="docutils literal notranslate"><span class="pre">jax.numpy</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.fft.html">jax.numpy.fft.fft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.fft2.html">jax.numpy.fft.fft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.fftfreq.html">jax.numpy.fft.fftfreq</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.fftn.html">jax.numpy.fft.fftn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.fftshift.html">jax.numpy.fft.fftshift</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.hfft.html">jax.numpy.fft.hfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.ifft.html">jax.numpy.fft.ifft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.ifft2.html">jax.numpy.fft.ifft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.ifftn.html">jax.numpy.fft.ifftn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.ifftshift.html">jax.numpy.fft.ifftshift</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.ihfft.html">jax.numpy.fft.ihfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.irfft.html">jax.numpy.fft.irfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.irfft2.html">jax.numpy.fft.irfft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.irfftn.html">jax.numpy.fft.irfftn</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.rfft.html">jax.numpy.fft.rfft</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.rfft2.html">jax.numpy.fft.rfft2</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.rfftfreq.html">jax.numpy.fft.rfftfreq</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.numpy.fft.rfftn.html">jax.numpy.fft.rfftn</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../jax.scipy.html"><code class="docutils literal notranslate"><span class="pre">jax.scipy</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.scipy.stats.bernoulli.logpmf.html">jax.scipy.stats.bernoulli.logpmf</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.scipy.stats.bernoulli.pmf.html">jax.scipy.stats.bernoulli.pmf</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.scipy.stats.bernoulli.cdf.html">jax.scipy.stats.bernoulli.cdf</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../_autosummary/jax.scipy.stats.bernoulli.ppf.html">jax.scipy.stats.bernoulli.ppf</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.lax.html"><code class="docutils literal notranslate"><span class="pre">jax.lax</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.random.html"><code class="docutils literal notranslate"><span class="pre">jax.random</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.sharding.html"><code class="docutils literal notranslate"><span class="pre">jax.sharding</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.debug.html"><code class="docutils literal notranslate"><span class="pre">jax.debug</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.dlpack.html"><code class="docutils literal notranslate"><span class="pre">jax.dlpack</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.distributed.html"><code class="docutils literal notranslate"><span class="pre">jax.distributed</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.dtypes.html"><code class="docutils literal notranslate"><span class="pre">jax.dtypes</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.ffi.html"><code class="docutils literal notranslate"><span class="pre">jax.ffi</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.flatten_util.html"><code class="docutils literal notranslate"><span class="pre">jax.flatten_util</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.image.html"><code class="docutils literal notranslate"><span class="pre">jax.image</span></code> module</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../jax.nn.html"><code class="docutils literal notranslate"><span class="pre">jax.nn</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../jax.nn.initializers.html"><code class="docutils literal notranslate"><span class="pre">jax.nn.initializers</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.ops.html"><code class="docutils literal notranslate"><span class="pre">jax.ops</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.profiler.html"><code class="docutils literal notranslate"><span class="pre">jax.profiler</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.stages.html"><code class="docutils literal notranslate"><span class="pre">jax.stages</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.test_util.html"><code class="docutils literal notranslate"><span class="pre">jax.test_util</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.tree.html"><code class="docutils literal notranslate"><span class="pre">jax.tree</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.tree_util.html"><code class="docutils literal notranslate"><span class="pre">jax.tree_util</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.typing.html"><code class="docutils literal notranslate"><span class="pre">jax.typing</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.export.html"><code class="docutils literal notranslate"><span class="pre">jax.export</span></code> module</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../jax.extend.html"><code class="docutils literal notranslate"><span class="pre">jax.extend</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../jax.extend.core.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.core</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.extend.linear_util.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.linear_util</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.extend.mlir.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.mlir</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.extend.random.html"><code class="docutils literal notranslate"><span class="pre">jax.extend.random</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../jax.example_libraries.html"><code class="docutils literal notranslate"><span class="pre">jax.example_libraries</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../jax.example_libraries.optimizers.html"><code class="docutils literal notranslate"><span class="pre">jax.example_libraries.optimizers</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.example_libraries.stax.html"><code class="docutils literal notranslate"><span class="pre">jax.example_libraries.stax</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../jax.experimental.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../../jax.experimental.checkify.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.checkify</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.experimental.compilation_cache.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.compilation_cache</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.experimental.custom_dce.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.custom_dce</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.experimental.custom_partitioning.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.custom_partitioning</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.experimental.jet.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.jet</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.experimental.key_reuse.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.key_reuse</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.experimental.mesh_utils.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.mesh_utils</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.experimental.multihost_utils.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.multihost_utils</span></code> module</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../jax.experimental.pallas.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pallas</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../jax.experimental.pallas.mosaic_gpu.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pallas.mosaic_gpu</span></code> module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../jax.experimental.pallas.triton.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pallas.triton</span></code> module</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../jax.experimental.pallas.tpu.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pallas.tpu</span></code> module</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.experimental.pjit.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.pjit</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.experimental.serialize_executable.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.serialize_executable</span></code> module</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../jax.experimental.shard_map.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.shard_map</span></code> module</a></li>
<li class="toctree-l3 has-children"><a class="reference internal" href="../../jax.experimental.sparse.html"><code class="docutils literal notranslate"><span class="pre">jax.experimental.sparse</span></code> module</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-22"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.BCOO.html">jax.experimental.sparse.BCOO</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_broadcast_in_dim.html">jax.experimental.sparse.bcoo_broadcast_in_dim</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_concatenate.html">jax.experimental.sparse.bcoo_concatenate</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_dot_general.html">jax.experimental.sparse.bcoo_dot_general</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_dot_general_sampled.html">jax.experimental.sparse.bcoo_dot_general_sampled</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_dynamic_slice.html">jax.experimental.sparse.bcoo_dynamic_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_extract.html">jax.experimental.sparse.bcoo_extract</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_fromdense.html">jax.experimental.sparse.bcoo_fromdense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_gather.html">jax.experimental.sparse.bcoo_gather</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_multiply_dense.html">jax.experimental.sparse.bcoo_multiply_dense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_multiply_sparse.html">jax.experimental.sparse.bcoo_multiply_sparse</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_update_layout.html">jax.experimental.sparse.bcoo_update_layout</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_reduce_sum.html">jax.experimental.sparse.bcoo_reduce_sum</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_reshape.html">jax.experimental.sparse.bcoo_reshape</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_slice.html">jax.experimental.sparse.bcoo_slice</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_sort_indices.html">jax.experimental.sparse.bcoo_sort_indices</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_squeeze.html">jax.experimental.sparse.bcoo_squeeze</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_sum_duplicates.html">jax.experimental.sparse.bcoo_sum_duplicates</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_todense.html">jax.experimental.sparse.bcoo_todense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../_autosummary/jax.experimental.sparse.bcoo_transpose.html">jax.experimental.sparse.bcoo_transpose</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../jax.lib.html"><code class="docutils literal notranslate"><span class="pre">jax.lib</span></code> module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.addressable_shards.html">jax.Array.addressable_shards</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.all.html">jax.Array.all</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.any.html">jax.Array.any</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.argmax.html">jax.Array.argmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.argmin.html">jax.Array.argmin</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.argpartition.html">jax.Array.argpartition</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.argsort.html">jax.Array.argsort</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.astype.html">jax.Array.astype</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.at.html">jax.Array.at</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.choose.html">jax.Array.choose</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.clip.html">jax.Array.clip</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.compress.html">jax.Array.compress</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.committed.html">jax.Array.committed</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.conj.html">jax.Array.conj</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.conjugate.html">jax.Array.conjugate</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.copy.html">jax.Array.copy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.copy_to_host_async.html">jax.Array.copy_to_host_async</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.cumprod.html">jax.Array.cumprod</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.cumsum.html">jax.Array.cumsum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.device.html">jax.Array.device</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.diagonal.html">jax.Array.diagonal</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.dot.html">jax.Array.dot</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.dtype.html">jax.Array.dtype</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.flat.html">jax.Array.flat</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.flatten.html">jax.Array.flatten</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.global_shards.html">jax.Array.global_shards</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.imag.html">jax.Array.imag</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.is_fully_addressable.html">jax.Array.is_fully_addressable</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.is_fully_replicated.html">jax.Array.is_fully_replicated</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.item.html">jax.Array.item</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.itemsize.html">jax.Array.itemsize</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.max.html">jax.Array.max</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.mean.html">jax.Array.mean</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.min.html">jax.Array.min</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.nbytes.html">jax.Array.nbytes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.ndim.html">jax.Array.ndim</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.nonzero.html">jax.Array.nonzero</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.prod.html">jax.Array.prod</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.ptp.html">jax.Array.ptp</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.ravel.html">jax.Array.ravel</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.real.html">jax.Array.real</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.repeat.html">jax.Array.repeat</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.reshape.html">jax.Array.reshape</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.round.html">jax.Array.round</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.searchsorted.html">jax.Array.searchsorted</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.shape.html">jax.Array.shape</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.sharding.html">jax.Array.sharding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.size.html">jax.Array.size</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.sort.html">jax.Array.sort</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.squeeze.html">jax.Array.squeeze</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.std.html">jax.Array.std</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.sum.html">jax.Array.sum</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.swapaxes.html">jax.Array.swapaxes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.take.html">jax.Array.take</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.to_device.html">jax.Array.to_device</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.trace.html">jax.Array.trace</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.transpose.html">jax.Array.transpose</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.var.html">jax.Array.var</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.view.html">jax.Array.view</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.T.html">jax.Array.T</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../_autosummary/jax.Array.mT.html">jax.Array.mT</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../about.html">About the project</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../faq.html">Frequently asked questions (FAQ)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../changelog.html">Change log</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../glossary.html">Glossary of terms</a></li>
</ul>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../config_options.html">Configuration Options</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
        <div class="header-article-item">





<nav aria-label="Breadcrumb">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../../advanced_guides.html" class="nav-link">Resources and Advanced Guides</a></li>
    
    
    <li class="breadcrumb-item"><i class="fa-solid fa-ellipsis"></i></li>
    
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">Pallas:Mosaic GPU</a></li>
    
    <li class="breadcrumb-item active" aria-current="page">Mosaic GPU...</li>
  </ul>
</nav>
</div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">


<a href="https://github.com/jax-ml/jax" target="_blank"
   class="btn btn-sm btn-source-repository-button"
   title="Source repository"
   data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>

</a>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/pallas/gpu/pipelining.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Mosaic GPU Pipelining</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pipelining-with-mosaic-gpu">Pipelining with Mosaic GPU</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compatibility-api-using-pl-pallas-call">Compatibility API using <code class="docutils literal notranslate"><span class="pre">pl.pallas_call</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-memory-spaces">GPU Memory Spaces</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-matmul-kernel-on-hopper-gpus">Example: Matmul Kernel on Hopper GPUs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#warp-specialization">Warp Specialization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-matrix-multiplication-with-warp-specialization">Example: Matrix Multiplication with Warp Specialization</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="mosaic-gpu-pipelining">
<span id="pallas-mgpu-pipelining"></span><h1>Mosaic GPU Pipelining<a class="headerlink" href="#mosaic-gpu-pipelining" title="Link to this heading">#</a></h1>
<p>This guide covers software pipelining using the Mosaic GPU backend for Pallas.</p>
<p>For a general overview of the pipelining API in Pallas, we recommend that users first read <a class="reference internal" href="../pipelining.html#pallas-software-pipelining"><span class="std std-ref">Software Pipelining</span></a>. Pipelining in Pallas is programmed explicitly. For those who are familiar with Triton, this is a significant difference in programming model because in Triton, pipelining is an optimization that is done automatically by the compiler.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">lax</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">jnp</span>
<span class="kn">from</span> <span class="nn">jax.experimental.pallas</span> <span class="kn">import</span> <span class="n">mosaic_gpu</span> <span class="k">as</span> <span class="n">plgpu</span>
<span class="kn">from</span> <span class="nn">jax.experimental</span> <span class="kn">import</span> <span class="n">pallas</span> <span class="k">as</span> <span class="n">pl</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
</pre></div>
</div>
</div>
</div>
<section id="pipelining-with-mosaic-gpu">
<h2>Pipelining with Mosaic GPU<a class="headerlink" href="#pipelining-with-mosaic-gpu" title="Link to this heading">#</a></h2>
<p>The recommended approach to pipeline using Mosaic GPU is to use the <code class="docutils literal notranslate"><span class="pre">plgpu.emit_pipeline</span></code> function to pipeline over sequential loops (and to use <code class="docutils literal notranslate"><span class="pre">plgpu.kernel</span></code> to partition the problem in parallel over the CUDA grid). <code class="docutils literal notranslate"><span class="pre">emit_pipeline</span></code> follows a similar API as <code class="docutils literal notranslate"><span class="pre">pl.pallas_call</span></code> except it exposes a few additional GPU-specific options.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">body</span></code>, <code class="docutils literal notranslate"><span class="pre">grid</span></code> have similar semantics as in <code class="docutils literal notranslate"><span class="pre">pl.pallas_call</span></code>. The <code class="docutils literal notranslate"><span class="pre">grid</span></code> denotes how many invocations of the <code class="docutils literal notranslate"><span class="pre">body</span></code> function to run. In contrast with a CUDA grid, the pipeline grid is guaranteed to run sequentially.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">in_specs</span></code> and <code class="docutils literal notranslate"><span class="pre">out_specs</span></code> also work similarly to <code class="docutils literal notranslate"><span class="pre">pl.pallas_call</span></code>, except they also accept <code class="docutils literal notranslate"><span class="pre">plgpu.BlockSpec</span></code> instances that can be used specify GPU-specific transforms, such as swizzling. See <a class="reference external" href="https://docs.jax.dev/en/latest/pallas/gpu/reference.html#memory-reference-transforms">memory reference transforms</a> for more detail on available transformations.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_concurrent_steps</span></code> controls the maximum number of concurrent memory transfers. Using additional concurrent steps will consume more SMEM to hold temporary buffers, but it can improve the utilization of the memory subsystem. We recommend autotuning this parameter. Low values (e.g. 2) can sometimes achieve higher occupancy (due to lower SMEM usage) which can improve throughput in ALU-heavy kernels, but will introduce more noise due to the hardware taking care of scheduling. Larger values (between 4 and 6) will work best for kernels that can‚Äôt take advantage of extra occupancy</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">delay_release</span></code> allows the user to specify an additional number of iterations to wait before the buffer is re-used by the pipeline. For example, a buffer copied into SMEM on iteration 0 with <code class="docutils literal notranslate"><span class="pre">delay_release=1</span></code> and <code class="docutils literal notranslate"><span class="pre">max_concurrent_steps=2</span></code> will not be re-used until iteration 3, as opposed to iteration 2 for a standard double-buffered strategy. <code class="docutils literal notranslate"><span class="pre">delay_release=1</span></code> is necessary if you don‚Äôt await a <code class="docutils literal notranslate"><span class="pre">plgpu.wgmma</span></code> operation on the pipeline operands, as otherwise the pipeline will begin overwriting the buffers while the WGMMA is still reading them. This is useful for certain optimizations such as allowing multiple async matmuls in flight to keep the tensor core pipeline filled, but care must be taken when using such a strategy as <strong>omitting this parameter will silent data races</strong>, and it reduces the efficiency of <code class="docutils literal notranslate"><span class="pre">emit_pipeline</span></code> as we are overlapping fewer memory transfers.</p></li>
</ul>
<section id="compatibility-api-using-pl-pallas-call">
<h3>Compatibility API using <code class="docutils literal notranslate"><span class="pre">pl.pallas_call</span></code><a class="headerlink" href="#compatibility-api-using-pl-pallas-call" title="Link to this heading">#</a></h3>
<p>As an alternative to <code class="docutils literal notranslate"><span class="pre">emit_pipeline</span></code> and to maintain compatibility with Pallas TPU, Mosaic GPU also implements the existing <code class="docutils literal notranslate"><span class="pre">pl.pallas_call</span></code> API. By default, <code class="docutils literal notranslate"><span class="pre">pl.pallas_call</span></code> on Mosaic GPU will partition your kernel in parallel over the CUDA grid. You can opt-in to pipelining by passing in a <code class="docutils literal notranslate"><span class="pre">plgpu.GPUCompilerParams</span></code> object as the <code class="docutils literal notranslate"><span class="pre">compiler_params</span></code> argument, which specifies the following options that are relevant for pipelining:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">dimension_semantics</span></code>: A tuple of <code class="docutils literal notranslate"><span class="pre">Literal['parallel',</span> <span class="pre">'sequential']</span></code> that specifies iteration semantics for each grid dimension. <code class="docutils literal notranslate"><span class="pre">parallel</span></code> will partition the corresponding dimension over the CUDA grid, and <code class="docutils literal notranslate"><span class="pre">sequential</span></code> dimensions will be pipelined sequentially. <strong>Note that if no dimensions are marked <code class="docutils literal notranslate"><span class="pre">sequential</span></code>, no pipelining will happen!</strong></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_concurrent_steps</span></code>: identical to the option in <code class="docutils literal notranslate"><span class="pre">plgpu.emit_pipeline</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">delay_release</span></code>: identical to the option in <code class="docutils literal notranslate"><span class="pre">plgpu.emit_pipeline</span></code>.</p></li>
</ul>
<p>Pipelining lets you re-use scratch buffers across the sequential iterations of the grid (e.g. for implementing reductions). Additionally, <code class="docutils literal notranslate"><span class="pre">pallas_call</span></code> supports using <code class="docutils literal notranslate"><span class="pre">plgpu.BlockSpec</span></code> objects in place of <code class="docutils literal notranslate"><span class="pre">pl.BlockSpec</span></code> objects when using the Mosaic GPU backend, allowing you to specify GPU-specific memory transformations.</p>
<p>We recommend that users use <code class="docutils literal notranslate"><span class="pre">plgpu.kernel</span></code> rather than <code class="docutils literal notranslate"><span class="pre">pl.pallas_call</span></code> as <code class="docutils literal notranslate"><span class="pre">plgpu.kernel</span></code> supports more features (such as specifying the number of warpgroups and warp specialization).</p>
</section>
</section>
<section id="gpu-memory-spaces">
<h2>GPU Memory Spaces<a class="headerlink" href="#gpu-memory-spaces" title="Link to this heading">#</a></h2>
<p>Refs exist primarily in one of two memory spaces, which can be explicitly specified by the <code class="docutils literal notranslate"><span class="pre">memory_space</span></code> argument of <code class="docutils literal notranslate"><span class="pre">BlockSpec</span></code>, i.e. <code class="docutils literal notranslate"><span class="pre">BlockSpec(memory_space=plgpu.GPUMemorySpace.GMEM)</span></code>.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">plgpu.GPUMemorySpace.SMEM</span></code> allocates a Ref in Shared Memory (SMEM). SMEM Refs can be dereferenced using array indexing syntax to store values in registers for compute, i.e. <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">y_ref[...]</span></code>. This memory space used for a Ref when using <code class="docutils literal notranslate"><span class="pre">emit_pipeline</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">plgpu.GPUMemorySpace.GMEM</span></code> allocates a Ref in Global Memory (GMEM/HBM). Any Refs allocated in GMEM are not pipelined, and values cannot be accessed directly using array indexing operations. Instead, GMEM must be accessed via SMEM using <code class="docutils literal notranslate"><span class="pre">plgpu.copy_gmem_to_smem</span></code> for reading, or <code class="docutils literal notranslate"><span class="pre">plgpu.copy_smem_to_gmem</span></code> for writing, or pipelined into SMEM using <code class="docutils literal notranslate"><span class="pre">plgpu.emit_pipeline</span></code>.</p></li>
</ul>
<p>The primary purpose of <code class="docutils literal notranslate"><span class="pre">emit_pipeline</span></code> is used to overlap TensorCore computation with data transfers between GMEM and SMEM, since asynchronous copies between GMEM/SMEM have a long latency, but all TensorCore computation must operate on registers (or SMEM Refs in the case of matrix multiplication).</p>
</section>
<section id="example-matmul-kernel-on-hopper-gpus">
<h2>Example: Matmul Kernel on Hopper GPUs<a class="headerlink" href="#example-matmul-kernel-on-hopper-gpus" title="Link to this heading">#</a></h2>
<p>Let‚Äôs begin with a matrix multiplication example designed to run on Hopper GPUs. This kernel utilizes the Hopper-specific <code class="docutils literal notranslate"><span class="pre">wgmma</span></code> (warpgroup matrix multiply accumulate) instruction. <code class="docutils literal notranslate"><span class="pre">wgmma</span></code> is issued by a single Mosaic GPU thread and runs asynchronously on the TensorCore.</p>
<p>Our example kernel implements a blockwise matrix multiplication of two matrices of shape <code class="docutils literal notranslate"><span class="pre">[M,</span> <span class="pre">K]</span> <span class="pre">&#64;</span> <span class="pre">[K,</span> <span class="pre">N]</span> <span class="pre">=</span> <span class="pre">[M,</span> <span class="pre">N]</span></code>, where each output block is computed in parallel over the CUDA grid. This grid is specified as the <code class="docutils literal notranslate"><span class="pre">grid</span></code> argument to the outer <code class="docutils literal notranslate"><span class="pre">plgpu.kernel</span></code>, and parallelizes over the non-contracting dimensions M, N of the matrix multiplication.</p>
<center><img src="../../_static/pallas/gpu/pipeline_matmul.svg" /></center>
<p>Within a program instance, we run a sequential pipeline using <code class="docutils literal notranslate"><span class="pre">plgpu.emit_pipeline</span></code> that reduces over the contracting dimension K of the matrix multiplication. On each iteration of the pipeline, we load one tile from each input matrix, multiply them, and then store the result in an accumulator Ref (<code class="docutils literal notranslate"><span class="pre">plgpu.ACC</span></code>). <code class="docutils literal notranslate"><span class="pre">plgpu.ACC</span></code> is a special type of Ref that lives in registers and holds the intermediate results of WGMMA. Once we have accumulated over the entire contracting dimension, we write out the result to the output Ref.</p>
<p>To perform the actual matrix multiplication, we call <code class="docutils literal notranslate"><span class="pre">plgpu.wgmma</span></code> with the accumulator, LHS, and RHS Refs as arguments in order to push the arguments into the TensorCore pipeline. All WGMMA operations are executed in order, so this can be viewed as pushing operations into a queue. Since <code class="docutils literal notranslate"><span class="pre">wgmma</span></code> is an asynchronous instruction, <code class="docutils literal notranslate"><span class="pre">plgpu.wgmma_wait(N)</span></code> is used to wait until there are no more than N <code class="docutils literal notranslate"><span class="pre">wgmma</span></code> operations left in-flight. In this particular implementation we wait for 1 in-flight WGMMA, meaning that the WGMMA we queue on the current iteration will be waited for on the next iteration.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">wgmma</span></code> wants it‚Äôs arguments to be in a specific format, defined in the <a class="reference external" href="https://docs.nvidia.com/cuda/parallel-thread-execution/#register-fragments-and-shared-memory-matrix-layouts">CUDA documentation</a>. These are implemented by the <code class="docutils literal notranslate"><span class="pre">TilingTransform</span></code> and <code class="docutils literal notranslate"><span class="pre">SwizzleTransform</span></code> transformations on the input BlockSpecs. Note that in the future transforms will be inferred automatically by Mosaic GPU and these will not need to be manually specified. See the <a class="reference external" href="https://docs.jax.dev/en/latest/pallas/gpu/reference.html#hopper-wgmma">wgmma reference</a> for full details on using this instruction.</p></li>
<li><p>We use the <code class="docutils literal notranslate"><span class="pre">delay_release</span></code> parameter in conjunction with <code class="docutils literal notranslate"><span class="pre">plgpu.wgmma_wait(1)</span></code> to always allow one <code class="docutils literal notranslate"><span class="pre">WGMMA</span></code> operation to stay in-flight in order to ensure good TensorCore utilization. Without this, we would be flushing the TensorCore pipeline on every iteration of the kernel.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">tile_m</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">tile_n</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">swizzle</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
  <span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float16</span>
  <span class="n">swizzle_elems</span> <span class="o">=</span> <span class="n">swizzle</span> <span class="o">//</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">itemsize</span>
  <span class="n">tile_k</span> <span class="o">=</span> <span class="n">swizzle_elems</span>
  <span class="n">grid_m</span> <span class="o">=</span> <span class="n">m</span> <span class="o">//</span> <span class="n">tile_m</span>
  <span class="n">grid_k</span> <span class="o">=</span> <span class="n">k</span> <span class="o">//</span> <span class="n">tile_k</span>
  <span class="n">grid_n</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="n">tile_n</span>
  <span class="k">assert</span> <span class="n">tile_m</span> <span class="o">%</span> <span class="n">swizzle_elems</span> <span class="o">==</span> <span class="mi">0</span>

  <span class="c1"># Note: Transforms will be inferred automatically</span>
  <span class="c1"># by Mosaic GPU in the future.</span>
  <span class="n">transforms</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">plgpu</span><span class="o">.</span><span class="n">TilingTransform</span><span class="p">((</span><span class="mi">8</span><span class="p">,</span> <span class="n">swizzle_elems</span><span class="p">)),</span>
    <span class="n">plgpu</span><span class="o">.</span><span class="n">SwizzleTransform</span><span class="p">(</span><span class="n">swizzle</span><span class="p">),</span>
  <span class="p">)</span>

  <span class="k">def</span> <span class="nf">kernel</span><span class="p">(</span><span class="n">a_gmem</span><span class="p">,</span> <span class="n">b_gmem</span><span class="p">,</span> <span class="n">o_gmem</span><span class="p">,</span> <span class="n">o_smem</span><span class="p">,</span> <span class="n">acc</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">pipeline_step</span><span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">a_smem</span><span class="p">,</span> <span class="n">b_smem</span><span class="p">):</span>
      <span class="n">plgpu</span><span class="o">.</span><span class="n">wgmma</span><span class="p">(</span><span class="n">acc</span><span class="p">,</span> <span class="n">a_smem</span><span class="p">,</span> <span class="n">b_smem</span><span class="p">)</span>
      <span class="n">plgpu</span><span class="o">.</span><span class="n">wgmma_wait</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># pl.program_id obtains the index into the grid.</span>
    <span class="n">pid_m</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">pid_n</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">pipeline</span> <span class="o">=</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">emit_pipeline</span><span class="p">(</span>
        <span class="n">pipeline_step</span><span class="p">,</span>
        <span class="n">in_specs</span><span class="o">=</span><span class="p">[</span>
            <span class="n">plgpu</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">(</span>
                <span class="p">(</span><span class="n">tile_m</span><span class="p">,</span> <span class="n">tile_k</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">k</span><span class="p">:</span> <span class="p">(</span><span class="n">pid_m</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">transforms</span><span class="o">=</span><span class="n">transforms</span>
            <span class="p">),</span>
            <span class="n">plgpu</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">(</span>
                <span class="p">(</span><span class="n">tile_k</span><span class="p">,</span> <span class="n">tile_n</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">k</span><span class="p">:</span> <span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">pid_n</span><span class="p">),</span> <span class="n">transforms</span><span class="o">=</span><span class="n">transforms</span>
            <span class="p">),</span>
        <span class="p">],</span>
        <span class="n">grid</span><span class="o">=</span><span class="p">(</span><span class="n">grid_k</span><span class="p">,),</span>
        <span class="n">max_concurrent_steps</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">delay_release</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">pipeline</span><span class="p">(</span><span class="n">a_gmem</span><span class="p">,</span> <span class="n">b_gmem</span><span class="p">)</span>
    <span class="c1"># Store WGMMA accumulator to SMEM and then to GMEM.</span>
    <span class="n">o_smem</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">acc</span><span class="p">[</span><span class="o">...</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>
    <span class="n">plgpu</span><span class="o">.</span><span class="n">commit_smem</span><span class="p">()</span>
    <span class="n">m_slice</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">ds</span><span class="p">(</span><span class="n">pid_m</span> <span class="o">*</span> <span class="n">tile_m</span><span class="p">,</span> <span class="n">tile_m</span><span class="p">)</span>
    <span class="n">n_slice</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">ds</span><span class="p">(</span><span class="n">pid_n</span> <span class="o">*</span> <span class="n">tile_n</span><span class="p">,</span> <span class="n">tile_n</span><span class="p">)</span>
    <span class="n">plgpu</span><span class="o">.</span><span class="n">copy_smem_to_gmem</span><span class="p">(</span><span class="n">o_smem</span><span class="p">,</span> <span class="n">o_gmem</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">m_slice</span><span class="p">,</span> <span class="n">n_slice</span><span class="p">])</span>
    <span class="n">plgpu</span><span class="o">.</span><span class="n">wait_smem_to_gmem</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">kernel</span><span class="p">(</span>
      <span class="n">kernel</span><span class="p">,</span>
      <span class="n">out_shape</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float16</span><span class="p">),</span>
      <span class="n">scratch_shapes</span><span class="o">=</span><span class="p">[</span>
          <span class="n">plgpu</span><span class="o">.</span><span class="n">SMEM</span><span class="p">((</span><span class="n">tile_m</span><span class="p">,</span> <span class="n">tile_n</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float16</span><span class="p">),</span>
          <span class="n">plgpu</span><span class="o">.</span><span class="n">ACC</span><span class="p">((</span><span class="n">tile_m</span><span class="p">,</span> <span class="n">tile_n</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
          <span class="p">],</span>
      <span class="c1"># grid specifies the CUDA grid.</span>
      <span class="c1"># Instances of `kernel` will be executed in parallel over this grid.</span>
      <span class="n">grid</span><span class="o">=</span><span class="p">(</span><span class="n">grid_m</span><span class="p">,</span> <span class="n">grid_n</span><span class="p">),</span>
      <span class="n">grid_names</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;m&quot;</span><span class="p">,</span> <span class="s2">&quot;n&quot;</span><span class="p">),</span>
  <span class="p">)(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="n">m</span> <span class="o">=</span> <span class="mi">132</span> <span class="o">*</span> <span class="mi">128</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">128</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="mi">64</span>
<span class="n">key1</span><span class="p">,</span> <span class="n">key2</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">key1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">key2</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">matmul</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">a</span> <span class="o">@</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="warp-specialization">
<h2>Warp Specialization<a class="headerlink" href="#warp-specialization" title="Link to this heading">#</a></h2>
<p>Warp specialization is a technique where we program each warp/warpgroup to perform a single task in order to give the GPU hardware the flexibility to schedule them at runtime. Recall that each streaming multiprocessor (SM) in a GPU contains warp schedulers that can swap execution between warps, so for example when one warp is stalling it can begin executing a different warp. In practice, this can be more performant than programming a single instruction stream where the compiler must statically schedule the operations and attempt to overlap them optimally.</p>
<p>In particular, we are interested in warpgroup specialization on Hopper+ GPUs, where it can be useful to have a separate warpgroup issuing TMAs (GMEM/SMEM copies) from the warpgroups performing arithmetic, since indexing calculations and issuing TMAs can take up a significant amount of time and potentially leave the TensorCore idle. The figure below depicts a standard, non-specialized kernel on the left where TMAs (async copies) and matrix multiplication are issued from a single instruction stream, and a warp-specialized version on the right where communication and arithmetic are handled on separate warpgroups. A <em>consumed barrier</em> is used to synchronize between the specialized warpgroups that signals to the memory warpgroup when it is safe to begin the next TMA.</p>
<center><img src="../../_static/pallas/gpu/warp_specialization.svg" /></center>
<p>Warp specialization can be enabled in Pallas by using the <code class="docutils literal notranslate"><span class="pre">plgpu.emit_pipeline_warp_specialized</span></code> helper. This pipeline helper handles all of the logic in the memory thread, and the user only needs to specify the work done in the compute threads. It shares the a similar API as the standard <code class="docutils literal notranslate"><span class="pre">emit_pipeline</span></code>, and currently supports the following arguments:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plgpu</span><span class="o">.</span><span class="n">emit_pipeline_warp_specialized</span><span class="p">(</span>
  <span class="n">body</span><span class="p">:</span> <span class="n">Callable</span><span class="p">,</span>
  <span class="o">*</span>
  <span class="n">grid</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
  <span class="n">in_specs</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">pallas_core</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">]</span> <span class="o">=</span> <span class="p">(),</span>
  <span class="n">out_specs</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">pallas_core</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">]</span> <span class="o">=</span> <span class="p">(),</span>
  <span class="n">max_concurrent_steps</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
  <span class="n">compute_context</span><span class="p">:</span> <span class="n">Callable</span>
  <span class="n">num_compute_wgs</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
  <span class="n">memory_registers</span><span class="p">:</span> <span class="nb">int</span>
  <span class="n">wg_axis</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
  <span class="n">memory_thread_idx</span><span class="p">:</span> <span class="nb">int</span> <span class="o">|</span> <span class="kc">None</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>There are a few arguments specific to this pipeline emitter, which are:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">num_compute_wgs</span></code> specifies how many compute threads/warpgroups to use. The pipeline emitter always uses a single memory thread, so in <code class="docutils literal notranslate"><span class="pre">plgpu.kernel</span></code> you should specify <code class="docutils literal notranslate"><span class="pre">num_threads=num_compute_wgs+1</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">memory_registers</span></code> controls how many registers to allocate to the memory thread. The remaining registers are partitioned evenly among the compute threads. The default value is 40 and should be adjusted up or down depending on whether register spills are encountered.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">wg_axis</span></code> the name of the thread/warpgroup axis (as specified by the <code class="docutils literal notranslate"><span class="pre">thead_name</span></code> argument of <code class="docutils literal notranslate"><span class="pre">plgpu.kernel</span></code>).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">memory_thread_idx</span></code> specifies which Pallas thread to designate as the memory thread. Defaults to the last thread.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">compute_context</span></code> is a enables you to specify a prologue/epilogue to the pipeline that only runs in the compute thread. The function allows you to define the initialization and consumption of a loop carry through the pipeline. All compute thread specific arrays should be instantiated here so the memory thread does not materialize them in registers ‚Äì otherwise, you may experience slowdowns due to register spills.</p></li>
</ul>
<p>The pipeline body of the warp specialized pipeline is run in parallel by all compute threads, and SMEM is shared between compute threads since they are scheduled within the same CUDA block.<code class="docutils literal notranslate"><span class="pre">lax.axis_index</span></code> can be used inside the kernel to obtain the Pallas thread index in order to divide up work amongst compute threads.</p>
</section>
<section id="example-matrix-multiplication-with-warp-specialization">
<h2>Example: Matrix Multiplication with Warp Specialization<a class="headerlink" href="#example-matrix-multiplication-with-warp-specialization" title="Link to this heading">#</a></h2>
<p>The following example extends the previous matrix multiplication example to use warp specialization. This particular kernel uses 2 compute threads, which operate on separate columns of the RHS matrix but share the same LHS. Each invocation of the pipeline therefore computes 2 adjacent blocks in the output matrix.</p>
<center><img src="../../_static/pallas/gpu/pipeline_matmul_ws.svg" /></center>
<p>We use the <code class="docutils literal notranslate"><span class="pre">compute_context</span></code> pattern to initialize the WGMMA accumulator, and copy the final accumulator from registers into SMEM. Here, the compute context is defined in the function <code class="docutils literal notranslate"><span class="pre">compute_thread</span></code>. It is critical that the accumulator be created inside of the <code class="docutils literal notranslate"><span class="pre">compute_thread</span></code> function to avoid allocating it in the memory thread which would waste registers. To perform the WGMMA, we wrap the <code class="docutils literal notranslate"><span class="pre">wgmma</span></code> instruction in a <code class="docutils literal notranslate"><span class="pre">pl.run_state</span></code> in order to create an accumulator ref that is initialized to the carry value.</p>
<p>Instead of using <code class="docutils literal notranslate"><span class="pre">pl.pallas_call</span></code> to call the kernel, we instead use the GPU-specific <code class="docutils literal notranslate"><span class="pre">plgpu.kernel</span></code> entry point. <code class="docutils literal notranslate"><span class="pre">plgpu.kernel</span></code> allows us to specify the number of threads to launch per CUDA block via the <code class="docutils literal notranslate"><span class="pre">num_threads</span></code> argument, and allows us to specify a <code class="docutils literal notranslate"><span class="pre">thread_name</span></code> we can use to query the Pallas thread index inside of the kernel.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">matmul_warp_specialized</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">tile_m</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">tile_n</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">swizzle</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
                            <span class="n">compute_wgs</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
  <span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float16</span>
  <span class="n">elems_128b</span> <span class="o">=</span> <span class="n">swizzle</span> <span class="o">//</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">itemsize</span>
  <span class="n">tile_k</span> <span class="o">=</span> <span class="n">elems_128b</span>
  <span class="n">grid_m</span> <span class="o">=</span> <span class="n">m</span> <span class="o">//</span> <span class="n">tile_m</span>
  <span class="n">grid_k</span> <span class="o">=</span> <span class="n">k</span> <span class="o">//</span> <span class="n">tile_k</span>
  <span class="n">grid_n</span> <span class="o">=</span> <span class="n">n</span> <span class="o">//</span> <span class="n">tile_n</span>
  <span class="k">assert</span> <span class="n">tile_m</span> <span class="o">%</span> <span class="n">elems_128b</span> <span class="o">==</span> <span class="mi">0</span>

  <span class="n">transforms</span> <span class="o">=</span> <span class="p">(</span>
          <span class="n">plgpu</span><span class="o">.</span><span class="n">TilingTransform</span><span class="p">((</span><span class="mi">8</span><span class="p">,</span> <span class="n">elems_128b</span><span class="p">)),</span>
          <span class="n">plgpu</span><span class="o">.</span><span class="n">SwizzleTransform</span><span class="p">(</span><span class="mi">128</span><span class="p">),</span>
      <span class="p">)</span>

  <span class="k">def</span> <span class="nf">kernel</span><span class="p">(</span><span class="n">a_gmem</span><span class="p">,</span> <span class="n">b_gmem</span><span class="p">,</span> <span class="n">o_gmem</span><span class="p">,</span> <span class="n">o_smem</span><span class="p">):</span>
    <span class="n">wg_idx</span> <span class="o">=</span> <span class="n">lax</span><span class="o">.</span><span class="n">axis_index</span><span class="p">(</span><span class="s2">&quot;wg&quot;</span><span class="p">)</span>
    <span class="n">wg_slice</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">ds</span><span class="p">(</span><span class="n">wg_idx</span> <span class="o">*</span> <span class="n">tile_n</span><span class="p">,</span> <span class="n">tile_n</span><span class="p">)</span>
    <span class="c1"># pl.program_id obtains the index into the pallas_call grid.</span>
    <span class="n">pid_m</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">pid_n</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">compute_thread</span><span class="p">(</span><span class="n">pipeline</span><span class="p">):</span>
      <span class="n">acc</span> <span class="o">=</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">layout_cast</span><span class="p">(</span>
          <span class="n">jnp</span><span class="o">.</span><span class="n">full</span><span class="p">((</span><span class="n">tile_m</span><span class="p">,</span> <span class="n">tile_n</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">Layout</span><span class="o">.</span><span class="n">WGMMA</span><span class="p">,</span>
      <span class="p">)</span>
      <span class="c1"># yield marks the place where the pipelined loop will be inserted.</span>
      <span class="c1"># Its argument are the initial carry values, and its result is the carry</span>
      <span class="c1"># value after the loop completes.</span>
      <span class="n">final_acc</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="n">acc</span><span class="p">)</span>
      <span class="n">o_smem</span><span class="p">[:,</span> <span class="n">wg_slice</span><span class="p">]</span> <span class="o">=</span> <span class="n">final_acc</span><span class="p">[</span><span class="o">...</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">kernel_body</span><span class="p">(</span><span class="n">_</span><span class="p">,</span> <span class="n">a_smem</span><span class="p">,</span> <span class="n">b_smem</span><span class="p">,</span> <span class="n">carry</span><span class="p">):</span>
      <span class="n">acc</span> <span class="o">=</span> <span class="n">carry</span>
      <span class="n">b_smem_wg</span> <span class="o">=</span> <span class="n">b_smem</span><span class="o">.</span><span class="n">at</span><span class="p">[:,</span> <span class="n">wg_slice</span><span class="p">]</span>
      <span class="k">def</span> <span class="nf">do_wgmma</span><span class="p">(</span><span class="n">acc_ref</span><span class="p">):</span>
        <span class="n">plgpu</span><span class="o">.</span><span class="n">wgmma</span><span class="p">(</span><span class="n">acc_ref</span><span class="p">,</span> <span class="n">a_smem</span><span class="p">,</span> <span class="n">b_smem_wg</span><span class="p">)</span>
      <span class="n">acc</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">run_state</span><span class="p">(</span><span class="n">do_wgmma</span><span class="p">)(</span>
                          <span class="n">plgpu</span><span class="o">.</span><span class="n">ACC</span><span class="o">.</span><span class="n">init</span><span class="p">(</span><span class="n">acc</span><span class="p">))</span>
      <span class="k">return</span> <span class="n">acc</span>

    <span class="n">pipeline</span> <span class="o">=</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">emit_pipeline_warp_specialized</span><span class="p">(</span>
        <span class="n">kernel_body</span><span class="p">,</span>
        <span class="n">in_specs</span><span class="o">=</span><span class="p">[</span>
            <span class="n">plgpu</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">(</span>
              <span class="p">(</span><span class="n">tile_m</span><span class="p">,</span> <span class="n">tile_k</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">k</span><span class="p">:</span> <span class="p">(</span><span class="n">pid_m</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">transforms</span><span class="o">=</span><span class="n">transforms</span>
            <span class="p">),</span>
            <span class="n">plgpu</span><span class="o">.</span><span class="n">BlockSpec</span><span class="p">(</span>
              <span class="p">(</span><span class="n">tile_k</span><span class="p">,</span> <span class="n">tile_n</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span> <span class="k">lambda</span> <span class="n">k</span><span class="p">:</span> <span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">pid_n</span><span class="p">),</span><span class="n">transforms</span><span class="o">=</span><span class="n">transforms</span>
            <span class="p">),</span>
        <span class="p">],</span>
        <span class="n">grid</span><span class="o">=</span><span class="p">(</span><span class="n">grid_k</span><span class="p">,),</span>
        <span class="n">compute_context</span><span class="o">=</span><span class="n">compute_thread</span><span class="p">,</span>
        <span class="n">max_concurrent_steps</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">num_compute_wgs</span><span class="o">=</span><span class="n">compute_wgs</span><span class="p">,</span>
        <span class="n">memory_registers</span><span class="o">=</span><span class="mi">40</span><span class="p">,</span>
        <span class="n">memory_thread_idx</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
        <span class="n">wg_axis</span><span class="o">=</span><span class="s2">&quot;wg&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="c1"># Call the pipeline</span>
    <span class="n">pipeline</span><span class="p">(</span><span class="n">a_gmem</span><span class="p">,</span> <span class="n">b_gmem</span><span class="p">)</span>
    <span class="c1"># Copy the output from SMEM to GMEM.</span>
    <span class="n">plgpu</span><span class="o">.</span><span class="n">commit_smem</span><span class="p">()</span>
    <span class="n">m_slice</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">ds</span><span class="p">(</span><span class="n">pid_m</span> <span class="o">*</span> <span class="n">tile_m</span><span class="p">,</span> <span class="n">tile_m</span><span class="p">)</span>
    <span class="n">n_slice</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">ds</span><span class="p">(</span><span class="n">pid_n</span> <span class="o">*</span> <span class="n">tile_n</span> <span class="o">*</span> <span class="mi">2</span><span class="p">,</span> <span class="n">tile_n</span> <span class="o">*</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">plgpu</span><span class="o">.</span><span class="n">copy_smem_to_gmem</span><span class="p">(</span><span class="n">o_smem</span><span class="p">,</span> <span class="n">o_gmem</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">m_slice</span><span class="p">,</span> <span class="n">n_slice</span><span class="p">])</span>
    <span class="n">plgpu</span><span class="o">.</span><span class="n">wait_smem_to_gmem</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">plgpu</span><span class="o">.</span><span class="n">kernel</span><span class="p">(</span>
      <span class="n">kernel</span><span class="p">,</span>
      <span class="n">out_shape</span><span class="o">=</span><span class="n">jax</span><span class="o">.</span><span class="n">ShapeDtypeStruct</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float16</span><span class="p">),</span>
      <span class="n">scratch_shapes</span><span class="o">=</span><span class="p">[</span>
          <span class="n">plgpu</span><span class="o">.</span><span class="n">SMEM</span><span class="p">((</span><span class="n">tile_m</span><span class="p">,</span> <span class="n">tile_n</span> <span class="o">*</span> <span class="mi">2</span><span class="p">),</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
          <span class="p">],</span>
      <span class="n">grid</span><span class="o">=</span><span class="p">(</span><span class="n">grid_m</span><span class="p">,</span> <span class="n">grid_n</span> <span class="o">//</span> <span class="mi">2</span><span class="p">),</span>
      <span class="n">grid_names</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;m&quot;</span><span class="p">,</span> <span class="s2">&quot;n&quot;</span><span class="p">),</span>
      <span class="n">num_threads</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>  <span class="c1"># 2 compute, 1 memory.</span>
      <span class="n">thread_name</span><span class="o">=</span><span class="s2">&quot;wg&quot;</span>
  <span class="p">)(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="n">m</span> <span class="o">=</span> <span class="mi">132</span> <span class="o">*</span> <span class="mi">128</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">*</span> <span class="mi">128</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">10</span> <span class="o">*</span> <span class="mi">64</span>
<span class="n">key1</span><span class="p">,</span> <span class="n">key2</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">key1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">k</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">key2</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">float16</span><span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">matmul_warp_specialized</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>

<span class="n">np</span><span class="o">.</span><span class="n">testing</span><span class="o">.</span><span class="n">assert_allclose</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">a</span> <span class="o">@</span> <span class="n">b</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="reference.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Writing Mosaic GPU kernels with Pallas</p>
      </div>
    </a>
    <a class="right-next"
       href="../design/index.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Pallas Design Notes</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pipelining-with-mosaic-gpu">Pipelining with Mosaic GPU</a><ul class="visible nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#compatibility-api-using-pl-pallas-call">Compatibility API using <code class="docutils literal notranslate"><span class="pre">pl.pallas_call</span></code></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpu-memory-spaces">GPU Memory Spaces</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-matmul-kernel-on-hopper-gpus">Example: Matmul Kernel on Hopper GPUs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#warp-specialization">Warp Specialization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-matrix-multiplication-with-warp-specialization">Example: Matrix Multiplication with Warp Specialization</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The JAX authors
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2024, The JAX Authors.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=5b4479735964841361fd"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=5b4479735964841361fd"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>